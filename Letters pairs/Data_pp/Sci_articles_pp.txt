machine learning enabled team performance analysis in the dynamical environment of soccer shitanshu kusmakar member ieee sergiy shelyag ye zhu member ieee dan dwyer paul gastin and maia angelova school of information technology deakin university geelong vic australia school of exercise and nutrition sciences deakin university geelong vic australia la trobe sport exercise medicine research centre la trobe university melbourne vic australia corresponding author maia angelova maia a deakin edu au this work was supported by the dsi collaborative research intelligent sensor processing for enhancing defence decision support under grant rm abstract team sports can be viewed as dynamical systems unfolding in time and thus require tools and approaches congruent to the analysis of dynamical systems the analysis of the pattern forming dynamics of player interactions can uncover the clues to underlying tactical behaviour this study aims to propose quantitative measures of a teams performance derived only using player interactions concretely we segment the data into events ending with a goal attempt that is shot using the acquired sequences of events we develop a coarse grain activity model representing a player to player interaction network we derive measures based on information theory and total interaction activity to demonstrate an association with an attempt to score in addition we developed a novel machine learning approach to predict the likelihood of a team making an attempt to score during a segment of the match our developed prediction models showed an overall accuracy of in predicting the correct segmental outcome from matches in our dataset the overall predicted winner of a match correlated with the true match outcome in of the matches that ended in a result furthermore the algorithm was evaluated on the largest available open collection of soccer logs the algorithm showed an accuracy of in the classification of the segments from matches and correctly predicted the match outcome in of matches that ended in a result the proposed measures of performance offer an insight into the underlying performance characteristics index terms dynamical systems network science distribution entropy football kolmogorov complexity machine learning performance analysis shannon entropy support vector machines soccer i introduction improving comprehension of strategic performance and success in team competition is an important goal in sports science data driven methods can effectively overcome the subjective limitations manual analysis of the match and offer better results for football clubs quantitative analysis can provide players and coaches with such insight by allowing them to improve their match and assessment of the event beyond what personal observation can accomplish traditionally methods of performance analysis push the study of one dimensional and discrete performance indicators towards probabilistic and correlational approaches however this results in somewhat limited functional information as it lacks the associate editor coordinating the review of this manuscript and approving it for publication was paul d yoo the understanding of the player to player interactions that support the actions of players and overall team behaviour it is reasonable to expect an analysis of such one versusone dynamics in team sports to be insufficient as multiplayer interactions are important in determining success and failure therefore in order to quantify and explain performance it has been advocated that performance analysis in team sports must also focus on the interactions between players that sustain the overall team behaviour from the dynamical systems view the understanding of how the coordination emerges from the interaction among the system components that is the player to player interaction is the key to performance analysis in team sports performance analysis approaches that consider the interactions of the players in many multiplayer team competitions like football are not well explored this work is licensed under a creative commons attribution license for more information see https creativecommons org licenses by volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer inspired by empirical studies of networked systems researchers have recently developed a variety of techniques and models to help us understand player interaction network in sports interaction or passing networks can be constructed from the observation of ball transfer between players a key challenge is to leverage the interaction networks to gain a functional understanding of the underlying team strategies for example by examining the structure of interaction networks recurrent pass sequences can be identified and linked to a teams playing style when the emphasis is put at the player level duch et al used the interaction networks to quantify and rank players contribution relative to the overall team activity due to dissimilarity and diversity in real world sports data there is no systematic program for predicting network structure in addition there are no particular subsets of diagnostics that are universally accepted since team networks are intrinsically subjective and dynamic objects it is often hard to determine a suitable way of network characterisation that governs team formation in team sports like football quantifying player to player interaction is the key for understanding the dynamic patterns that generate a scoring opportunity this motivated us to develop an approach that quantitatively characterises players interaction in team sports in this study a data driven approach to the study of complex player interactions from event stream data generated during football matches henceforth referred to as soccer is employed the proposed framework can be used to quantify player interactions and connect that with the outcome using a machine learning approach data driven approaches for soccer analytics are given importance with the availability of the event stream data e g opta wyscout stats secondspectrum scisports and statsbomb cintia et al in their work extracted pass based performance measures to learn the correlation to match outcome using a machine learning approach more recently pappalardo et al in their work employed a machine learning approach to rank players their approach is based on computing statistical features from the event stream data for each player which are then utilised to learn feature weights in a supervised learning framework i e relative to the match outcome the authors then use the learned weights to compute the rating of a player in another recent study by decroos et al the authors have performed a segmental analysis of different match states to extract several associative features of player performance which are then used to determine the scoring or conceding probability using an ensemble classifier in contrast to the above mentioned studies that consider individual players actions or cumulative team statistics the proposed study describes a segment of a match using a set of activity and entropy based quantifiable markers that capture both inter and intra player interactions to quantify interaction among players in team sports conceived as dynamical systems unfolding in time it is important to use appropriate measures the proposed study considers the behaviour of multiple players and the emergent nature of performance to develop pattern forming dynamics that is the dynamic physical relationships that a player may establish with the teammates and opponents to make a goal we developed a coarse grain activity model of player toplayer interaction from the possession chain data that can be used to quantify the dynamic patterns underlying the interaction among players we used the concepts of information theory retrieval to quantify the complexity of a pattern representing player interactions during sub segments of the match another key challenge from the analytics perspective is the format of the soccer log data as different vendors use different data formats therefore an analyst has to develop complex pre processors specific to a dataset to tackle the challenges posed by the variety of event stream formats and to benefit the data science community we propose an approach that uses only a limited amount of information the proposed approach only uses the possession information such as player team action type and result from the event stream data the segmental analysis was thus performed using only the possession information to quantify the team performance and stability in team dynamics during a specific module that is a match segment furthermore based on the derived performance measures we developed a machine learningenabled decision support system for automated prediction of a teams likelihood of a successful attempt at goal ii approach a dataset in this study we have analysed the dataset from a season of major league soccer division of the united states and canada the dataset consists of the possession chain data from matches the interaction information possession chain comprises of time and duration of all ball passes and tackles between players the dataset also includes the nature of the interaction which can be categorised as being between teammates or between opposing players table the positional information includes the x y position of all individuals throughout the entire match minutes b coarse grain player interaction model given a set of possession chain information for each match representing a set of events pass shot etc between players and the game outcome a match is split into a number of segments where each segment represents a phase of the match that begins with either the start of the match or after an attempt shot at the goal and ends with a shot see fig further throughout the text the teams in an adversarial relationship during a match were denoted by team and team for each match in the dataset using the possession information corresponding to every segment in the match we propose a coarsegrain model to find quantifiable measures of performance that demonstrate an associationship with the outcome of that segment that is which team team or team makes an attempt to score by taking a shot at the oppositions goal volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer table an example of ball possession chain data the table shows a part of a ball possession chain dataset which represents events in the st half of a match figure segmentation of the possession chain data a match is split into different segments of varying length or the number of events in a segment ending with a shot the red and blue shaded cells represent possession by different teams each segment was individually evaluated for measures of performance coarse grain models derived from possession chain data each of the match segments was studied separately the segments represent a sequence of ball possession change events leading to an attempt to score each team in a soccer match has players with allowed replacements based on the sequence of events in each segment we define two types of coarse grain models the first model weighs all the events e g pass shot at goal ball lost equally whereas the second model weighs events based on their type more specifically a higher weight of an event denotes a higher relevance as we are interested in measures that quantify a successful attempt to score we assign higher weights to shots and recoveries and lower weights to events like ball lost and faults for each segment we first generate a pairwise player matrix mi j where i j each element of which was initialised to zero the matrix m contains players of both teams i j and i j for team and team respectively and any element mi j represents the interaction of the i th player with j th player in the segment the value of the mi j element denotes the number of times the players interacted or the number of times the players interacted weighed by the type of event for example if player of team passes the ball to player of team the element m of the matrix is incremented by i e m m similarly if player of team recovers the ball in a tackle from player of team then the element m of the matrix is incremented by i e m m therefore the diagonal blocks of the matrix m denote interactions of the players within a team whereas the offdiagonal blocks represent the inter team player interactions thus the matrix m such that mi j i j was termed as the interaction matrix the matrix m represents the connections on the network of players agents related to activitybased decision making to the directed transfer of information ball from one agent to another this coarse grain interaction model m represents the network of connections accumulated over a sequence of events during a segment of the match we analysed the interaction between players based on the following approach a unit increment each element mi j of the interaction matrix is incremented by for an interaction between the i th and j th player of the same team ball passed or players of the different team ball recovery tackle ball lost etc as follows mi j mi j b weighted increment each element mi j of the interaction matrix is weighted by the type of the event et more specifically we assign a weight to each event for evaluation of its contribution given that we are mostly interested in goal attempts we introduce a higher weight for shots in comparison to passes ball losses and other events that lead to loss of ball possession mi j mi j wet volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer where wet is the weight corresponding to the event et as we are interested in the likelihood of a successful attempt at goal we assign a high weight wshot to shots a low weight wpass to passes and an average weight wshot pass to all other event types as suggested by decroos et al c quantification of team performance from coarse grain models to quantify the performance of a team in each segment of the match four measures were proposed total activity index tai to quantify the interaction in a segment the matrix m is further divided into four blocks by summing all the elements mi j in top left team i j in and the bottom right team i j in which represent the overall activity of each team that is obtained by summing the activity of all players in a team the off diagonal elements represent the interaction between players of both teams any row in the top left team i in or bottom right team i in block of matrix m represents the interaction of the i th player with the rest of his team similarly any row in the off diagonal blocks of the matrix m represents player i i of team losing the ball to player j j of team and vice versa we introduce the average team activity matrix t as follows t t t t t where each element of matrix t represents the average activity of each block in m as follows t x n i j mi j t x n i j n mi j t x n i x n j n mi j t x n i n x n j mi j where n and pn i j mi j represent a players activity team i and team i respectively the overall activity ac of each team in a segment is then calculated as ac t t t ac t t t where p i j ti j is a normalisation constant the total activity index tai of the match is then computed as follows tai ac ac information entropy as a measure of performance it has been advocated that performance analysis in a team sports should consider the dynamical nature of the match and must consider player to player interaction the stability and consistency of interaction between different players of a team have been considered as a measure of performance in soccer matches entropy quantifies the uncertainty coming from the random aspect of the dynamics entropy as a measure can be utilised to quantify the consistency of patterns representing player to player interaction in the match a shannon entropy previously shannon entropy has been used as a measure of uncertainty in team sports to quantify the variability associated with the movements of players in a match in this work we have used shannon entropy to quantify the patterns representing player to player interaction during a segment of the match shannon entropy is a measure of the uncertainty or unpredictability in the estimate of the information content of a random variable the shannon entropy h is defined as follows h x n i pi ln pi where pi is the probability of the i th element in the sequence b kolmogorov complexity as an alternative to the probabilistic notion of information content the kolmogorov complexity is based on the concept of recursive function kolmogorov complexity allows the characterisation of chaotic motion in dynamical systems and the analysis of spatiotemporal patterns the kolmogorov complexity c n of a sequence with n samples is the length of the shortest binary program that can generate that sequence as output an appropriate measure of kolmogorov complexity can be defined by h n as follows h n c n b n where b n n log n in this work kolmogorov complexity of the signal was calculated following kaspar et al c distribution entropy distribution entropy disten computes the complexity of a time varying sequence using the distribution of the inter vector distances unlike approximate and sample entropy disten offers high robustness for short length sequences and reduced dependence on pre determined parameters disten has been previously used in many biomedical applications to quantify the complexity of short length signals in the context of soccer disten can be used to characterise the complexity of the dynamical network patterns representing the player to player interaction volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer during a segment the disten of a vector can be defined as disten m log x i pi log pi where is the number of bins in the probability distribution obtained from the data with the lag and embedding dimension m these parameter values are selected based on common recommendations from literature d entropy derived performance indexes to quantify the complex behaviour in which players interact during a soccer match three entropy measures were calculated based on the type of the entropy three indexes were defined shannon entropy index sei kolmogorov complexity index kci and distribution entropy index dei let s n denote the entropy for a sequence of length n we calculate s n for each row in each of the four blocks of the interaction matrix m we introduce a matrix s to represent the team entropy complexity matrix as follows s s s s s here the elements of matrix s represent the averaged entropy complexity of player to player interaction in each block of matrix m as follows s n x n i h mi j n s n x n i n h mi j n n s n x n i h mi j n n s n x n i n h mi j n where n the overall complexity for each team in a segment is given by s s s s s s s s the three entropy derived indexes sei kci and dei of a segment in the match are then computed as follows derivedindex s s where the derivedindex is sei kci dei for s denoting shannon entropy kolmogorov complexity and distribution entropy respectively d machine learning approach the possession chain data from each segment in a match was quantified using the proposed measures which were then used as features for predicting the team that makes the shot during the segment in the model training phase the predictive model was trained using a supervised framework where each segment ending in a shot was given a label if team makes the shot and a label if the opposition makes the shot during the testing and validation phase the learned model was then used to predict the team making the shot in a segmental manner the outcome of the game i e team winning the match was determined based on the classification of the segments team team where the shot ends in a goal for each game we report the segmental performance and the predicted match outcome i e winner of the match we now describe the classifier and the learning procedure support vector machine support vector machines svm are state of art binary state classifiers which are suited for pattern recognition and classification problems with good robustness to overfitting given an i i d learning set x y x y xi yi where x n y the kernel function maps the input feature space to a high dimensional space where the data is linearly separable offering the ability to learn non linear functions and decision boundaries the decision function separating the two classes is learned as a hyperplane the optimisation problem can be formulated as min b k k c n xn i l subject to yi x b i i n where c is a positive regularisation constant and is the slack term by using the lagrange multiplier techniques the optimisation problem in svm is reduced to a dual optimisation problem max k w xn i i xn i xn j iyi jyjkhxi xji subject to pn i iyi and i c i n the learned decision function can then be represented as f x sgn xn i iyikhxi xi b where khxi xji represents the kernel function in this study we have used the gaussian radial basis kernel function learning classification models the classification models were trained using a leave oneout cross validation approach let n represents the total number of matches in leave one out cross validation approach the dataset corresponding to a match is left out while the dataset from the remaining matches n is volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer figure a player interaction matrix m computed for the unit increment for a segment of match g b same as a for the weighted increment c team activity matrix t computed for the unit increment for a segment of match g d same as c for the weighted increment the self interaction main diagonal of the matrix m has been saturated in the left panel to reveal the interaction between the players every element mi j represents the ball originator and receiver for an event the main diagonal blocks in a and b represent the interaction between players of the same team whereas the top right and bottom left corner blocks represent interactions with the players of the opposition team the lighter the color the higher the value of activity between the players for the shown segment team made an offensive attack against the team which is also evident in the higher activity lighter color of team as shown in c and d used for training the svm classifier a feature selection using lasso technique was applied on the training set for finding the least correlated and most discriminating features thus ensuring the test data left out match was not a part of feature selection and model learning procedure iii results and discussion in this study we have analysed each match by segmenting into sequences that end with a shot the possession chain data in each segment was first mapped onto a matrix m representing match integrated ball possession activity of players fig to calculate the estimate of complexity and non linear dynamics in a match of soccer using the proposed coarse grain model of teams activity we introduced four quantitative measures of team performance tai sei kci and dei in addition a machine learning approach was presented where we developed machine learning models to predict the outcome of a segment based on the proposed quantitative measures of performance we first explain the quantitative measures of performance derived from the proposed coarse grain model of player interactions network a total activity index tai b shannon entropy index sei c kolmogorov complexity index kci and d distribution entropy index dei followed figure match g atlanta united fc team vs san jose earthquakes team season final results temporal evolution of the proposed quantitative markers of performance a total activity index tai b shannon entropy index sei c kolmogorov complexity index kci and d distribution entropy index dei derived using the weighted network of connections represented by matrix m the vertical dashed lines indicate the moments at which a goal was scored in the match the red and blue lines represent the goal scored by team and team respectively each interval on the timeline represents the time stamp of the segment ending with a shot by e the performance of the proposed machine learning approach and future work a total activity index tai the total activity index tai is a measure of a teams activity relative to the other during a segment based on the definition of tai a positive value of tai indicates that team is likely to take the shot at the end of the segment while a negative value indicates team table fig a the underlying hypothesis was that the more frequently or longer the players of a team interact during a segment the more likely it is that this team scores in the particular segment of the match this was further corroborated by the minimum and the maximum values of tai as seen for example in match g atlanta united fc team vs san jose earthquakes team season final result that correspond to the segments when first team was trying to score and then team was trying to equalise by maintaining a higher possession of the ball the segment ending at th and th minutes of the match g fig a when plotted with respect to the ground truth i e the outcome of the segment w r t to the team taking the shot the distribution of tai is close to normal for both the teams fig a fig b the descriptive statistics relating to the performance of tai are shown in table results showed significantly different p means for both teams table although a certain overlap could be seen among the tai value ranges derived using the unit and weighted increment matrices table fig a and fig b the area under the receiver operator characteristics curve auc values of and for tai derived from the unit and weighted increment matrices show a good class separability volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer figure a distribution of the proposed measures of performance on shots with respect to the true segmental outcome shown for all matches in the dataset a b tai c d sei e f kci and g h dei derived using unit and weighted network of connections represented by the interaction matrix m the distribution of the derived quantitative measures tai sei kci and dei was close to normal with both teams having a significantly different means p the better performance of the weighted increment matrix shows the introduced bias towards the segment outcome shot due to the higher weights given for the events that are likely to result in goal attempts in comparison to normal passes furthermore the use of weights provides an alternative evaluation function that offers the opportunity to consider the types of events appearing in a pattern and the patterns support to determine its relevance finally the tai derived from the coarse grain activity model shows good potential as a quantitative measure of performance in a team sport like soccer b shannon entropy index sei shannon entropy gives a measure of uncertainty to quantify the randomness associated with a time varying signal the shannon entropy index sei quantifies the underlying variability in player to player interaction for a team relative to the other team the shannon entropy of a team in a segment would be low if only few players interact with each other thus minimising the randomness and the associated unpredictability whereas it would be high if different players are continuously interacting with each other a higher entropy indicates that there is more uncertainty in patternforming dynamics governing the interaction among players alternatively a higher entropy represents that players are not constrained to a specific role and assume a higher tactical role e g players moving both forward backward and through the sides of pitch thus forging more player to player interactions in team sports a longer possession of the ball is likely to forge more player to player interactions especially during a strategy leading to an offensive on the opposition more players are likely to be involved e g in a match of soccer midfielders centre forwards wing forwards can be a part of an attack therefore we hypothesised that the shannon entropy for the team that is attacking would be higher relative to the other therefore as defined in section ii c d sei would be if team is attacking and if team is attacking table fig b the minimum and the maximum values of sei denote an offensive behaviour by team and team respectively segments ending at th and th minute in fig b thus sei can be a good marker indicating when a team makes an offensive against the opposition the sei index correlated with the segment outcomes that is whether team or team takes the shot highest auc table the mean sei for team and team were significantly different for both unit and weighted increment matrix table a similar observation was made from the distribution when sei was plotted with respect to the true outcome of the segment fig c and fig d based on the descriptive statistics table the sei index can be used as a potential marker of a teams performance derived from a coarse grain network model representing player to player interaction c kolmogorov complexity index kci the use of kolmogorov complexity was motivated by the presumption that interaction among players during a segment can be both random or synchronised if certain players interact more frequently let us consider two vectors sx and sy of length each only a maximum of players of each team were active during any segment of the match however a pattern length of was considered as a soccer match can have a maximum of substitutes that represents interaction pattern of players sx and sy the value of the i th element in vectors sx and sy represents the number of times the i th player i interacted with player sx or sy including any self interaction both the sequences sx and sy have the same shannon entropy of and disten of m whereas both have a different kolmogorov complexity and volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer table mean standard deviation and area under the receiver operator characteristic curve statistics of the indexes tai sei kci and dei derived using two different types of interaction matrix m over all matches in the dataset the statistics show that the proposed indexes have significantly different values corresponding to the team taking the shot at the goal a positive value of the derived indexes denotes that team takes the shot while a negative value indicates team relative to whom the indexes are computed respectively sequence sx has a pattern composed of units in recursion whereas sequence sy has no obvious pattern thus sy has a higher complexity in the context of soccer if the players are interacting in a synchronised manner that is few particular players are part of a strategy offensive or defense such patterns would be represented by simpler sequences with lower complexity or unpredictability from a coaches point of view it is important to assess the dynamics of pattern formation occurring in each segment of the match to decode the underlying strategy the proposed kolmogorov complexity index derived from the player toplayer interaction network matrix m gives a quantitative measure of local numerical relations in which the dynamics of a teams pattern formation varies relative to the other team for example if certain players are only restricted to particular parts of the playing pitch as in the formation the playerto player interactions in such a segment would be represented by a less complex patterns like sx on the other hand if a team allocates more players in sub segments of a match to prevent oppositions attacking move i e a defensive strategy or to create an offensive move at oppositions goal the playerto player interactions would be represented by more complex patterns without any recursive sub patterns as shown by sy therefore kolmogorov complexity derived kci index captures the complexity of patterns that is different from shannon entropy derived index or sei kci showed a good correlation when plotted with the segmental outcome of a match auc table a kci value favoured team while a kci indicates a shot taken by team fig c for match g the segment ending at the th minute represents a case when team is making an offensive against the opposition to level the scores at which is shown by the maximum value of kci at the th minute fig c the kci index followed a distribution close to normal when plotted on the true segment outcome that is with respect to the team taking the shot fig e and f with a significantly different mean values for both the teams table kci is a measure to quantify the regularity of complex patterns in which players interact during team sports it gives a numerical relation in which the dynamics of a teams pattern formation varies over the segments of a match kci can allow coaches to discover identify and quantify segments during a match when a team interacts in more complex or rather synchronised patterns d distribution entropy index dei the distribution entropy disten measures the complexity of patterns governing player to player interactions by taking into account the hidden information in the state space via estimating the probability density of inter vector distances a chaotic sequence has the maximum disten thus patterns of player to player interaction with high variability would be characterised by a high disten and vice versa the distribution entropy derived index dei quantifies the chaotic patterns underlying player to player network of interaction for a team relative to the other similar to shannon entropy derived index a dei would indicate a higher variability associated with an attacking move in patterns governing player to player interaction for team when computed relative to team fig d however a particular advantage of dei over sei is that sei can be affected by the variance of the sequences representing player interaction patterns while dei is derived using a probability density function with fixed bin number thus dei is more robust as it considers inter vector distances let us consider two vectors sx and sy that represent the interaction patterns of player x and player y during a segment of the match and having the same number of total interactions psx psy the in sy represents the number of times the th player of the team interacted with player y during the segment the shannon entropy for sx and sy is and respectively whereas disten is and respectively pattern sy represents that player y interacts more frequently times with the th player which results in high inter vector distances thus leading to a higher disten value the interaction pattern represented by sy indicates the events when player y is continuously interacting with a particular player th player in the team this pattern might signify an underlying strategy where the players defender midfielder volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer figure match g atlanta united fc team vs san jose earthquakes team season final results the segmental analysis for match g showing the predicted outcomes for every segment compared to the true outcomes a segmental likelihoods b predicted outcomes and c the true match outcome which is the team taking the shot at the oppositions goal at the end of the segment the vertical bars on the match timeline show the segments where a team scored a goal red bars indicate team and blue bars indicate goals scored by team the intervals on the timeline indicate the time stamp corresponding to the segments ending with a shot are interacting with a particular player forward as a part of a strategy to generate a scoring opportunity disten provides an ability to encode such patterns thus distribution entropyderived index dei can be a good marker to characterise the complexity of player to player interaction patterns such that the underlying strategy can be quantified as a measure of a teams performance during a segment of the match when plotted with respect to the outcome of the segment the dei values were normally distributed fig g and h the mean dei values for the teams in the adversarial relationship were significantly different table and a good class separability was achieved with an auc of and for dei derived using unit and weighted interaction network of players e performance of the machine learning approach we developed an automated machine learning model to predict the outcome of a match segment using the proposed measures of performance quantification tai sei kci and dei our machine learning approach showed a mean sensitivity of confidence interval ci a specificity of ci and an overall accuracy of in predicting the segmental outcomes of the matches although our dataset comprised of only matches it should be noted that we performed a segmental analysis on segments of different duration segments ending with shot resulting in a sizeable number of samples temporal segments for training and validating the machine learning classifier in addition our approach is based on a robust cross validation approach that ensures no bias of the learned model to the ground truth the predicted segmental outcomes for all the matches in the database are shown in table one match g ended in draw among the rest the predicted outcome correlated with the ground truth i e the winner of the match in of matches furthermore the application of the automated segmental analysis is not limited to the overall match outcome it also helps to analyse the underlying local prediction statistics the outcome of our developed prediction model on a complete match is shown in fig the prediction models give the segmental likelihood of an attempt to volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer table the predictive performance of our developed machine learning models shown are the segments predicted in favour of a team with the overall prediction accuracy the predicted winner and the true match results table validation of the proposed approach on the largest open collection of soccer logs from major competitions shown are the total number of matches matches ending in result the number of analysed segments mean performance measures with ci confidence interval over all matches of each competition and the accuracy depicting the percentage of matches where the predicted winner correlated with the true match outcome goal for both the teams fig a in the particular match shown in fig team shown in red won the match the segments where a goal was scored are marked with segments and shown in fig it can be seen in fig that segments where team has scored a goal segments and have a higher likelihood similarly the likelihood for team is higher in the segments where they scored a goal segments and show where both the teams are engaged in gaining the possession of the ball as they want to equalise the segments where the predicted outcome fig b did not match the true outcome fig c are the ones where the possession of the ball is continuously changing between the teams to quantify the minority of segments where the model does not provide a sufficient agreement with the ground truth data in future we would incorporate more sophisticated measures by introducing player labels forwards mid fielders defence to understand player to player interaction using concepts of mutual information retrieval f validation and comparisons on public dataset to elaborate on the efficacy of the proposed approach a thorough performance evaluation was carried out on the largest available public dataset of soccer logs this dataset comprised event logs possession chain data from matches of major competitions table the proposed machine learning approach showed an overall sensitivity of a volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer specificity of f score of and an auc of in classifying a total of segments ending in shot i e whether team or team makes the shot at the goal table the match outcome segments leading to a goal correctly correlated in of matches that ended in a result the performance measures with the ci calculated over all matches in a competition for each competition are also reported table the european cup and the italian first division had the lowest and the highest auc for segmental analysis among the competitions overall the auc of the segmental performance was close to the overall auc for each of the competitions which shows the consistent performance of the proposed approach across the competitions the italian first division had the highest number of segments ending in shot from the matches the proposed machine learning approach resulted in an auc of ci in correctly classifying the segments furthermore the predicted match winner correlated with the ground truth in of the matches of the italian first division that ended in a result the overall performance of the proposed approach on matches and segments shows the efficacy of the proposed quantitative markers tai sei kci and dei of a teams performance furthermore to elaborate the efficacy of the proposed quantifiable markers of team performance we compared the results of the proposed approach with studies that employ a machine learning approach for evaluating performance a direct comparison of the proposed segmental analysis approach can be done with the study by decross et al they employed a segmental analysis to learn the importance of players actions based on the outcome of a match state e g success in taking a shot at opponents goal on the contrary pappalardo et al defined a feature vector for each team and modelled the outcome of the match win loss using a linear support vector machines classifier as both the studies use different datasets therefore to ensure a direct comparison of the proposed approach the algorithms by decross et al and pappalardo et al are run on the soccer logs from matches of competitions the model estimation and the learning task was performed using a leave one out crossvalidation approach as explained in section ii d additionally the results on the public dataset were compared with the study by cintia et al who analysed the match outcome using pass based performance indicator h indicator and evaluated the performance on the german spanish italian and english division leagues for a comparison of the segmental performance the algorithm by decross et al was used to model the segments ending in a shot with an xgboost classifier the algorithm by decross et al showed an overall auc of table in comparison the proposed approach showed a similar performance with an overall auc of on analysed segments table further for a comparison of the correctly predicted match outcome the algorithm table comparisons of the proposed approach with some recent machine learning based studies by pappalardo et al was employed the algorithm by pappalardo et al could correctly classify the match outcome in of matches that ended in a result among the competitions table in comparison the proposed approach could correctly classify the match winner in of matches furthermore in comparison to the study by cintia et al who reported a mean accuracy in correctly predicting the match outcome the proposed approach showed a higher mean accuracy of german spanish italian and english division in correctly predicting the match outcome the improved performance of the proposed approach shows the robustness and efficiency of the proposed quantitative markers tai sei kci and dei in capturing a teams underlying performance characteristics table the performance of the proposed approach can be attributed to the use of kernelised classifier and the nonlinearity of the proposed indices like sei kci and dei that can quantify the underlying non linear dynamics of player interaction based on the performance validation on external dataset and comparison with recent studies it can be concluded that the proposed approach offers a data driven framework for evaluating a teams performance in a segmental manner offering the potential for predictive analytics in sport sciences using data science research volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer figure the feature histograms showing players activity shannon entropy shnen kolmogorov complexity kolcmp and distribution entropy disten derived from the player interaction matrix m the derived parameters are normalised to ensure feature commensurablity shown here for a segment of match g a team and b team the dotted horizontal lines in subplots a and b represent the mean across all the players of a team the rectangular box in histogram plot for team subplot a indicates the players p p p p p and p who maintain a ball possession activity that is higher than the teams mean player p who makes the shot at goal had the highest interaction with the rest of the players during team ball possession activity in contrast the ball possession activity of team is mainly among players p p p and p g interpretability for sports analysts our analysis shows that the interaction between players is essential for generating a scoring opportunity to outline the applicability of the proposed features we use histogram plots representing each players feature values that are derived from the player interaction matrix m for a segment of the match fig the histograms illustrate the level of interaction of each player when their team has possession of the ball the players that are more frequently involved in the ball possession have feature values above the teams mean value which is represented by the dashed line as shown in fig across this match segment team performs better than team because the segment ends with team having a successful attempt at scoring i e a shot at goal six players of team p p p p p and p maintain a level of interaction as indicated by the rectangular box in a above the teams mean which is higher than team where only three players are above the teams mean as indicated by the rectangular box in fig b the feature activity shows the players that are more frequently involved in a teams ball possession activity the remaining features shnen kolcmp and disten were also above average for more players in team than for team sports analysts can interpret this as an association between the complexity of passing between players and the likelihood of having a shot at goal in other words when a team has possession of the ball there may be a benefit in making a relatively large number of passes between a large proportion of the team as they move the ball towards their opponents goal post the usual analysis of an opponents tactics is a resourceintensive procedure as most tactical analyses are performed by manually reviewing the match videos or scouting matches in person to identify the players that are constantly part of the ball possession activity and are involved in generating scoring opportunities the features used in the present analysis may enable the automatic identification of such players using a data driven approach for example player p in team is one such player who had the highest ball possession activity during the shown segment of the match indicated by an in fig a identifying the players that are more frequently involved in match states that end with an attempt at scoring i e a shot at goal may assist sports analysts and team staff to develop strategies suited to an opponents playing style the proposed study presents different characteristics of a teams performance during a segment of a match that ends with a shot on the goal although there are different ways to define match segments e g a segment ending with the ball going out a foul etc the purpose of the study was to identify the characteristics leading to an attempt at scoring a goal therefore in this study we analysed segments ending with a shot on the goal which is also a limitation of the study furthermore the influence of match location quality of opposition match type etc were not controlled for while developing the predictive models thus further research is required to investigate the effects of these variables to further enhance the understanding of teams and players performances iv conclusion our study proposes information theory derived quantifiable measures of performance that can uncover the dynamic patterns underlying team sports like soccer the study provides first evidence of a machine learning enabled approach for volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer automated predictive analysis of performance in a segmental manner offering the potential for uncovering local numerical markers of team performance our developed predictive models show a mean accuracy of in predicting the segmental outcome of the likelihood of team making a successful attempt to score a goal on our dataset comprising matches in addition the segmental outcomes could predict the correct overall winner in of the matches that resulted in a winner furthermore the validation on an external dataset comprising segments from matches showed the robustness of the approach finally the study demonstrates that the analysis we present can help uncover the pattern dynamics of a teams network derived using possession chain data by quantitatively analysing measures of performance that have a specific distribution and that can be used to predict the performance of a team acknowledgment the authors would like to thank dr a kalloniatis and dr t wilkin for the useful and constructive comments during the development of the material for this article author contributions statement s k performed the analysis and wrote the manuscript s s y z and m a contributed to the analysis of the results d d provided the dataset d d and p g helped in formulating the studys significance from a sports perspective all authors contributed to and reviewed the manuscript references t mcgarry applied and theoretical perspectives of performance analysis in sport scientific issues and challenges int j perform anal sport vol no pp apr i franks use of feedback by coaches and players in proc sci football iii pp c wright c carling and d collins the wider context of performance analysis and it application in the football coaching process int j perform anal sport vol no pp dec d ara jo k davids and r hristovski the ecological dynamics of decision making in sport psychol sport exerc vol no pp nov p s glazier game set and match substantive issues and future directions in performance analysis sports med vol no pp aug l vilar d ara jo k davids and c button the role of ecological dynamics in analysing performance in team sports sports med vol no pp jan k davids d ara jo and r shuttleworth applications of dynamical systems theory to football in proc th world congr sci football sci football v j cabri t reilly and d ara jo eds abingdon u k routledge pp b travassos k davids d ara jo and t p esteves performance analysis in team sports advances from an ecological dynamics approach int j perform anal sport vol no pp apr h folgado k a p m lemmink w frencken and j sampaio length width and centroid distance as measures of teams tactical performance in youth football eur j sport sci vol pp s s jan p passos k davids d ara jo n paz j mingu ns and j mendes networks as a novel tool for studying team ball sports as complex social systems j sci med sport vol no pp mar c braham and m small complex networks untangle competitive advantage in australian football chaos interdiscipl j nonlinear sci vol no may art no j m buld j busquets j h mart nez j l herrera diestra i echegoyen j galeano and j luque using network science to analyse football passing networks dynamics space time and the multilayer nature of the game frontiers psychol vol p oct j ramos r j lopes and d ara jo whats next in complex networks capturing the concept of attacking play in invasive team sports sports med vol no pp jan l gyarmati and x anguera automatic extraction of the passing strategies of soccer teams arxiv online available http arxiv org abs p cintia f giannotti l pappalardo d pedreschi and m malvaldi the harsh rule of the goals data driven performance indicators for football teams in proc ieee int conf data sci adv analytics dsaa oct pp j duch j s waitzman and l a n amaral quantifying the performance of individual players in a team activity plos one vol no art no e r hristovski k davids d araujo and p passos constraints induced emergence of functional novelty in complex neurobiological systems a basis for creativity in sport nonlinear dyn psychol life sci vol no p j m buld j busquets i echegoyen and f seirullo defining a historic football team using network science to analyze guardiolas f c barcelona sci rep vol no pp dec p silva r duarte p esteves b travassos and l vilar application of entropy measures to analysis of performance in team sports int j perform anal sport vol no pp aug l pappalardo p cintia p ferragina e massucco d pedreschi and f giannotti playerank data driven performance evaluation and player ranking in soccer via a machine learning approach acm trans intell syst technol vol no pp nov t decroos l bransen j van haaren and j davis actions speak louder than goals valuing player actions in soccer in proc th acm sigkdd int conf knowl discovery data mining jul pp l vilar d ara jo k davids and y bar yam science of winning soccer emergent pattern forming dynamics in association football j syst sci complex vol no pp feb r duarte d ara jo h folgado p esteves p marques and k davids capturing complex non linear team behaviours during competitive football performance j syst sci complex vol no pp feb t decroos j van haaren and j davis automatic discovery of tactics in spatio temporal soccer match data in proc th acm sigkdd int conf knowl discovery data mining jul pp k davids c button d ara jo i renshaw and r hristovski movement models from sports provide representative task constraints for studying adaptive behavior in human movement systems adapt behav vol no pp mar p silva r duarte j sampaio p aguiar k davids d ara jo and j garganta field dimension and skill level constrain team tactical behaviours in small sided and conditioned games in football j sports sci vol no pp c e shannon a mathematical theory of communication bell syst tech j vol no pp jul oct f kaspar and h g schuster easily calculable measure for the complexity of spatiotemporal patterns phys rev a gen phys vol no pp jul a lempel and j ziv on the complexity of finite sequences ieee trans inf theory vol it no pp jan p li c liu k li d zheng c liu and y hou assessing the complexity of short term heartbeat interval series by distribution entropy med biol eng comput vol no pp jan p li c karmakar j yearwood s venkatesh m palaniswami and c liu detection of epileptic seizure based on entropy analysis of shortterm eeg plos one vol no art no e g c cawley leave one out cross validation based model selection criteria for weighted ls svms in proc ieee int joint conf neural netw proc jul pp r tibshirani regression shrinkage and selection via the lasso j roy stat soc b methodol vol no pp jan volume s kusmakar et al machine learning enabled team performance analysis in the dynamical environment of soccer l pappalardo p cintia a rossi e massucco p ferragina d pedreschi and f giannotti a public data set of spatio temporal match events in soccer competitions sci data vol no pp dec j a hanley and b j mcneil a method of comparing the areas under receiver operating characteristic curves derived from the same cases radiology vol no pp sep d r brillinger some data analyses using mutual information brazilian j probab statist vol pp dec t chen and c guestrin xgboost a scalable tree boosting system in proc nd acm sigkdd int conf knowl discovery data mining aug pp shitanshu kusmakar member ieee received the masters degree in clinical engineering from iit madras india in and the ph d degree in electrical and electronic engineering from the university of melbourne australia in he was a melbourne india postgraduate scholar with the university of melbourne he is currently a research fellow of complex data analytics with the school of information technology deakin university australia his doctoral research focused on developing an artificial intelligence ai based system for automated detection of epileptic seizures using a wearable sensing device his research interests include artificial intelligence machine learning cognitive computing health ai and time series analysis sergiy shelyag received the ph d degree from the university of g ttingen germany in he worked as a research fellow with the department of applied mathematics university of sheffield u k and with the department of mathematics and physics queens university belfast u k he was a future fellow with the department of mathematical sciences monash university australia he worked as a senior lecturer with the university of northumbria newcastle u k he is currently a senior lecturer with the school of information technology deakin university geelong vic australia his current research interests are in mathematical and computational modelling of complex physical processes data analytics machine learning and dynamical systems ye zhu member ieee received the ph d degree in artificial intelligence with a mollie holman medal for the best doctoral thesis of the year from monash university australia in he joined deakin university in as a research fellow of complex system data analytics he has been a lecturer with the school of information technology deakin university australia since his research works focus on clustering analysis anomaly detection and their applications for pattern recognition and information retrieval dan dwyer received the ph d degree from griffith university queensland he worked as a research active academic with the university of tasmania and the university of newcastle he worked as a sport scientist with the victorian institute of sport in melbourne he is currently a senior lecturer of applied sport science with deakin university his research focuses on the measurement analytics and prediction in sport he is a member of the centre for sport research paul gastin is currently a professor and the head of sport and exercise science la trobe university his teaching and research focuses on innovation in sport science and coaching to enhance the performance of people and organisations across the sport participation spectrum he worked in olympic paralympic and professional sport in australia and overseas holding senior positions at the victorian institute of sport the uk sports institute and uk sport he is an essa fellow and an accredited level sport scientist and high performance manager maia angelova received the ph d degree from the university of sofia she was a lecturer of physics with the somerville college university of oxford and a professor of mathematical physics with northumbria university u k until she is currently a professor of data analytics and machine learning with the school of information technology deakin university geelong vic australia she is also the director of the data to intelligence research centre and leads data analytics research lab deakin university her research interests include mathematical modelling data analytics time series machine learning and dynamical systems with applications to health a long range generalized predictive control algorithm for a dfig based wind energy system j s sol s chaves lucas l rodrigues c m rocha osorio and alfeu j sguarezi filho senior member ieee abstract this paper presents a new long range generalized predictive controller in the synchronous reference frame for a wind energy system doubly fed induction generator based this controller uses the state space equations that consider the rotor current and voltage as state and control variables to execute the predictive control action therefore the model of the plant must be transformed into two discrete transference functions by means of an auto regressive moving average model in order to attain a discrete and decoupled controller which makes it possible to treat it as two independent single input single output systems instead of a magnetic coupled multiple input multiple output system for achieving that a direct power control strategy is used based on the past and future rotor currents and voltages estimation the algorithm evaluates the rotor current predictors for a defined prediction horizon and computes the new rotor voltages that must be injected to controlling the stator active and reactive powers to evaluate the controller performance some simulations were made using matlab simulink experimental tests were carried out with a small scale prototype assuming normal operating conditions with constant and variable wind speed profiles finally some conclusions respect to the dynamic performance of this new controller are summarized index terms direct power control doubly fed induction generator flux oriented control generalized predictive control long range predictive control wind energy systems i introduction renewable energy production is worldwide accepted by academics and industries like an interesting alternative to traditional generation mainly due to the growing scarcity of fossil fuels and the dreadful effect of the greenhouse gases on the climate of earth a phenomenon known as global warming of all possible arrangements for wind energy systems the most commonly used is the double fed induction generator dfig based configuration mainly due to its control stability since it is possible to access to the stator and rotor windings to the partial power electronic converter sizing as the back to back converter only processes the of the rated power but keep in mind that the total cost of the system is much higher due primarily to the constant maintenance of the slip rings and the continuous replacement of the rotor brushes on the other hand the generalized predictive control gpc is a part of the model based predictive controllers mbpc family this theory was formulated by clarke et al in the late s its basic algorithm its properties and interpretations its possible applications and extensions were explained for himself and his co workers in the following years in the next decade for zhang et al presented an interesting gpc application in the synchronous reference frame for regulating the stator current of an induction machine im with experimental results but without a good dynamic response in steady state kennel et al in developed a practical optimized gpc with flux oriented control foc for an im using a speed control strategy the prototype can supply the demand for processing power considering the realtime conditions in a digital processor application and the consequent great calculation effort of the gpc algorithms in an interesting paper revisited the operating principle of mbpcs and identifies its constitutive elements the prediction model the cost function and the optimization algorithm summarizing the most recent research and providing details about the different proposed solutions then in mahmoud and oyedeji presents a special survey for adaptive and predictive control strategies for wind turbines which include the nonlinear mbpcs but without delving into the gpc strategy which is also part of that broader theory of the predictive controllers the gpc mbpc proposed here uses a new dfigs dynamic model based on a discrete transfer function and a controlled auto regressive integrated moving average carima model equation to make a stator power control by means of a long range rotor current prediction which will have an implicit integral term although this is a wellknown stochastic model its application to the dfig remained unpublished until now therefore the gpc control law must minimize a quadratic cost function that uses the past states of the rotor currents the n step ahead rotor current predictors and the rotor current references to estimate the next rotor voltages and thus controlling the stator powers at real time manuscript received may accepted july this work was supported by ufabc cnpq and capes recommended by associate editor yanjun liu corresponding author j s sol s chaves citation j s sol s chaves l l rodrigues c m rocha osorio and a j sguarezi filho a long range generalized predictive control algorithm for a dfig based wind energy system ieee caa j autom sinica vol no pp sept all the authors are with the department of energy engineering at the federal university of abc ufabc santo andr sp brazil e mail sebastian chaves ufabc edu br lucaslrodri gmail com cmrochaos hotmail com alfeu sguarezi ufabc edu br color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier jas ieee caa journal of automatica sinica vol no september some gpc formulations across the time has been focused on different control problems for wind energy systems such as the pitch angle control the mppt control the system stability etc also the use of different types of generators like the permanent magnet pmsg the squirrel cage induction generator sqig and even the switching reluctance generator srg were considered for all the above mentioned the main motivation of this research is to demonstrate that a long range gpc formulation with direct power control dpc strategy that uses a control horizon equal to the prediction horizon can govern the dfigs stator powers when generator is working under normal operating conditions and constant and variable wind speed profiles are assumed from all of these gpc formulations mentioned below the work of vieira dias et al proposes a gpc robust strategy that forcing to the control signal to follows its reference very quickly and without overshoot that rejecting disturbances and using for tuning the set point only a single parameter as is done in this paper the parameter that must be tuned via a heuristic method is called the controller weighting factor an mppt with a gpc for a pmsg based stand alone wind system was proposed by mohamed amine bouzid et al with a better performance than the classic pi controller however the proposed controller was evaluated only by simulation results thus for the long range gpc proposed here a set of experimental results are present to ensure its correct performance a predictive hysteresis current control for a srg drive was presented by r et al in this algorithm predicts the plant behavior considering the finite set of the power converter states and choosing the most suitable voltage vector so as to reduce the current amplitude error and to minimize a cost function a multiple vector predictive power control direct model for the grid side converter control of a wind energy system pmsg based using a full field programmable gate array was presented in in another predictive controller proposed by kou et al describes an optimized control action that can be directly applied to the power converter yielding a better controller performance and achieving a short computation time using a finite control set of variables with longer prediction horizons in addition new nonlinear controllers can deal mimo systems in the non triangular form as is the case of the dfig state space model using a fuzzy logic controller and a state filter or combining the back stepping recursive design with lyapunov function theory and thus resolve this stochastic finite time control problem therefore this new strategies could be used to improve the time response and the performance of the dfig based wind energy systems vrdq the long range gpc proposed here performs its control action estimating the rotor current predictors and then applying the proper voltage vectors in the dfig rotor terminals after the minimization of the quadratic cost function composed of the rotor currents and its references as well as the increment in the rotor voltage this long range gpc dq also requires constant switching frequency and constant sampling time these control parameters are shown in appendix b moreover some classical controllers as pi or nonlinear controllers like fuzzy adaptive and robust controllers etc precise of a big set of rules or a tricky matrix calculation in counter position this longrange gpc dq only requires one weighting factor to be adjusted via a heuristic procedure however what it can be an advantage can also be a disadvantage since the iterative trial and error procedure cannot ensure optimal value for this factor therefore the approach proposed by rodriguez et al which consists of an analytical method using the fake algebraic ricatti equation for the optimal tuning of this weight factor could be used to improve the dynamic response of this gpc dq for demonstrate all above mentioned about this long range gpc dq authors decides the following paper structure the dfig model based on carima model and the equations for calculating the generator discrete transfer functions are described in section ii next in section iii the gpc dq algorithm is explained in all detail including the rotor current predictors calculation the estimation of the past states of the system the cost function minimization and its corresponding predictive control law to evaluate the next component of the rotor voltage then simulation tests are depicted in section iv for fixed and variable rotor speed also including a comparison with a pi controller and a parameter variation test experimental test are presented in section v to corroborate the performance of this gpc strategy finally some conclusions about the dynamic performance of the new gpcdq are written in section vi ii the dfig model in the stator flux oriented reference frame v sq vs dq vsd sq s dq s sd the long range gpc for dfig uses the synchronous reference frame also named the dq frame therefore the dynamic model for the controller must be in the same reference as the plant i e the long range gpc dq must be fixed respect to the stator flux space vector just as the dfig model is this last is also known as the stator flux oriented control sfoc strategy therefore the dynamic model of the plant meets the following orientation conditions and x t d ird t dt x t d irq t dt x t ird t x t irq t u t vrd t u t vrq t according to a state space feedback model for dfig can be written in dq frame considering the stator and rotor currents and voltages due to the gpc algorithm explained below is focused on the rotor side converter rsc as is depicted in fig the rotor currents and rotor voltages are chosen as state and control variables respectively these can be written using the standard nomenclature for state variables for control variables therefore the space state equations for dfig as shown as follows x t rr lr x t slx t lr u t x t slx t rr lr x t lr u t sllm lslr s rr lr l m lslr lm sl where are the rotor resistance and rotor inductance is the total coupling factor is the mutual inductance and is the dfig slip angular speed in ap ieee caa journal of automatica sinica vol no september pendix b the dfig parameters are summarized r s however since an expression for the rotor voltage for this state feedback model is required in this paper authors decide to maintain the commonly used notation with and subscripts for the rotor and stator variables and also due to the need to use for obtaining the dfig model other variables of the machine such as stator and rotor fluxes therefore the above mentioned expression for rotor voltage can be derived according with as follows vr dq rr ir dq d r dq dt j sl r dq the use of the stator flux orientation method for the closed control loops allows the independent control of the active and reactive power of the stator by regulating the rotor currents since the dfig is connected to the mains by means of a backto back converter the control goal is to command the stator powers by applying the right voltage to the rotor terminals replacing the quadrature rotor flux component from in its possible to write an expression for the rotor voltages in terms of rotor currents in this way vrdq rr irdq d dt lm isdq lr irdq j sl lm isdq lr irdq isolating in the expression for direct stator flux component the stator current vector considering that the stator flux is constant and its derivative equal to zero due to the plant is oriented by the sfoc it is possible to rewrite the expression into two separate state equations therefore for d axis dird t dt rr lr ird t slirq t lr vrd t and for q axis di rq t dt slird t rr lr i rq t lr v rq t sllm lslr s t these equations are equivalent to and and it is possible rewrite it in two independent transfer functions by means of the laplace transform one for the d axis and another one for the q axis in this case the transfer functions become orthogonal due to the sfoc this is an additional advantage for the proposed gpc controller because a multiple input multiple output system mimo can be treated by two independent and totally decoupled single input singleoutput siso systems iii the gpc dq control algorithm n the long range gpc dq derived for dfig power control is described in following sections a gpc algorithm gives an integral action of the closed loop system and the present state is estimated using the model and some old known input and output data this nonlinear controller works with a constant sample time step equal to ms and its prediction horizon is for a better comprehension of the longrange gpc dq a fully explanation is done starting with the dfig carima model and the estimation of its associated polynomials in section iii a then the rotor current predictors estimation and the past states of the system are addressed in section iii b finally the cost function minimization and the predictive control law estimation are explained in sections iii c and iii d respectively a obtaining the dfig carima model coefficients for digital control applications a continuous transfer function must be discretized using for instance the ztransform therefore since that the plant is oriented by sfoc and can be discretized a simplified transfer function for d axis given by is g s rr s lr lr s rlrr ird s vrd s sl lr graphically the previous transfer function is presented in fig and its possible to see the cross coupling terms applying the z transform to this continuous transfer function two discrete transfer functions using the standards z transform tables can be obtained only a transfer function for the d axis is shown h z tlr e rr lrt z a z b z a z a z b z by means of the carima model from and the discrete transfer function a set of coefficients for and polynomials can be derived normally the prediction is calculated using instead of and since to the fact that for this gpc the polynomial is affected by two discrete time delays the coefficients for both polynomials can be estimated as follows a z a z a z and b z b z b z for is possible to write from and a z a z e rr lrt z e rr lrt z a z a z z a z a a e rr lrt a e rr lrt where with it can be seen that comparing with the coefficients for polynomial are and v rq vrd sl ls s l m sl lr sl lr r r s lr r r s lr irq ird fig a simplified ft for dfig sol s chaves et al a long range generalized predictive control algorithm for a dfig based wind energy system b z vrd vrq b t lr b t lr owing to that is a polynomial related with the rotor voltage components and as can be seen in and its coefficients can be estimated from and as follows and with the and coefficients already calculated a z b z it is now possible to determine the gpc dq rotor current predictors fig and fig resume the above estimation additionally in these figures the dfig variables necessary to realize this estimation are shown b the n step ahead current predictor calculation t vrd vrq n t when the dfig carima model is already done and supposing that at an arbitrary instant of time the rotor voltage components and will be well defined because the gpc dq operates in an incremental form as is demonstrated in these voltages will be kept constant for future steps of sample time therefore the gpc dq must n ird k irq k k be predicted these steps ahead and give the current predictors and with as the sample period this is presented in the light grey box of fig vrd k vrq k k n the general form for the rotor current predictors are commonly expressed by two parts the past states of the system and the part related to the future controls of the plant that is and hence the rotor currents at sample time can be written for sample steps in the future for the first future sample step n irdq k g vrdq k s vrqd k irdq k frdq k and for the sample step n i rdq k n gn vrdq k n sn vrqd k n irdq k n frdq k n dfig gearbox back to back coverter rsc gsc svm grid side control control law rotor current predictors estimation of parameters rotor side control discretization min j transformer grid ir abc i r r t i r dq t i r dq k v r dq k v r dq k n i r dq k n vs abc is abc s sl ps qs s m i r dq t v r dq t i r dq k v r dq k i r dq k vrd k v rq k sl sl v r r t vr r k v r abc v s abc is abc abc r dq dq r z z fig block diagram for the gpc dq with dfig based wind energy system mtm i mt gpc control law z past states of the system m z dfig plant a z rotor current prediction i rqd k dfig carima model polynomials z f rqd k n i rqd k n f rqd k n i rqd k n v vrqd k rqd k irdq k v irqd k n rdq k power estimator rotor currents generator b z ps ref qs ref fig control law and rotor current predictors estimation for the gpc dq ieee caa journal of automatica sinica vol no september nu nv nu nv n for this gpc dq the control horizon is equal to the prediction horizon it means that g g gm sl lr g z g g gm t s s sm g z sl lr s z s s sm t terms are derived from the step response of the model and the cross coupling term with these terms it is possible built a parameter vector correspondingly terms are parameters obtained from combining and thus another parameter vector can be obtained both terms and can be calculated as follows g s g b g b a b gm a gm a gm b sm b sm s s b g s a s b g sm a sm a sm b gm b gm where sl lr terms and are the past states of the system since frd frq an incremental form is used and the system is in steady state they are initially equal to zero therefore the expressions for these past states for the first sample instant can be estimated as follows frd k a frd k a frd k b frq k b frq k f rq k a frq k a frq k b frd k b frd k and for the sample instant in this way n f rd k n a frd k n a frd k n b frq k n b frq k n f rq k n a frq k n a frq k n b frd k n b frd k n for this long range gpc dq the past states are the dfig rotor currents finally it is possible rewrite separating d and q components in a matrix form to obtain the definitive expression for rotor currents predictors i rq k i rq k n ird k ird k n gs g s vvrd rq kk f rq k f rq k n frd k frd k n i rq k i rq k n ird k ird k n the first element of the is equal to zero because the s z cross coupling effect in dfig does not appear until the second sample instant occurs the rotor current predictors calculation and the estimation of the past states are graphically presented in fig c estimating the cost function j the gpc dq must be applying a control sequence that minimizes a quadratic cost function of the form j n m irqd k m i iqd k m t irqd k m i rqd k m vt rqd k i vrqd k n i n n i rqd k m where is the finite number of output prediction sample instants is the identity matrix is a weighting factor that must be adjusted by heuristic procedure and are the future reference currents the cost function is shown in fig and it can be seen the input variables to make possible the long range predictive control action and the output control signals rotor voltages increments which are necessary for the rotor current predictors calculation moreover in fig also appears the boxes representing the dfig carima polynomials and the past states of the system explained in sections iii a and iiib respectively d obtaining the control law vrdq k when the cost function of the future current errors is minimized the long range gpc dq provides a relationship for the next rotor voltage inputs based on the estimation of the control voltage increments vvrd rq kk mt m i mt i rd k i rd k n i rq k i rq k n frd k frd k n f rq k f rq k n ird k ird k n i rq k i rq k n where m gs g s i r dq k n fr dq k n k k n note that the predictive control law needs the rotor current references the past states of the system and the rotor currents in instant sample and the rotor current predictors in instant sample making up an extended column vector this is depicted in fig and presented in therefore the long range predictive control law estimates the new control rotor voltages for the gpc dq as follows v rq k vrq k vrq k vrd k vrd k vrd k detailed block diagrams for the long range gpc dq for its control law and for the rotor current predictors are depicted in figs and for a better comprehension of the algorithm developed here and explained in previous sections sol s chaves et al a long range generalized predictive control algorithm for a dfig based wind energy system additionally a flowchart illustrating this control algorithm is present in fig in appendix a iv simulation results computational simulations were developed using matlab simulink to evaluate the performance of the gpc dq four types of tests on normal operating conditions are considered fixed rotor speed variable rotor speed performance comparison and parameters variation a fixed rotor speed the rotor angular speed was fixed at rpm and step response for both powers were made these responses are presented in fig red signal is the reactive stator power and the dotted blue line represent its power reference the green signal and the dotted orange signal are the active stator power and its reference the settling time is very short and a detailed view of this is shown in fig the steady state error is equal to zero for both stator powers in fig the settling time for the active stator power is equal to ms b variable rotor speed for doing this test a variable wind speed profile are simulated the mechanical speed is increased from rpm sub synchronous speed through rpm synchronous speed to rpm super synchronous speed value red signal in fig shown this speed profile green and blue signals are the power responses dotted lines are the power references similarly to the precedent test the long range gpc dq can respond to the power references with a little and short overshoot and zero steady state error in the reference frame the rotor current for the r component is also shown its response and the phase inversions are in concordance with the speed profile this is depicted in fig near to s and to s two phase inversions can be observed frequency changes are also observed between s and s c performance comparison to demonstrate the superior performance of the gpc a comparative simulation is conducted between it and a pi controller using the same plant that is the same circuit model with the dfig parameters resumed in appendix b pi gains are adjusted following the procedure stated in i rq i rq in fig it can be seen the time responses for both controllers settling times for when a step variation is done in its reference at s red line are presented for the pi controller the settling time is equal to s grey dot line and for the long range gpc dq is equal to s black line while the pi controller takes seconds to stabilize the long range gpc dq just takes milliseconds therefore this predictive controller is faster than the pi controller despite its small oscillation around its reference in the steady state less than qs q s p s p s tempo s p w q var fig fixed speed stator power responses and ps t qs t s p s p s tempo s magnitude a fig detailed view for step response for ps t qs r q s p s p s rpm rpm rpm tempo s p w q var fig power responses and for the variable rotor speed test ps t qs t tempo s magnitude a fig alpha component for the rotor current for the variable rotor ir r speed test ieee caa journal of automatica sinica vol no september d parameter variation analysis rr lm lm to verify the robustness of the proposed long range gpcdq algorithm a of increment in the and parameters of the dfig is performed the values of the stator and the rotor leakage inductance are modified and consequently the values of and are affected too the power response for the q component rotor current is presented in fig again the gpc shows a faster response a detail of this settling time ms is shown in fig usually in dfig based wind systems an increase of more than in its parameters values is an alert to take out the generator to maintenance v experimental results some tests are carried out to validate the gpc dq in an experimental setup a kw rated power dfig was used coupled to a dc motor to emulate the mechanical wind power its parameters are present in the appendix b v i m the gpc dq was implemented using a digital signal processor dsp model tms f and electronic boards to acquire voltage current and speed signals the main components and its configuration are presented in fig a photograph for the dfig small scale system used for the experimental purposes is depicted in fig khz n the weighting control factor is equal to the pwm commutation frequency is equal to and the control horizon is equal to for all tests two types of different tests in normal operation conditions were made to validate this new controller fixed rotor speed time s i rq i rqgpc dq irqpi s s s amplitude a i rq i rqgpc dq i rqpi fig comparison between a pi controller and the gpc dq i rq i rq tempo s magnitude a i fig step response for parameter variations test rq t i rq i rq tempo s magnitude a i fig detailed view for the step response of rq t dfig rsc gsc dc motor c a d converter pwm dsp tms f d a converter scope pwm pulses hall effect sensors for v and i signals interface boards v s v i p q i s ir vr fig connection diagram for experimental setup sol s chaves et al a long range generalized predictive control algorithm for a dfig based wind energy system and variable rotor speed both of these are described in detail in the next sections a fixed speed test m rpm i rd t i rq t the rotor mechanical speed was fixed at both current references and are varying in a series of steps this steps are shown in fig ird t irq t i rd t i rq t in this test both currents and follows its references even when a little noise due to the back to back converter pwm commutation is observed the gpc dq step response is tested generating step signals that varies from a to a and from a to a for and from a to a for ird t for a better comprehension of these responses a detailed zoom for is shown in fig being possible to see the settling time equal to ms when its reference rises from a to a in ms approx i rq t ird t irq t another detailed response this time for is shown in fig while remains constant in a varies from a to a the same settling time is achieved in ms isa t ird t irq t analyzing the precedent figures it can be concluded that the gpc dq step response is fast enough for its application in dfig based wind power systems in both figs and it can be noted a change in the stator current phase a response when and varies the corresponding isa t vsa t r s ms amplitude for rises from a to a in fig while the stator voltage amplitude for phase a remains invariant its period changes from at to r s at ms in fig remaining invariant the phase a stator voltage frequency b variable speed test m ir r r ird t irq t for this test a variable wind speed profile was emulated using a variable dc motor mechanically coupled to the dfig a variation in the rotor speed was made from the subsynchronous speed at rpm passing by synchronous speed rpm and achieving a super synchronous speed at rpm in fig four signals can be seen the mechanical rotor speed the rotor current for a phase in its rotor reference frame and the rotor current components in synchronous reference frame ir r t ird t i rq t when the mechanical speed rises from rpm and pass up to its synchronous speed until rpm approximately at s a frequency inversion in the a phase rotor current can be observed while the rotor current components still remains constant when the speed descends again to rpm another little phase inversion can be seen when the speeds rise to sub synchronous to synchronous speed in the left side of the fig approx in s vi conclusion a new dfig discrete model was presented based on the carima model with this dynamic model a new long range generalized predictive controller was achieved this new gpc dq presented satisfactory experimental results when it dfig dc motor ac sources signal acquisition boards ac source ac source rectifier scope dsp back to back converter fig small scale laboratory setup a a a a a i rq ird i rd i rq mv mv mv mv s parar ms fig rotor fixed speed test step response for and ird t irq t rad s rad s v v sa i sa ird i rq a a a ms a v v mv mv parar ms ms fig rotor fixed speed test a detailed step response for ird t i sa v sa ird i rq ms a a a a a v a v v mv mv ms parar ms i fig rotor fixed speed test a detailed step response for rq t ieee caa journal of automatica sinica vol no september n was tested by means of a dsp system and a small scale laboratory experimental setup the dynamic responses for the power steps imposed to the system satisfied the requirement for a grid connected dfig wind energy system working under normal operating conditions and confirm that the gpc dq is totally applicable to these kinds of renewable energy systems the proposed algorithm had been explained in all detail although for simulation and experimental purposes the prediction horizon is restricted to a value no greater than but a flowchart for the general case is presented in the appendix a an additional advantage of this gpc strategy relative to other controllers such as pi lies in the fact that a single weight parameter needs to be adjusted however its value could not be the optimal one in simulation scenarios the dfig parameter variations were also tested again the gpc remains a null steady state error and a little and short overshoot the settling time for the power responses is similar to presented in the fixed and variable speed tests for the fixed step response test both the settling time and the steady state error are fast enough with a few of milliseconds of time near to ms and a percentage less than also when gpc dq is compared with a pi controller a faster settling time is observed in variable rotor speed conditions the gpc dq performance is adjusted to the requirements due to the fact that the rotor currents and therefore the stator powers remain constants and following the references imposed in addition the generator can provide active and reactive power to the grid when it passes from sub synchronous to super synchronous speed the pwm commutation noise present in experimental results is contemptible when dfig is linked to the power network and additional filters are not needed appendix a flowchart for the gpc dq the flowchart for the long range gpc dq is presented as a help to understand the iterative estimation of the rotor current predictors and the predictive control voltages for the rotor terminals n this flowchart was thought for the general case it means when the control and prediction horizons are equal to appendix b dfig parameters for experimental tests a kw dfig was used its parameters are shown in table i these parameters were determined according to the ieee standard additionally the weighting factor parameter and the pwm switching frequency are listed too same parameters are used for the simulation scenarios table i dfig and test bench parameters parameter value stator resistance per phase rs stator inductance per phase ls h rotor resistance per phase rr rotor inductance per phase lr h mutual inductance lm h synchronous stator speed s rad s pole pairs pp nominal active power ps kw nominal stator voltage vs y v nominal rotor voltage vr y v pwm sample reriod t ms control horizon n weighting control factor rpm rpm rpm rpm a a ird i rq i r r m a v v v s mv parar ms fig rotor variable speed test rotor currents response start choose n n n is n n is n n calculate g z and s z calculate the n n matrix mtm i calculate vrd t vrq t yes no yes no end while t ird and irq vrd t vrq t while t ts ird t and irq t with i rq and ird ird t ird t calculate v r dq t vrdq t fig flowchart for the gpc dq sol s chaves et al a long range generalized predictive control algorithm for a dfig based wind energy system references gwec global wind report annual market update tech rep global wind energy council gwec wind power is crucial for combating climate change tech rep global wind energy council w cao y xie and z tan wind turbine generator technologies intech open access publisher f blaabjerg and k ma future on power electronics for wind turbine systems ieee journal of emerging and selected topics in power electronics vol no pp r datta and v t ranganathan variable speed wind power generation using a doubly fed wound rotor induction machine a comparison with alternative schemes ieee power engineering review vol pp july f blaabjerg and k ma wind energy systems in proc of the ieee j a baroudi v dinavahi and a m knight a review of power converter topologies for wind generators renewable energy vol no pp d w clarke c mohtadi and p tuffs generalized predictive control part i the basic algorithm automatica vol no pp d clarke c mohtadi and p tuffs generalized predictive control part ii extension and interpretations automatica vol no pp d w clarke and c mohtadi properties of generalized predictive control automatica vol no pp l zhang r norman and w shepherd long range predictive control of current regulated pwm for induction motor drives using the synchronous reference frame ieee transactions on control systems technology vol no pp r kennel a linder and m linke generalized predictive control gpc ready for use in drive applications in proc pesc ieee nd annual power electronics specialists conf vol pp s vazquez j rodriguez m rivera l g franquelo and m norambuena model predictive control for power converters and drives advances and trends ieee transactions on industrial electronics vol pp feb m s mahmoud and m o oyedeji adaptive and predictive control strategies for wind turbine systems a survey ieee caa journal of automatica sinica vol pp march a linder r kanchan r kennel and p stolze model based predictive control of electric drives cuvillier m a bouzid a massoum and s zine generalized predictive control of standalone wind energy generation system international journal of renewable energy research vol no pp feb z zhang f g hui fang j rodrguez and r kennel multiplevector model predictive power control for grid tied wind turbine system with enhanced steady state control performance ieee transactions on industrial electronics vol pp aug d d r m morandin s bolognani and m castiello model predictive hysteresis current control for wide speed operation of a synchronous reluctance machine drive industrial electronics society ieee pp oct s v dias w a da silva l l dos reis and j c t campos robust generalized predictive control applied to the rotor side converter of a wind power generator system based on dfig in proc th ieee ias int conf on industry applications induscon pp ieee p kou d liang j li l gao and q ze finite control set model predictive control for dfig wind turbines ieee transactions on automation science and engineering s sui c l p chen and s tong fuzzy adaptive finite time control design for nontriangular stochastic nonlinear systems ieee transactions on fuzzy systems vol pp jan s sui s tong and c l p chen finite time filter decentralized control for nonstrict feedback nonlinear large scale systems ieee transactions on fuzzy systems vol pp dec a l l f murari j a t altuna r v jacomini c m rochaosorio j s sol s chaves and a j s filho a proposal of project of pi controller gains used on the control of doubly fed induction generators ieee latin america transactions vol pp feb c rocha osorio j s sol s chaves i r casella c capovilla j a puma and a s filho gprs egprs standards applied to dtc of a dfig using fuzzy pi controllers international journal of electrical power energy systems vol pp c m rocha osorio j s sol s chaves l l rodrigues j a puma and a s filho deadbeat fuzzy controller for the power control of a doubly fed induction generator based wind power system isa transactions vol pp l l rodrigues o a c vilcanqui a l l f murari and a j s filho predictive power control for dfig a fare based weighting matrices approach ieee journal of emerging and selected topics in power electronics vol pp june g abad j l pez m rodr guez l marroyo and g iwanski dynamic modeling of the doubly fed induction machine pp wiley ieee press a j s filho a l l f murari a nd et al a state feedback dfig power control for wind generation eletr nica de pot ncia sobraep vol pp march a l l f murari proposta de projeto de ganhos de controladores pi empregados no controle de geradores de indu o com rotor bobinado aplicados a sistemas e licos m s thesis ufabc santo andr sp brazil d r david model predictive control with integral action a simple mpc algorithm modeling identification and control vol pp i dogan microcontroller based applied digital control john wiley sons ltd ieee standard test procedure for polyphase induction motors and generators ieee std revision of ieee std pp feb ieee caa journal of automatica sinica vol no september j s sol s chaves received the b s and m s degrees in electrical engineer in and industry automation in from national university of colombia respectively he received the ph d degree in energy engineering from federal university of abc ufabc in he is a full time professor teaching in the areas of electrical machines renewable energies industry automation and power electronics his research interests has been focused on the control of doubly fed induction generators for wind energy systems smart grids and gis software for renewable energy applications using the sustainable development perspective and applying the following political and ethical principles the energy democratization the society decarbonization and the technological innovation for the benefit of the poorest people lucas l rodrigues received the bachelors degree in electrical engineering from federal university of bahia in salvador brazil in in he received the master degree in renewable energy and predictive control from federal university of abcufabc in santo andr brazil nowadays he is working at his ph d degree in the same area of knowledge his research interests are power electronics predictive control renewable energy doubly fed induction generators and meta heuristic optimization babichenko anatolii ntu khpi department of tsa and em kharkiv ukraine kravchenko yana ntu khpi department of tsa and em kharkiv ukraine babichenko juliya ukrsurt department of heat engineering and heat engines kharkiv ukraine velma volodymyr nuph department of pharmaceutical preparation technologies kharkiv ukraine german eduard ntu khpi department of tsa and em kharkiv ukraine mail kravchenko y o ukr net assessment of the forecast of the operating mode of the complex of ammonia secondary condensation production in uncertainty conditions abstract the article describes the analysis of conditions of secondary condensation complex functioning there the software algorithm for estimating the forecasts of functioning mode is developed which makes it possible to prepare the complex operator in conditions of existing uncertainty for making decisions in supervisor control mode keywords secondary condensation ammonia production forecast evaluation uncertainty conditions one of the most crucial stages of separation of ammonia synthesis of am series units operating in ukraine is secondary condensation which performs the final removal of commercial ammonia from the synthesis cycle previous studies have improved the energy efficiency of the secondary condensation stage by creating an optimal structure that ensures the exclusion of electrically driven turbocompressor refrigeration unit from the operational scheme and reducing the thermal load on low temperature evaporators lte technological complex of this structure is formed by additional heat exchanger ah condensing column c high temperature evaporator hte with steam jet refrigerating unit sjru and two ltes with waterammonia refrigerating units wacu functioning of the secondary condensation complex is done under conditions of uncertainty which is mainly due to constant changes in the external heat load due to the application of aircooled circulating gas g units at the previous stage of primary condensation and the lack of possection technical sciences sibility of continuous automatic control of ammonia concentration in cg at the same time the range of changes at the inlet of the complex temperature of the cg and ammonia concentration in the cg is c and respectively under such circumstances the cg temperature at the hte inlet which is included in the sjru operational scheme will also change therefore it is necessary to apply a control system to stabilize the cg temperature at the inlet at c and therefore stabilize the cg temperature at the hte inlet at c which was provided by the functional scheme of secondary condensation process control however the process equipment of the complex of secondary condensation is characterized by significant inertia which is due to its large metal intensity only the mass of cc is about tons at the same time sjru is a rather complicated technological system in which change of refrigerating capacity and consequently expenses of the coolant to hte are in general performed at the expense of changing the number of working ejectors therefore for the purpose of increasing control reliability and possibility of preparation of the operator for such changes in the supervisor control mode which is provided by the functional control scheme a subsystem of decision making support is necessary it should provide assessment of forecasts of possible changes in cg temperature at the inlet of hte setting of this temperature will condition the determination of forecast of necessary sjru refrigerating capacity and consequently of coolant costs to hte and monoethanolamine mea solution to sjru steam generator which provides necessary supply of working steam vapor for ejection of the coolant from hte in the process of algorithm development there have been used equations of mathematical description of ah heat exchange hte evaporator subroutine of calculation of heat transfer coefficients heat transfer and ammonia concentration in ah at the inlet and outlet of secondary condensation complex a nh in and anh p according to the developed algorithms stated in and located in stab and stoch files the algorithm contains convergence cycles which provide alignment of heat flows from the side of the pipe and inter pipe space and in the process of heat exchange ah to determine the temperature of cg at the outlet of its inter pipe space then according to this temperature the necessary cooling capacity of sjrus is sequentially determined which will condition the stabilization of ah temperature at the outlet of htes at the level of c coolant consumption in htes for its provision consumption of working ammonia vapor for ejection and consumption of iea solution for obtaining this vapor in sjrus steam generator at that the algorithm contains the following main functional blocks block call of the task to be solved in a certain period of time or by operators command block open the prog file that serves this task block a sub program for reading the necessary dani file information which receives and stores the current information about inlet and outlet variables and structural characteristics of the object obtained from the information management complex tdc block stab data reading sub program for determining heat transfer coefficients of actual k a and calculated according to formulas accepted at design k da block stoch sub program on fulfillment of stationary conditions process reproducibility and hypothesis on normality of empirical distribution and determination of functional dependencies for numerical estimation of volume concentrations of ammonia in cg at inlet a nh in and outlet a nh p of secondary condensation complex using matlab optimization toolbox package which according to research should be calculated using equations a f nh in ppc pc a f nh v v nhm icg p p lte a p nh in cg where p pc pcg are primary condensation pressure and cg at the ah inlet mpa pc p lte are primaryassessment of the forecast of the operating mode of the complex of ammonia secondary condensation production in uncertainty conditions condensation temperature and cg respectively at the outlet of lte c vnhm vip cg are volume flow rate of nitrogen hydrogen mixture nhm and cg at the inlet of ah nm s block determination of condensed ammonia flow rate m cd ah in cg flow of inter pipe space ah and total thermal resistance r t ea is done using the following equations m v a a cd ah ip cg nh in v nh out v a p nh p in v nh in cg a p nh p out v nh out cg r t k ea a ap ip a where a nh in v anh out v are respectively the concentration of ammonia vapour in the cg inlet and outlet of ah volume ratio pnh in pnh out is the partial pressure of ammonia vapor in cg correspondingly at inlet and outlet of inter pipe space of ah mpa pa ip a are the coefficients of heat transfer correspondingly from the side of pipe and inter pipe space calculated by krausold equations w m k block subprogram of calculating the functional dependence on the numerical estimate of uncertainty r ea using the matlab optimization toolbox software which should be searched by equation r f ea mcd ah block flow rate of cg m a kg s at cc outlet is determined by the following formulas m v a a a cd cc ip a nh in nh p nh p g v a a ev s nhm nh p nh p m m g v cc cd cc ev s m m m ap ip a v cc where m cd cc gev s mvcc is the flow rate of ammonia respectively condensed into cc obtained by evaporation during the heat exchange with nhm and liquid ammonia from cc kg s vipa vnhm are respectively the volume flow rate of cg at the ah inlet and nitrogen hydrogen mixture at the cc inlet nm s block setting the initial temperature approximation p cg p cc at the outlet of the ae pipe space with determination of the heat flow pa from the ae pipe space by the following formula ap p ag p ag p cg p cc l ain l ain v m c m i i aout where m p ag m lain are respectively the mass flow rate of cg gas mixture at the outlet and liquid ammonia at the outlet of the ae pipe space kg s c p ag is the average heat capacity of cg gas mixture kj kg k ilain ivaout are respectively the enthalpy of liquid ammonia at the outlet and ammonia vapor at the outlet of the ae pipe space kj kg cc p c is the cg temperature at inlet of ae pipe space which is provided by cg temperature at cc inlet at the following level ht cge c and cg temperature at lte outlet at the following level p lte c approximation step c block setting the initial temperature approximation cg hte ip cg at the outlet of the ae inter pipe space with determination of the amount of condensed ammonia m cd a according to equations and heat flow ip a from the ae inter pipe space according to the following formula ip a ip ag ip acg ie cg cg hte cd a in m c m r a m m la cd a cla cg ip ht cge where m ip ag mcd a m la are respectively the amount of gas mixture at the outlet of the ae inter pipe space condensed and liquid ammonia space in the ae inter pipe space kg s c ip acg c la are respectively the average heat capacity of gas mixture of ae inter pipe space and liquid ammonia kj kg k rip a is specific heat of condensation in the ae inter pipe space kj kg cg ip is the temperature of cg at the inlet of the ae inter pipe space c block estimation of error margin of convergence condition a of heat flows pa and ip a and transition in case of its fulfillment to calculation of heat flow at due to heat exchange according to the following formulas section technical sciences at ae a mn k f a k r ae ap ip a t ea where f a is the surface of heat exchange of ae m mn a is the log mean temperature difference of ae c block estimation of error margin of convergence condition a of heat flows pa ip a and ta and in case of its fulfillment given determined temperature ht cge transition to the calculation of m in hte kg s at the inlet of hte inter pipe space according to the equation m m c m r m m in hte p hte p cg cg hte cg hte cd hte cd l hte c ht d e lhte ht cge ht cge cl cl cl c r where m p hte mcd hte m lhte is the flow rate of gas mixture condensed ammonia and liquid ammonia respectively in the pipe space of the hte kg s c pcg c l hte are the average heat capacity of gas mixture and liquid ammonia respectively in the pipe space of the hte kj kg k rcd rcl is heat of ammonia condensation and vaporization in pipe and inter pipe space of hte kj kg cl is coolant ammonia temperature at hte inlet c cl is the coolant boiling point in inter pipe space of hte c block calculation of the flow rate of working ammonia vapor mv to sjru ejectors mea solution flow m to produce this vapor as well as the total amount of coolant vapor and mttl working vapor to air condensers according to the following formulas m m r c v v m m u v in hte m m m ttl v where r v is the specific heat of ammonia vaporization at c and pressure of mpa kj kg c is the specific heat capacity of mea solution kj kg k c c is the temperature of mea solution at inlet and outlet respectively u ejection coefficient block formation of pspr current data array of the decision making subsystem in particular mv m mttl and formation of results block closing the prog file and task termination table shows as an example separate results of calculations according to the given algorithm implemented in the matlab r a software table separate results of assessment of forecasts of the subsystem of support for management decision making in the conditions of changes in cg temperature at the secondary condensation complex inlet with ammonia concentration in cg at the inlet of vol performance data cg temperature at the secondary condensation complex inlet cg ip c additional heat exchanger ah heat flow at mw heat transfer coefficient k ae w m k temperature at the outlet of the inter pipe space cg hte c temperature at the outlet of the pipe space cg p c assessment of the forecast of the operating mode of the complex of ammonia secondary condensation production in uncertainty conditions amount of condensed ammonia in the inter pipe space mcd a t h log mean temperature difference mn a c high temperature evaporator and sjru heat flow hte mw amount of condensed ammonia in the pipe space m cd hte t h coolant quantity at the inlet of the inter pipe space m in hte t h quantity of working ammonia vapor to sjru ejectors m v t h mea solution quantity for working vapor m t h the developed algorithmically programmed software of the decision support subsystem allows to predict the modes of its functioning for such a complex inertial object with high metal intensity as a complex of secondary condensation it gives an opportunity to prepare the complex operator in conditions of existing uncertainty for decisionmaking in the supervisor control mode in order to stabilize the temperature conditions of circulating gas cooling references babichenko a velma v babichenko j kravchenko y krasnikov i system analysis of the secondary condensation unit in the context of improving energy efficiency of ammonia production eastern european journal of enterprise technologies vol p doi fuzzy inverse model control invmc of an underwater vehicle andrzej piegat marcin pluci ski technical university of szczecin institute of computer science and information systems ul o nierska pl szczecin poland phone fax email andrzej piegat ii tuniv szczecin pl marcin plucinski ii tuniv szczecin pl abstract the imc structure often used in fuzzy and neural control requires both a model of the plant and its inversion the paper presents how a more parsimonioues structure using only the model inversion invmc can be designed and how the inversion can be found the invmc structure was applied for control of the underwater vehicle uv krab ii introduction for control of nonlinear systems the imc structure described eg in and shown on fig a is often used a b a wp anti windup figure the imc a and the invmc b structure its advantage is internal stability of the whole system when both the plant and the imc controller separately are stable it refers also to nonlinear plants and controllers as fuzzy and neural ones the imc structure enables to approach the so called perfect control the same can also be shown for the invmc structure which is simpler the common advantage of both structures is a small number of parameters to be tuned in the imc it is usually only the filter time constant and in the invmc the pre controller gain in the pid control gains must be tuned what considerably complicates the synthesis in the literature not many application examples of the pure invmc structure can be found usually the model inversion is combined with the model or with other controller types as eg with a predictive or pd controller below an algorithm shortly because of the paper limitation for the invmc system will be presented it is described precisely in the book fuzzy modeling and control by piegat being prepared for printing in polish design algorithm of invmc systems designing of invmc systems consists of two main steps i design of the inversion f inv of the controlled plant f ii structure choosing and tuning of the pre controller lets assume the plant realizes the nonlinear input output mapping the ideal inversion f inv would be the one realizing dynamic requirements determined by the reference model fig because in real dynamic systems it is impossible as real reference models can be assumed eg inertia ones as given by figure inversion design in the system with reference model for c one gets fast inversion for c slow one the fast inversion has usually strong differential action and generates great control signals m often greater than saturations of the setting unit with c we can decrease and slow down the control signal and increase the system robustness however the same we can get with an appropriately slow pre controller therefore below will be used fast inversion c described by if the inversion is accurately tuned fig then k what implicates replacing in according to the variables k i by e k n m i i n m and then inverting it we get inversion condition of its high accuracy is a good chosen structure rule base membership functions etc and good tuning what can be made eg in the system with reference model fig where the plant f is replaced by its model f error backpropagation also other possibilities for the tuning given in can be used when the inversion is accurate we can replace on fig b the connection f inv f by the inversion reference model p fig figure system with reference model enabling design of the pre controller gc this way the pre controller design can be made basing on linear control systems methods for many plants of the type n m good control quality is achieved with integral pre controller given in discrete version by q r r ts to increase the control quality application of anti windup fig b is recommended the gain kc is the only parameter tuned in the system its value can be pre calculated basing on the reference model gref which should not be mistaken for the inversion reference model p fig it can be assumed as inertia of the order n m becauseboth the plant and the inversion have nonlinearities and the setting unit has saturations the gain kc is finally tuned experimentally in the system with the real plant f or its model f experiment the invmc was applied for uv krab ii scheme of the controlled plant f is shown on fig a figure controlled plant with partial correction of the propellers nonlinearities stabilizing feedback ks a and propellers static characteristic b the nonlinear dynamics of the uv can be approximated by transfer function where course angle deg m propeller torque nm the transfer function coefficients depend on the uv velocity experiments realized by pluci ski have shown that the nominal function has coefficients k p deg s nm and tp s fuzzy inversion f inv of the plant f from fig was determined by inverting of its fuzzy model f what can not be shown because of the paper limitation and next tuning it in the system with the reference model fig at z z membership functions for inputs and outputs are shown on fig p p p p p q r s t s s q s s q s q q s figure membership functions of the inputs e k e k e k and of the output m k of fuzzy inversion f inv b membership functions in the inversion are based on extrapolation truth described by piegat in owing to this the inversion output m doesnt saturate and the uv can be rotated at arbitrarily big angle the inversion rule base is given in tab and rule base rules r r if e k n and e k and e k then m k where and product operator tab rules r r of the model inversion f inv k k n n n n p p p p n n n n n p p p p p n n n n p p p p rules r r if e k p and e k and e k then m k tab rules r r of the model inversion f inv k k n n n n p p p p n n n n n p p p p p n n n n p p p p analysis of the rule base shows its monotonicity in the invmc system a pre controller according to was applied its gain kc was determined experimentally to get possibly short setting time at small overshoot and quick disturbance rejection the optimal value is kc bigger gains give not shorter control time with much stronger oscillations fig a and b present results of experiments realized with the described control system the given course was set into deg and deg fig a and b present the reaction of the system on the disturbance acting on the input of the uv time second time second figure uv course angle a and the propeller torque b for the fuzzy invmc system it was found that for the gain taken for experiments kc the system work stable in a broad interval of uv transfer function coefficients variation kp tp time second time second figure uv course angle a the disturbance and the uv propeller torque b for the fuzzy invmc system reaction on the disturbance d nm conclusions the fuzzy invmc system proved its efficiency in practice the most important advantages of the described system are the easiness of its tuning and the possibility of stability proving during simulations which results are presented on fig and uv propellers worked for a long time with maximum values and without big number of overswitchings this facts causes that the system works with short control time fig a and b show that the system has good possibility of compensating disturbances acting on the input of the uv references r babuska j sousa h b verbruggen model based design of fuzzy control systems proceedings of eufit august aachen germany m brown c harris neurofuzzy adaptive modelling and control prentice hall k j hunt and others neural networks for control systems a survey automatica vol pp m morari e zafiriou robust process control prentice hall d neumerkel f lohnert artificial neural networks state of the art in automation automatisierungstechnische praxis no pp in german a piegat extrapolation truth proceedings of eufit september aachen germany pp m pluci ski adaptive course angle control system of unmanned underwater vehicle with fuzzy knowledge base about the plant dissertation technical university of szczecin poland in polish j sousa r babuska h b verbruggen adaptive fuzzy model based control proceedings of eufit august aachen germany pp d wloka and others neural networks in robotics automatisierungstechnische praxis no pp performance analysis of hybrid fuzzy pid controller action on boiler drum level control abstract boiler unit plays a vital role in power plant and controlling the boiler drum level is one of the critical operations nowadays fuzzy logic controller or conventional pid controller is designed in matlab simulation and further prototype is implemented using embedded technology hybrid platform is required in power plant to overcome this and to get better solution intelligent controller fuzzy logic is used to tune the conventional pid controller automatically in online process hybrid fuzzy pid controller result proves that it offer better performance in terms of settling time rising time steady state error than conventional pid controller in boiler drum level control the simulation results are achieved by using labview fuzzy pid tool keywords boiler drum level soft computing hybrid fuzzypid controller i introduction in power plants boiler drum is a closed vessel in which steam is produced by burning coal in the furnace the main input variable of the boiler drum are fuel feed water and air the outputs of the boiler system are electrical power steam pressure steam temperature and flue gas as shown in fig according to these parameters the drum level may increase or decrease the boiler drum level should be monitored continuously it should be within a certain limited value if the measurement of boiler drum level is not within the limited value water carryover may be occurred if the level is low in drum overheating occur in boiler water tube this cause the damages in boiler drum surface and creates the cracks in the surface if the value of level in boiler drums more than the limited value it may transfer the moisture into the turbine which reduce the boiler efficiency fig basic block diagram of drum level control p meenu is described the boiler drum level control which is done by fuzzy logic controller if the water level is too low in boiler drum it leads to boiler explosion if the water level is very in boiler drum it cause damage to turbine due to the affect of separator steam water serious consequences occur in high or low level so we must take strict control conventional proportional integral derivative controller is popular three element control if there is any process disturbance the element control does perform well due to the lack of proportional controller gain knowledge the collected data from the pid controller is used to gain the knowledge on the intelligent control technique and developed fuzzy logic control wang zhuo is illustrated the water in the boiler drum has a nonlinear characteristics large time varying strongcoupling and multivariable using mathematical model of boiler drum fuzzy control theory is designed for controlling the boiler drum level system both fuzzy logic and conventional logic are used in controlling the drum level in matlab software verification and simulation are done and get the result of drum level system by observing that static and dynamic characteristics of fuzzy controller is improved and also best real time control of boiler drum level is achieved k ghousiya begum explained the boiler drum level controlling using an intelligent model in three element boiler drum the parameters are determined using pid tuning methods like ziegler nicholas method tyreus luyben method and internal model control in which imc used to tune the pid controller keyur solkanki describes the approach for controlling a very crucial parameter of boiler level of the boiler drum using pid controller imc based pid tuning method is used with feedwater and feedback strategy is used to control two element drum level besides also the modeling of the process for level control and implemented it in simulink hardware model has also been developed and proved open loop validation for theoretically derived model and practical model and simulation results k padma priya described the boiler drum level control using labview software the boiler parameters are measured and controlled using embedded system through labview water droplet identifiers are used to check the steam dry or wet dry steam is required for power production if wet steam passed to turbine it will damage and production is affect water droplets identifiers detect the wet steam and converted into dry steam gowthaman e prasanna moorthy v saravanan s naveenbalu k aravind s naveen s assistant professor eie dept hindustan college of engg tech gowthameie gmail com assistant professor sr grade electrical engineering government college of technology prasanna gct ac in ug scholar eie dept hindustan college of engg tech coimbatore ieee online international conference on green engineering and technologies ic get shaoyuan li presented the paper about the new development of the boiler turbine coordinated control strategy using fuzzy reasoning and auto tuning techniques boilerturbine system is a very complex process due to multivariable nonlinear slowly time varying plant with large settling time and a lot of uncertainties the main stream pressure control loop and power output are strongly coupled in boiler turbine unit the automatic coordinated control of two loops is very challenging problem the gaussian partition a special subclass of fuzzy inference is used to self tune the main steam pressure pid controllers yu daren analyzed the work about possibilities of applying feedback linearization techniques to the non linear control of super heat steam pressure and power output of boiler turbine generating unit computer simulation is used to design and evaluated the nonlinear coordinated controller the simulation results are helps to compare the proposed strategy with conventional strategy improvements of nonlinear control system are observed by the results zaiyi liao describes the optimize control of the boiler in multi zone heating system by an inferential model based predictive control scheme to save energy and to improve thermal comfort it has only three inputs they are outside air temperature total solar radiation falling on the exterior building and temperature of water in the boiler drum inputoutput data are collected from the portable temperature loggers to estimates the parameter for the modeling the simulation results show that overall performance of the heating system is improved compared to the conventional boiler control scheme gowthaman e designed self tuned pid controller for controlling speed of pmdc motor with fuzzy rules fuzzy rules are framed based on two fuzzy inputs error change in error and three fuzzy outputs such as proportional gain kp integral gain ki derivative gain kd dynamically pid controller parameters are updated through fuzzy interference system in platform of labview fig closed loop control for boiler drum level control the above block diagram fig presents the overview of proposed boiler drum level control which consists of the following one major stage the hybrid fuzzy pid with self tuning technique i e if and then fuzzy control or rule base according to the error and change in error that used to provide necessary actions to achieve optimal response through the auto selection of pid controller parameters ii mathematical modeling of boiler drum the modeling of boiler drum is described based on steam drum valve transfer function for steam drum is described in equation fig modeling block diagram of boiler drum s s s g s p transfer function for valve s g pv s s s the equation shows the series of process transfer function and valve transfer function from the fig s g pv s s s the transfer function of the boiler drum is s g pv s s s iii design and implementation of hybrid fuzzy pid controller in industrial control systems widely used controller is pid controller which is a generic feedback control loop mechanism error value calculated by the pid controller is the difference between a measured process variable pv and a desired set point sp the steady state error of any process could be eliminated by pid controller through the integral action on error and expecting output changes through derivative action on the error with respect to the set point in pid controller algorithm the output y t and error input e t are related by the following equation p d i de t y t k e t e t dt t dt t where kp proportional gain ki integral time and kd derivative time for tuning the three model controller parameters kp ki and kd several methods are available currently the parameter of three mode p i d controller are adjust manually by tuning methods like ziegler nichols method for closed loop system and process reaction curve method for open loop system online international conference on green engineering and technologies ic get using the some mathematical analysis above mentioned common methods are applicable for manual tuning process for some selecting precise value of controller parameter these manual tuning is not appreciable for eliminating the steady state error of any process to rectify such problems natural system are combined with intelligent agent to get better solution in this developed system fuzzy based rule is given to the pid controller for precise and faster optimal response it helps to achieve desired set point with minimum rise time and settling time than conventional pid controller the basic structure of hybrid pid controller is shown in fig fig basic block of hybrid fuzzy pid controller the relation between fuzzy input parameter error and change in error and fuzzy output parameter kp ki and kd are given by the hybrid fuzzy pid controller which is shown in figure by the principle of fuzzy self tuning the three parameters kp ki and kd are modified in order to achieve control actions if there is any update in error and change in error for various level as set point fig basic structure of hybrid fuzzy pid the error and change in error values are determined from previous set point values and present set point values to perform good dynamic steady state and static performance without any disturbances five fuzzy labels nl ns ze ps and pl are framed for fuzzy input parameters such as error and change in error values which are mentioned in table i similarly six fuzzy labels pvs ps pms pm pl pvl are framed for fuzzy output parameters as represented in table ii the triangular membership function is used for both input variables e ec and output variables kp ki kd as shown in the figure a b c d e table i fuzzy set values for inputs fuzzy label description ze negative large ps negative small pl zero ze positive small ps positive large pl negative large in the developed fuzzy system the pid control parameters has been modified as follows kp k p kp ki ki ki kd kd kd table ii fuzzy set values for outputs fuzzy label description pvs positive very small ps positive small pms positive medium small pm positive medium pl positive large pvl positive very large a b online international conference on green engineering and technologies ic get c d e fig fuzzy control of mfs a error e b change in error ec c proportional gain kp d intregral gain ki e derivative gain kd the twenty five rules are formulated for the developed fuzzy logic control system inputs and outputs are related as shown in fig the working flowchart proposed drum level control is shown in fig few of the rules are listed below rule if error e is negative large nl and change in error ec also negative large nl then change in proportional gain kp is positive very large pvl and change in integral gain ki is positive medium pm and change in derivative gain kd is positive very small pvs r if e is ns and ec is nl then kpis pvl and ki is pm and kdis pms r if e is ze and ec is nl then kpis pvl and ki is pm and kdis pm r if e is ps and ec is nl then kpis pvl and ki is pm and kdis pl r if e is pl and ec is nl then kpis pvl and ki is pm and kdis pvl fig output variable kd versus the input variable error e and change in error ec fig flowchart of closed loop drum level control iv experimental setup this section is described simulation of conventional pid controller and hybrid fuzzy pid controller action on drum level control conventional pid controller existing approach parameters are tuned by using zeiglar nicholas tuning methodology the proposed approach fuzzy tuning is achieved by trial and error method a conventional pid controller action on drum level control fig conventional pid controller simulation block diagram window the above simulation block diagram fig describes the conventional pid controller based on the boiler drum level control it consist of the transfer function of the boiler conventional pid controller and their parameter like proportional gain kp integral time i and derivative time d online international conference on green engineering and technologies ic get a b fig simulation result of pid controller front panel window a before settling point b after settling point in conventional pid controller the set point cm is attained after seconds it takes more time to settled due to overshoot present in the simulation results fig a b b hybrid fuzzy pid controller action on drum level fig hybrid fuzzy pid controller block diagram window the above block diagram window fig describes the simulation block of hybrid fuzzy pid controller based on the boiler drum level control the simulation block is designed using boiler drum model parameters membership functions and rule viewer for input and output variables are available in the fuzzy tuned controller fig simulation results for hybrid fuzzy pid controller in hybrid fuzzy pid controller simulation results there is no overshoot using logic rules fuzzy controller is designed to rectify the overshoot which is occurred in the conventional pid controller also settling time also decreased in hybrid fuzzy pid controller here set point cm is settled at seconds as shown in fig table iii tentative results of pid controller based drum level control set point level settling time ts s rise time tr s steady state error in cm of level of level of level of level table iv hybrid fuzzy pid based drum level control set point level settling time ts s rise time tr s steady state error in cm of level of level of level of level c comparison of conventional pid and hybrid fuzzy pid controller performance on drum level control the designed hybrid fuzzy pid controller performance is investigated on various parameters settling time rise time and steady state error under various set point drum level ranges along with conventional pid controller performance tentative results are consolidated in table iii table iv respectively online international conference on green engineering and technologies ic get fig hybrid fuzzy pid and conventional pid controller performance on settling time the comparison of hybrid fuzzy pid and conventional pid controller performance on settling time is shown in fig the performance of both digital controllers are investigated for the setpoint ranges cm cm cm and cm of drum level with their settling time from the graph it is clear that hybrid fuzzy pid controller to boiler drum level the response settled to cm of drum level quickly at the time instance of seconds whereas conventional pid controller obtained seconds i e s more than that to settle due to improper selection of pid controller parameters fig hybrid fuzzy pid and conventional pid controller performance on rising time similarly hybrid fuzzy pid controller and conventional pid controller responses are compared with different drum level ranges for rise time from the above fig it is well known that the rise time of hybrid fuzzy pid response is less than conventional pid controller fig hybrid fuzzy pid and conventional pid controller performance on steady state error the fig is illustrated the steady state error sse comparison of both conventional and hybrid pid controllers with various drum level ranges such as cm cm cm and cm the steady state error of hybrid fuzzy pid controller is constant zero cm for various operating level ranges whereas conventional pid controller is obtained s s s s respectively v conclusion the conventional pid and hybrid fuzzy pid controller techniques are successfully implemented for closed loop control of boiler drum level control system the performances of two different controllers are analyzed by the investigation of settling time rise time dead time and steady state error with simulation results from the labview platform based simulation results it is concluded that the hybrid fuzzy pid parameters are tuned automatically to meet the desired response and also results are short listed at various set point of the level the hybrid fuzzy pid controller offers the better performance control over the conventional pid controller while comparing with conventional pid controller performance hybrid fuzzy pid controller offers better dynamic response shorter settling time rise time and zero steady state error references p meenu and g priya fuzzy logic based boiler drum level control and production system in international journal for research and development in engineering pp wang zhuo jilin wang shichaoa and jiang yanyan simulation of control of water level in boiler drum in world automation congress pp zaiyi liao and arthur l dexter an inferential model based predictive control scheme for optimizing the operation of boilers in building space heating systems ieee transaction on control system vol no pp september yu daren and xu zhiqiang nonlinear coordinated control of drum boiler power unit based on feedback linearization ieee transaction on energy conversion vol no pp march shaoyuan li hangbo liu wen jian cai yeng chai soh and li hua xie a new coordinated control strategy for boiler turbine system of coal fired power plant ieee tranaction on control system vol no pp keyur solanki jalpa shah nishith bhattt modeling and simulation of prototype of boiler drum level control international journal on mechanical engineering and robotics vol no pp k ghousiya begum d mercy h kiren vedi and m ramathilagam an intelligent model based level control of boiler drum international journal of emerging technology and advanced engineering vol no pp january k padma priya p naveen kumar monitoring and controlling of boiler drum parameters using lab view international journal of innovative research in science engineering and technology vol no pp may e gowthaman and cd balaji self tuned pid based speed control of pmdc drive in ieee international multi conference on automation computing control communication compressed sensing kerala india pp march machine learning for credit card fraud detection lu s fernando torres lu s fernando torres follow min read mar introduction from food delivery apps to online clothing stores the internet made it easier to purchase whatever we want whenever we want with the convenience of using our credit cards to do so credit cards are useful for a bunch of things it saves us from the inconvenience of having to carry large amounts of cash with us to each place we go and it also allows us to advance a purchase that can be paid over time according to statista the number of worldwide transactions rose from billion to billion per year from to considering that we have billions of transactions happening daily all over the world and that people are using their credit cards more than ever through cellphone apps online stores and using digital wallets it isnt hard to imagine that there are fraudsters looking for ways to make purchases using somebody elses name and money in a scenario such as this one it is extremely important that credit card companies are able to identify fraudulent transactions in order to avoid charging customers for items they didnt purchase and thats why i developed a machine learning classification model to teach a computer how to identify patterns that can tell when a transaction is in fact genuine or a fraud for this project i used pythons scikit learn library and tested four different classification algorithms to identify which one of them would achieve the best results with our dataset machine learning briefly machine learning is a branch of artificial intelligence and it focuses on the use of data and algorithms to teach a computer to imitate the human way of learning improving its accuracy gradually through experience and performance of tasks machine learning is an important component of data science and it uses statical methods and trains algorithms to perform tasks such as predictions and classifications based on some input data that allows the algorithm to produce an estimate about a pattern in the data if you wish to know more about machine learning and its use click here to read an ibm article on the matter scikit learn scikit learn is an open source machine learning library for python that offers simple and efficient tools for classification regression clustering dimensionality reduction data preprocessing comparisons validations and parameters choosing for different algorithms classification algorithms for this project i used four different classification algorithms to perform the task of identifying patterns that make up fraudulent transactions i used the decision tree classifier which is a simple algorithm used to predict the value of a certain target variable through the learning of simple decision rules inferred from the data features i also used the random forest classifier ada boost classifier and gradient boosting classifier algorithms based on ensemble methods which combines predictions of several base estimators to improve robustness over a single estimator the random forest classifier for instance is based on the construction of hundreds of random decision trees in order to get into a final result ill leave a link below for the scikit learn online documentation where youll be able to read and acquire in depth explanations on classifiers how they work and the math behind them credit card fraud detection project development this project was developed in python language in jupyter notebook and published on github and kaggle the first step of course is importing all the necessary libraries for development libraries for exploring handling and visualizing dat import pandas as pd numpy as np matplotlib pyplot as plt seaborn as sns plotly express as px sklearns preprocessing library from sklearn preprocessing import standardscaler importing train and test data split from sklearn model selection import train test split sklearns metrics to evaluate our models from sklearn metrics import accuracy score precision score confusion matrix recall score f score classifiers from sklearn ensemble import randomforestclassifier from sklearn ensemble import adaboostclassifier from sklearn ensemble import gradientboostingclassifier from sklearn tree import decisiontreeclassifier setting theme style and color palette to seaborn sns set theme context notebook style darkgrid palette muted a understanding the dataset i then used pandas read csv method to obtain our data and used df head to see the dataframe the dataset used for this project was the credit card fraud detection dataset available on kaggle and it contains credit card transactions that were made during the month of september by european clients during two days it has transactions and variables the variable time contains the seconds elapsed between each transaction and the first transaction in the dataset amount contains the value of each transaction and lastly we have class which is a binary feature that tells us if that certain transaction was genuine or a fraud we also have other features v v v that are numerical inputs result of a pca transformation whose content couldnt be displayed due to their confidential nature during exploration analysis it was possible to see that the dataset contains only numerical inputs and no null values were present i proceeded to an analysis on the amount feature with the describe method statistics on the amounts df amount describe round it is possible to see that of transactions were below and the highest transaction amount was much higher than the average amount of lets plot amounts on a scatter plot to see how these values are distributed in the dataset distribution of amount fig px scatter df x amount y df index color df class title distribution of amount values fig update layout xaxis title transaction amount in yaxis title transactions fig show png it looks like most transactions are genuine represented by the blue dots we can also see that all high value transactions were in fact genuine with no apparent fraudulent transaction being made above it seems however way too difficult to see any fraudulent transaction on the scatter plot those were supposed to be represented by yellow dots and all i can see are some small yellow colors behind all that blue on the left this leaves us with a question how many transactions were in fact fraud i used plotly to plot a pie chart to see how our classes are distributed in the dataset and this below is the result ive gotten visualizing class distribution fig px pie df class values df class value counts names genuine fraud title fraudulent x genuine transactions in the dataset fig show png df class value counts well no wonder it wasnt easy to see fraudulent transactions on the scatter plot only of transactions were fraudulent thats out of transactions we have a huge class imbalance to work on here lets try to get some more information on fraudulent transactions df query class amount describe the highest fraud amount was on average fraudulent amounts costed around lets plot these values on a scatter plot once again distribution of fraudulent transactions amount fig px scatter df query class x amount y df query class index title distribution of fraudulent amounts fig update layout xaxis title transaction amount in yaxis title transactions fig show png preparing data the time variable wont be any useful for this project so i dropped it from the dataframe df df drop columns time axis after doing that i divided the dataset into the independent variables x and the target variable y x df drop columns class axis y df class visualizing target variable class y visualizing independent variables x after doing that it is time to split our data into training set and testing set ive split them into for training and for testing train x test x train y test y train test split x y test size random state print x train size train x shape print x test size test x shape print x test proportion s round len test x len train x len test x print y train size train y shape print y test size test y shape print y test proportion s round len test y len train y len test y we now have transactions for training and transactions for testing after that i used standardscaler to normalize amount values since these values were way too discrepant when compared to the other features in the dataset it is important to have all variables in a similar scale because otherwise the algorithm will attribute a heavier importance to the variable that has the biggest scale which will end up in a biased algorithm and negatively impact our results we dont want that standardscaler is actually pretty simple and it standardizes a feature by subtracting the mean and then dividing values by the standard deviation in order to avoid the data leakage effect it is important to apply standardscaler on each training and testing set separately scaling data on the training set scaler standardscaler train x amount scaler fit transform train x amount values reshape train x scaling data on the testing set scaler standardscaler test x amount scaler fit transform test x amount values reshape test x after that we proceed to deal with the imbalance of our data i used imblearn library to apply smote in order to oversample the fraudulent data which means that i increased the total number of fraudulent transactions by synthetically generating more of them based on the frauds that we already have in the dataset just to remember this is how fraudulent and genuine transactions are distributed in the dataset y value counts genuine transactions fraud i then applied smote on the training set from imblearn over sampling import smote train x train y smote fit resample train x train y reshaping data and counted values on the train y set train y value counts now we have a balance between genuine and fraudulent transactions in the training set note that ive only applied oversampling in the training set while maintaining the test set untouched with its original proportions and i did that because the test set must be a full representation of reality applying classifiers well after all that work done lets see how well the algorithms can predict the fraudulent transactions on the training set applying random forest classifier random forest randomforestclassifier n estimators random state random forest fit train x train y y predictions rf random forest predict test x applying decision tree classifier decision tree decisiontreeclassifier random state decision tree fit train x train y y predictions dt decision tree predict test x applying ada boost classifier ada boost adaboostclassifier n estimators random state ada boost fit train x train y y predictions ab ada boost predict test x applying gradient boosting classifier gradient boosting gradientboostingclassifier n estimators random state gradient boosting fit train x train y y prediction gb gradient boosting predict test x before evaluating how well our models performed ill briefly talk about the evaluation metrics for classification algorithms evaluation metrics for classification models when dealing with classification models there are some evaluation metrics that we can use in order to see the efficiency of our models one of those evaluation metrics is the confusion matrix which is a summary of predicted results compared to the actual values of our dataset this is what a confusion matrix looks like for a binary classification problem tp is for true positive and it shows the correct predictions of a model for a positive class fp is for false positive and it shows the incorrect predictions of a model for a positive class fn is for false negative and it shows the incorrect predictions of a model for a negative class tn is for true negative and it shows the correct predictions of a model for a negative class beyond the confusion matrix we also have some other relevant metrics they are accuracy accuracy simply tells us the proportion of correct predictions this is how we calculate it precision precision tells us how frequently our model correctly predicts positives this is how we calculate it recall recall which can also be referred to as sensitivity can tell us how well our model predicts the class that we want to predict this is how we calculate it f score lastly f score is the harmonic mean of precision and recall this is how we calculate it random forest scores lets the metrics for random forest and its confusion matrix printing evaluation metrics for random forest metrics accuracy accuracy score test y y predictions rf precision precision score test y y predictions rf recall recall score test y y predictions rf f score f score test y y predictions rf metrics df pd dataframe metrics columns metrics results metrics df confusion matrix for random forest confusion matrix rf confusion matrix test y y predictions rf visualization plt figure figsize ax plt subplot sns heatmap confusion matrix rf annot true fmt g ax ax ax set xlabel predicted values ax set ylabel actual values ax set title confusion matrix random forest ax xaxis set ticklabels genuine fraud ax yaxis set ticklabels genuine fraud plt show decision tree scores ada boost scores gradient boosting scores lets see how many fraudulent and genuine transactions we have in the testing set to compare these values with what we have on the confusion matrixes above counting how many fraudulent and how many genuine transactions we have on the testing set test y value counts the model that predicted the most numbers of fraudulent transactions correctly was the ada boost classifier who correctly identified frauds out of with a recall the highest of all the other algorithms remember recall tells us how well our model predicts the class we want to predict conclusion when we work with a machine learning model we must always know for a fact what it is that were trying to get from that model in this project our goal is to detect fraudulent transactions when they occur and the model that best performed such task was the ada boost classifier with a recall of correctly detecting fraudulent transactions out of however it is also important to note that the ada boost classifier had the biggest number of false positives that is genuine transactions were mistakenly labeled as fraud thats of all genuine transactions a genuine purchase being incorrectly identified as a fraud could be a problem in this scenario it is necessary to understand the business and make a few questions such as how cheap would a false positive be would we keep the ada boost classifier with the best performance in detecting frauds while also detecting a lot of false positives or should we use the random forest classifier who also performed pretty well identifying frauds recall and reduced the number of false positives of genuine transactions flagged as fraud but that would also imply in a larger number of fraudsters getting away with it and customers being mistakenly charged these questions and a deeper understanding of how the business works and how we want to approach solving a problem using machine learning are fundamental for a decision making process to choose whether or not if were willing to deal with a larger number of false positives to detect the largest amounts of frauds as possible github and kaggle if you wish to see the full project and how i developed it step by step you can see it on github and also kaggle where plotly plots are interactive references credit card fraud detection dataset decision trees ensemble methods machine learning number of purchase transactions on global general purpose card brands american express diners discover jcb mastercard unionpay and visa from to in billions principal component analysis thank you lu s fernando torres follow me on linkedin follow me on kaggle what are ensemble methods in machine learning aqeel anwar towards data science aqeel anwar follow published in towards data science min read jan a visual walkthrough of the ensemble methods in machine learning with a cheatsheet background lets say you moved to a new place and want to dine out how do you find a good place solution find a food critic who is really good at his her work and see if he she has any recommendations for the restaurants in your area solution use google and randomly look at one users review for a couple of restaurants solution use google and look at multiple users reviews for a couple of restaurants and average their ratings let us analyze each of the above mentioned solutions solution food critics are in general much accurate it is difficult to find a food critic maybe the food critic you found was a strict vegetarian and you are not in that case the recommendations from the food critic will be biased solution on the other hand picking up a random persons star rating for a restaurant on the internet is much less accurate easier to find solution collectively it can be just the right amount of accuracy you need easier to find over the internet much less biased since the users who have rated the restaurants come from various backgrounds hence without the need to ask from a food critic you can get a reasonably good recommendation on restaurants just by looking at a collective opinion of a group of random but large people this is known as the wisdom of the crowd and is the backbone to various informative websites like quora stack exchange and wikipedia etc what are ensemble methods ensemble methods in machine learning use more than one weak learner collectively to predict the output instead of training one large complex model for your dataset you train multiple small simpler models weak learners and aggregate their output in various ways to form your prediction as shown in the figure below inference ensemble method image by author types of ensemble methods generally speaking there are three different types of ensemble methods commonly used in ml these days bagging boosting stacking these methods have the same wisdom of the crowd concept but differ in the details of what it focuses on the type of weak learners used and the type of aggregation used to form the final output bagging in bagging bootstrap aggerating multiple weak learners are trained in parallel for each weak learner the input data is randomly sampled from the original dataset with replacement and is trained a random sampling of the subset with replacement creates nearly iid samples during inference the test input is fed to all the weak learners and the output is collected the final prediction is carried out by voting on the outputs of each weak learner the complete steps are shown in the block diagram below ensemble method bagging image by author in bagging methods the weak learners usually are of the same type since the random sampling with replacement creates iid samples and aggregating iid variables doesnt change the bias but reduces variance the bagging method doesnt change the bias in the prediction but reduces its variance boosting in boosting multiple weak learners are learned sequentially each subsequent model is trained by giving more importance to the data points that were misclassified by the previous weak learner in this way the weak learners can focus on specific data points and can collectively reduce the bias of the prediction the complete steps are shown in the block diagram below ensemble method boosting image by author the first weak learner is trained by giving equal weights to all the data points in the dataset once the first weak learner is trained the prediction error for each point is evaluated based on the error for each data point the corresponding weight of the data point for the next learner is updated if the data point was correctly classified by the trained weak learner its weight is reduced otherwise its weight is increased apart from updating the weights each weak learner also maintains a scalar alpha that quantifies how good was the weak learner in classifying the entire training dataset the subsequent models are trained on these weighted sets of points one way of carrying out training on a weighted set of points is to represent the weight term in the error instead of using mean squared error a weighted mean squared error is used ensuring that data points with higher assigned weight are given more importance in being correctly classified the other way could be weighted sampling i e sample points based on their weights when training in the inference phase the test input is fed to all the weak learners and their output is recorded the final prediction is achieved by scaling each weak learners output with the corresponding weak learners weight alpha before using them for voting as shown in the diagram above stacking in stacking multiple weak learners are trained in parallel which is similar to what happens in bagging but unlike bagging stacking does not carry out simple voting to aggregate the output of each weak learner to calculate the final prediction rather another meta learner is trained on the outputs of weak learners to learn a mapping from the weak learners output to the final prediction the complete block diagram can be seen below ensemble method stacking image by author stacking usually has weak learners of different types hence a simple voting method that gives equal weights to all the weak learners prediction doesnt seem like a good idea it would have been if the weak learners were identical in structure that is where the meta learner comes in it tries to learn which weak learner is more important the weak learners are trained in parallel but the meta learner is trained sequentially once the weak learners are trained their weights are kept static to train the meta learner usually the meta learner is trained on a different subset than what was used to train the weak learners cheat sheet the following cheat sheet covers the topic of ensemble methods that might come in handy cheat sheet ensemble methods source http cheatsheets aqeel anwar com summary instead of training one network ensemble methods use multiple weak learners and aggregate their individual output to create final predictions a comparison of different ensemble methods can be seen in the table below comparison of ensemble methods image by author bonus compact cheat sheets for this topic and many other important topics in machine learning can be found in the link below cheat sheets for machine learning interview topics a visual cheatsheet for ml interviews www cheatsheets aqeel anwar com medium com if this article was helpful to you feel free to clap share and respond to it if you want to learn more about machine learning and data science follow me aqeel anwar or connect with me on linkedin shailey dash towards data science shailey dash follow published in towards data science min read nov though decision trees look simple and intuitive there is nothing very simple about how the algorithm goes about the process deciding on splits and how tree pruning occurs in this post i take you through a simple example to understand the inner workings of decision trees iris decision tree from scikit learn image source sklearn decision trees are a popular and surprisingly effective technique particularly for classification problems but the seemingly intuitive interface hides complexities the criterion for selecting variables and hierarchy can be tricky to get not to mention gini index entropy wait isnt that physics and information gain isnt that information theory as you can see there are lots of tricky problems on which you can get stuck on the best way to understand decision trees is to work through a small example which has sufficient complexity to be able to demonstrate some of the common points one suddenly goes not sure what happens here this post is therefore more like a tutorial or a demo where i will work through a toy dataset that i have created to understand the following what is a decision tree root node sub nodes terminal leaf nodes splitting criteria entropy information gain vs gini index how do sub nodes split why do trees overfit and how to stop this how to predict using a decision tree so lets get demonstrating what does a decision tree do lets begin at the real beginning with core problem for example we are trying to classify whether a patient is diabetic or not based on various predictor variables such as fasting blood sugar bmi bp etc this is obviously a prediction problem for a new patient we also have patient records to help us develop an understanding of which features are most useful in predicting unlike other classification algorithms such as logistic regression decision trees have a somewhat different way of functioning and identifying which variables are important the first thing to understand in decision trees is that they split the predictor space i e the target variable into different sub groups which are relatively more homogenous from the perspective of the target variable for example if the target variable is binary with categories and shown by green and red dots in the image below then the decision tree works to split the target variable space into sub groups that are more homogenous in terms of having either s or s target variable splitting process image source author that is the overall concept let us begin with understanding the various elements of a decision tree understanding components of a decision tree a decision tree is a branching flow diagram or tree chart it comprises of the following components a target variable such as diabetic or not and its initial distribution a root node this is the node that begins the splitting process by finding the variable that best splits the target variable node purity decision nodes are typically impure or a mixture of both classes of the target variable or green and red dots in the image pure nodes are those that have one class hence the term pure they either have green or red dots only in the image decision nodes these are subsequent or intermediate nodes where the target variable is again split further by other variables leaf nodes or terminal nodes are pure nodes hence are used for making a prediction of a numerical or class is made lets see this visually structure of a decision tree image source my collection in general a decision tree takes a statement or hypothesis or condition and then makes a decision on whether the condition holds or does not the conditions are shown along the branches and the outcome of the condition as applied to the target variable is shown on the node arrows leading away from a node indicate a condition which is being applied to the node arrows pointing to a node indicate a condition that is being satisfied this is the first level of the decision tree understanding the flow of splitting the decision space into smaller spaces which ultimately become more and more homogenous in the target variable this ultimately leads to a prediction decision trees offer tremendous flexibility in that we can use both numeric and categorical variables for splitting the target data categoric data is split along the different classes in the variable numeric is a little more tricky as we have to split into thresholds for the condition being tested such as and for example a numeric variable can appear multiple times in the data with different cut offs or thresholds also final classifications can be repeated the important things from data science perspective are flow of information through the decision tree how does decision trees select which variable to split on at decision nodes how does it decide that the tree has enough branches and that it should stop splitting now let us look at a simplified toy example to understand the above process more concretely first the problem we have data for data points of student data on pass or fail an online ml exam to understand the basic process we begin with a dataset which comprises a target variable that is binary pass fail and various binary or categorical predictor variables such as whether enrolled in other online courses whether student is from a maths computer science or other background whether working or not working the dataset is given below toy dataset for online ml exam source author notice that only one variable student background has more than levels or categories maths cs others it is one for the advantages of decision trees compared to other classification models such as logistic regression or svm that we do not need to carry out one hot encoding to make these into dummy variables let us first look at the flow of how a decision tree works and then we will dive into the complexities of how the decisions are actually made flow of a decision tree a decision tree begins with the target variable this is usually called the parent node the decision tree then makes a sequence of splits based in hierarchical order of impact on this target variable from the analysis perspective the first node is the root node which is the first variable that splits the target variable to identify the root node we would evaluate the impact of all the variables that we have currently on the target variable to identify the variable that splits the exam pass fail classes into the most homogenous groups our candidates for splitting this are background working status and other online courses what do we hope to achieve with this split suppose we begin with working status as the root node this splits into sub nodes one each for working and not working thus the pass fail status is updated in each sub node respectively sample decision tree flow image source author created so this is the basic flow of the decision tree as long as there is a a mixture of pass and fail in a sub node there is scope to split further to try and get it to be only one category this is termed the purity of the node for example not working has pass and fail hence it is purer than the working node which has p and f a leaf node would be one which contains either pass or fail class only a node which is impure can be branched further for improving purity however most of the time we do not necessarily go down to the point where each leaf is pure it is also important to understand that each node is standalone and hence the attribute that best splits the working node may not be the one that best splits the not working node now let us move on to learning the core part of decision trees key questions how does the tree decide which variable to branch out at each level greedy top down approach decision trees follow a a top down greedy approach that is known as recursive binary splitting the recursive binary splitting approach is top down because it begins at the top of the tree and then it successively splits the predictor space at each split the predictor space gets divided into and is shown via two new branches pointing downwards the algorithm is termed is greedy because at each step of the process the best split is made for that step it does not project forwards and try and pick a split that might be more optimal for the overall tree the algorithm therefore evaluates all variables on some statistical criteria and then chooses the variable that performs best on the criteria variable selection criterion here is where the true complexity and sophistication of decision lies variables are selected on a complex statistical criterion which is applied at each decision node now variable selection criterion in decision trees can be done via two approaches entropy and information gain gini index both criteria are broadly similar and seek to determine which variable would split the data to lead to the underlying child nodes being most homogenous or pure both are used in different decision tree algorithms to add to the confusion it is not clear which one is the preferred approach so one has to have an understanding of both let us begin with entropy and information gain criterion what is entropy entropy is a term that comes from physics and means a measure of disorder more specifically we can define it as entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder randomness or uncertainty the term and the concept are used in diverse fields from classical thermodynamics where it was first recognized to the microscopic description of nature in statistical physics and to the principles of information theory https en wikipedia org wiki entropy in information theory the entropy of a random variable is the average level of information surprise or uncertainty inherent to the variables possible outcomes https en wikipedia org wiki entropy information theory in the context of decision trees entropy is a measure of disorder or impurity in a node thus a node with more variable composition such as pass and fail would be considered to have higher entropy than a node which has only pass or only fail the maximum level of entropy or disorder is given by and minimum entropy is given by a value leaf nodes which have all instances belonging to class would have an entropy of whereas the entropy for a node where the classes are divided equally would be entropy is measured by the formula where the pi is the probability of randomly selecting an example in class i let us understand this a bit better in the context of our example so the initial entropy of at the parent node is given by the probability of getting a pass vs fail in our dataset the target variable has passes and fails hence the probabilities for the entropy formula are now essentially what a decision tree does to determine the root node is to calculate the entropy for each variable and its potential splits for this we have to calculate a potential split from each variable calculate the average entropy across both or all the nodes and then the change in entropy vis a vis the parent node this change in entropy is termed information gain and represents how much information a feature provides for the target variable entropy parent is the entropy of the parent node and entropy children represents the average entropy of the child nodes that follow this variable in the current case since we have variables for which this calculation must be done from the perspective of the split work status online course status student background to calculate entropy first let us put our formulas for entropy and information gain in terms of variables in our dataset probability of pass and fail at each node i e the pi entropy average entropy at child nodes note average entropy is the weighted average of all the sub nodes that a parent node splits into thus in our example this would be sub nodes for working status and sub nodes for student background information gain parent node calculations first we will calculate parent node entropy using the formula above use any log calculator online to calculate the log values in our case they work out to mathematical note log to base of anything less than is a negative number hence we multiply by a minus sign to get a positive number so far this is just the entropy of the parent node now we have to decide which attribute or variable to use to split this to get the root node calculating the root node for this we have to calculate a potential split from each variable calculate the average entropy across both the nodes and then the change in entropy via a vis the parent node let us begin with work status variable and calculate the entropy of the split we then calculate the average entropy for the working status split as a weighted average with weights of the share of observations from the total number of observations that fall in each sub node information gain entropy parent entropy child calculations are shown in the spreadsheet below in a similar fashion we can evaluate the entropy and information gain for student background and online courses variables the results are provided in the table below we will drop this variable for the time being and move on to evaluate the other variables the spreadsheet below shows the entropy calculations for all the variables root node entropy calculations source author to find the root node attribute we look at the information gain from student background vis a vis initial parent entropy this shows the maximum reduction of hence this is the attribute that would be selected as the root node the others variables working status and online courses show a much lower decrease in entropy vis a vis the parental node so on the basis of the above calculations we have determined what the root node would be the tree would now look as follows root node of decision tree student background splits the target variable into groups everyone from cs background clearly passes and hence this is a terminal or leaf node everyone from other backgrounds fails and this is also a terminal node maths background is split into pass and fail and hence it is impure and there is some scope for further splitting to attain greater purity now to split the maths background sub node we need to calculate entropy and information gain for the remaining variables i e working status and online courses we would then select the variable that shows the highest information gain the entropy and information gain calculations for the maths background node can be seen in the table below notice we now have the maths background as the node that is being split hence average entropy for the splits is calculated using it as a base note putting in log throws an error however mathematically we can use the limit normally we just dont include the pj case in the calculation however i have included just to show the complete calculation by convention log since y log y as y https www icg isy liu se courses infotheory lect pdf the entropy for each potential split is splitting the maths subnode image source author as we can see information gain is higher for the working status variable hence this is the variable used to continue branching maths node branching we now see that the maths node has split into terminal node on the right and one node which is still impure notice now almost all our nodes are terminal nodes there is only one node which is not terminal we can try splitting it further using other online courses anyway you get the picture in any case most decision trees do not necessarily split to the point where every node is a terminal node most algorithms have built in stops which we will discuss a little further down further if the decision tree continues to split we have another problem which is that of overfitting again we shall discuss that below after we have briefly reviewed an alternative approach to developing a decision tree using the gini index gini index the other way of splitting a decision tree is via the gini index the entropy and information gain method focuses on purity and impurity in a node the gini index or impurity measures the probability for a random instance being misclassified when chosen randomly the lower the gini index the better the lower the likelihood of misclassification the formula for gini index where j represents the no of classes in the target variable pass and fail in our example p i represents the ratio of pass total no of observations in node so lets take an example from the decision tree above lets begin with the root node and calculate the gini index for each of the splits the gini index has a minimum highest level of purity of it has a maximum value of if gini index is it indicates a random assignment of classes now let us calculate the gini index for the root node for student background attribute in this case we have nodes gini formula requires us to calculate the gini index for each sub node then do a weighted average to calculate the overall gini index for the node maths sub node pass fail cs sub node pass fail others sub node pass fail as we can see the probability for misclassification in cs node is zero since everyone passes similarly no scope for misclassification on others node as everyone fails only the maths node has possibility of misclassification and this is quite high given that the maximum gini index is the overall gini index for this split is calculated similarly to the entropy as weighted average of the distribution across the nodes similarly we can also compute the gini index for working status and online courses these are given below working not working online courses the gini index is lowest for the student background variable hence similar to the entropy and information gain criteria we pick this variable for the root node in a similar fashion we would again proceed to move down the tree carrying out splits where node purity is less gini index vs information gain depending on which impurity measurement is used tree classification results can vary this can make small or sometimes large impact on your model there seems to be no one preferred approach by different decision tree algorithms for example cart uses gini id and c use entropy the gini index has a maximum impurity is and maximum purity is whereas entropy has a maximum impurity of and maximum purity is how does a prediction get made in decision trees now that we have understood hopefully in detail how decision trees carry out splitting and variable selection we can move on to how they do prediction actually once a tree is trained and tested prediction is easy the tree basically provides a flow chart based on various predictor variables suppose we have a new instance entering the flow along with its values of different predictor variables unlike training and test data it will not have the class for the target attribute we are trying to predict this class by moving down the tree testing its values of different predictor variables at different branches ultimately the new instance will move into a leaf node and will be classified according to the class prevailing in the leaf node suppose it looks like the below configuration based on our tree we would first check the math branch then working yes branch that as we have seen is a leaf node and the new observation would be classified on the basis of the majority vote in this node i e since it is pass this new observation would also be predicted to be pass in practice when the algorithm is evaluating a new example and reaches a leaf node the prediction is based on the modal value of categories in the leaf node as seen in the above case the working node is not fully pure however we go with the prediction of the modal value which is pass in general most leaf nodes are not pure and hence for categorical prediction we use the modal value for prediction if it is a numerical prediction regression tree we predict the mean value of the target values at each leaf node overfitting and decision trees overfitting can be a big challenge with decision trees even in our toy example we can see the algorithm continues to split till it reaches a leaf node often the leaf node may just have one or two instances this will clearly lead to a complex tree structure which may not generalize well to a test scenario this is because each leaf will represent a very specific set of attribute combinations that are seen in the training data and the tree will not be able to classify attribute combinations not seen in the training data there are several ways we can prevent the decision tree from becoming too unwieldy broad approaches to avoiding overfitting are distinguished pre pruning or early stopping preventing the tree from growing too big or deep post pruning allowing a tree to grow to its full depth and then getting rid of various branches based on various criteria ensembling or using averages of multiple models such as random forest we will only briefly overview at pre and post pruning techniques here ensemble techniques such as random forest require more explanation and hence will be tackled in a separate article pruning pre pruning the pre pruning technique refers to the early stopping of the growth of the decision tree the pre pruning technique involves tuning the hyperparameters of the decision tree model prior to the training pipeline the hyperparameters of the decisiontreeclassifier in sklearn include max depth min samples leaf min samples split which can be tuned to early stop the growth of the tree and prevent the model from overfitting the best way is to use the sklearn implementation of the gridsearchcv technique to find the best set of hyperparameters for a decision tree model a challenge with the early stopping approach is that it faces a horizon problem where an early stopping may prevent some more fruitful splits down the line post pruning in this technique we allow the tree to grow to its maximum depth then we remove parts of the tree to prevent overfitting we effectively consider subtrees of the full tree which are evaluated on a criteria and then removed hence we are effectively going up the tree and converting leaves to nodes and subtrees the criteria whether a particular consolidation goes through or not is usually mse for regression trees and classification error for classification trees a challenge with post pruning is that a decision tree can grow very deep and large and hence evaluating every branch can be computationally expensive an important post pruning technique is cost complexity pruning ccp which provides a more efficient solution in this regard ccp is a complex and advanced technique which is parametrized by the parameter in scikit learn decisiontreesclassifier module so how does ccp work and what does it do the basic problem that ccp addresses is how to determine the best way to prune a tree intuitively we would select a sub tree to prune such that its removal leads to a lower test error rate this can be done using cross validation or if we have sufficient sample the validation set approach however given the number of sub trees in a fully grown tree for even a small sample this is likely to be a very computationally and time intensive process cost complexity pruning also known as weakest link pruning gives us a way to do just this rather than considering every possible subtree we consider a sequence of trees indexed by a nonnegative tuning parameter the tuning parameter controls a trade off between the subtrees complexity and its fit to the training data when then the subtree t will simply equal t because then just measures the training error however as increases there is a price to pay for having a tree with many terminal nodes and so the quantity will tend to be minimized for a smaller subtree an introduction to statistical learning p essentially the parameter is very similar to the penalty term in lasso regression the basic equation for ccp is given below ccp equation hastie p image source james et al this is one complex equation but lets try and understand a bit further some definitions rm rm is the rectangle i e the subset of predictor space corresponding to the mth terminal node y rm is the predicted response associated with mth terminal leaf yi y rm mse for the sub tree referenced by terminal node m we are using regression tree approach for this equation for simplicity i am following the equation and approach in james et al t is the no of terminal nodes in the tree t now lets see what the equation is doing we are essentially minimizing cost or loss given by yi y rm across all terminal nodes now is a term that multiplies the total number terminal nodes in the tree if then we are minimizing the training loss the tree will be the same as the original tree however with we add a penalty term which increases with the number of terminal nodes t this means the overall cost gets minimized for a smaller subtree minimal cost complexity pruning recursively finds the node with the weakest link the weakest link is characterized by an effective alpha where the nodes with the smallest effective alpha are pruned first https scikit learn org stable auto examples tree plot cost complexity pruning html what does this mean it means that the algorithm is hunting out nodes where the training loss is already high and hence can only be minimized with a small on the other hand nodes where the training loss is smaller can accommodate a larger penalty term as part of minimization to get an idea of what values of ccp alpha could be appropriate scikit learn provides decisiontreeclassifier cost complexity pruning path that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process as increases more of the tree is pruned we then have a tradeoff between bias and variance with the ccp alpha effectively we increase the bias of the model i e we simplify it however on the con side this means we have to tolerate increasing levels of impurity in the terminal nodes we see that as increases both no of nodes and tree depth reduces how to determine optimal plotting ccp alpha vs train and test accuracy we see that when and keeping the other default parameters of decisiontreeclassifier the tree overfits leading to a training accuracy and testing accuracy as alpha increases more of the tree is pruned thus creating a decision tree that generalizes better at some point however further increases in actually lead to a decrease in test accuracy as the model becomes too simplified in this example setting ccp alpha maximizes the testing accuracy refer to https scikit learn org stable auto examples tree plot cost complexity pruning html for details accuracy vs alpha image source sklearn advantages and disadvantages of trees decision trees trees give a visual schema of the relationship of variables used for classification and hence are more explainable the hierarchy of the tree provides insight into variable importance at times they can actually mirror decision making processes white box model which is explainable and we can track back to each result of the model this is in contrast to black box models such as neural networks in general there is less need to prepare and clean data such as normalization and one hot encoding of categorical variables and missing values note the sklearn implementation currently does not support categorical variables so we do need to create dummy variables similarly it does not support missing values but both can be handled in theory model can be validated statistically disadvantages prone to overfitting and hence lower predictive accuracy decision trees can be unstable because small variations in the data might result in a completely different tree being generated this problem for example can be mitigated by using decision trees within an ensemble can be non robust i e a small change in the data can cause a large change in the final estimated tree predictions are approximate based on relevant terminal nodes hence it it may not be the best method to extrapolate the results of the model to unseen cases decision tree learners create biased trees if some classes dominate it is required to balance the dataset prior to fitting with the decision tree so thats it for decision trees form start to at least two thirds of the way there are a lot of complexities hence i cannot say end i hope you liked this blog on the inner workings of decision trees one thing is clear this is far from a simple technique i have so far reviewed only the complexities of how variables hierarchy is chosen and a tree structure is built up and how pruning is done there are many types of decision tree algorithms even in scikit learn these include id c c and cart exploration of these models is for another blog references g james d witten t hastie and r tibshirani an introduction to statistical learning with applications in r springer advanced exploratory data analysis eda with python michael notter epfl extension school michael notter follow published in epfl extension school min read feb k how to quickly get a handle on almost any tabular dataset find the code to this article here getting a good feeling for a new dataset is not always easy and takes time however a good and broad exploratory data analysis eda can help a lot to understand your dataset get a feeling for how things are connected and what needs to be done to properly process your dataset in this article we will touch upon multiple useful eda routines however to keep things short and compact we might not always dig deeper or explain all of the implications but in reality spending enough time on a proper eda to fully understand your dataset is a key part of any good data science project as a rule of thumb you probably will spend of your time in data preparation and exploration and only in actual machine learning modeling investigation of structure quality and content overall the eda approach is very iterative at the end of your investigation you might discover something that will require you to redo everything once more that is normal but to impose at least a little bit of structure i propose the following structure for your investigations structure investigation exploring the general shape of the dataset as well as the data types of your features quality investigation get a feeling for the general quality of the dataset with regards to duplicates missing values and unwanted entries content investigation once the structure and quality of the dataset is understood we can go ahead and perform a more in depth exploration on the features values and look at how different features relate to each other but first we need to find an interesting dataset lets go ahead and load the road safety dataset from openml structure investigation before looking at the content of our feature matrix x lets first look at the general structure of the dataset for example how many columns and rows does the dataset have and how many different data types do those features include structure of non numerical features data types can be numerical and non numerical first lets take a closer look at the non numerical entries even though sex of driver is a numerical feature it somehow was stored as a non numerical one this is sometimes due to some typo in data recording these kind of things need to be taken care of during data preparation once this is taken care of we can use the describe function to investigate how many unique values each non numerical feature has and with which frequency the most prominent value is present using the code df x describe exclude number structure of numerical features next lets take a closer look at the numerical features more precisely lets investigate how many unique values each of these feature has this process will give us some insights about the number of this process will give us some insights about the number of binary unique values ordinal to unique values and continuous more than unique values features in the dataset conclusion of structure investigation at the end of this first investigation we should have a better understanding of the general structure of our dataset number of samples and features what kind of data type each feature has and how many of them are binary ordinal categorical or continuous for an alternative way to get such kind of information you could also use df x info or df x describe quality investigation before focusing on the actual content stored in these features lets first take a look at the general quality of the dataset the goal is to have a global view on the dataset with regards to things like duplicates missing values and unwanted entries or recording errors duplicates duplicates are entries that represent the same sample point multiple times for example if a measurement was registered twice by two different people detecting such duplicates is not always easy as each dataset might have a unique identifier features e g an index number or recording time that is unique to each new sample so you might want to ignore them first and once you are aware about the number of duplicates in your dataset you can simply drop them with drop duplicates missing values another quality issue worth to investigate are missing values having some missing values is normal what we want to identify at this stage are big holes in the dataset i e samples or features with a lot of missing values per sample to look at number of missing values per sample we have multiple options the most straight forward one is to simply visualize the output of df x isna with something like this this figure shows on the y axis each of the individual samples and on the x axis if any of the features contains a missing value while this is already a useful plot an even better approach is to use the missingno library to get a plot like this one from both of these plots we can see that the dataset has a huge hole caused by some samples where more than of the feature values are missing for those samples filling the missing values with some replacement values is probably not a good idea therefore lets go ahead and drop samples that have more than of missing values the threshold is inspired by the information from the data completeness column on the right of this figure per feature as a next step lets now look at the number of missing values per feature for this we can use some pandas trickery to quickly identify the ratio of missing values per feature from this figure we can see that most features dont contain any missing values nonetheless features like nd road class junction control age of vehicle still contain quite a lot of missing values so lets go ahead and remove any feature with more than of missing values small side note missing values there is no strict order in removing missing values for some datasets tackling first the features and than the samples might be better furthermore the threshold at which you decide to drop missing values per feature or sample changes from dataset to dataset and depends on what you intend to do with the dataset later on also until now we only addressed the big holes in the dataset not yet how we would fill the smaller gaps this is content for another post unwanted entries and recording errors another source of quality issues in a dataset can be due to unwanted entries or recording errors its important to distinguish such samples from simple outliers while outliers are data points that are unusual for a given feature distribution unwanted entries or recording errors are samples that shouldnt be there in the first place for example a temperature recording of c in switzerland might be an outlier as in very unusual while a recording at c would be an error similarly a temperature recording from the top of mont blanc might be physical possible but most likely shouldnt be included in a dataset about swiss cities of course detecting such errors and unwanted entries and distinguishing them from outliers is not always straight forward and depends highly on the dataset one approach to this is to take a global view on the dataset and see if you can identify some very unusual patterns numerical features to plot this global view of the dataset at least for the numerical features you can use pandas plot function and combine it with the following parameters lw lw stands for line width means that we dont want to show any lines marker instead of lines we tell the plot to use as markers for each data point subplots true subplots tells pandas to plot each feature in a separate subplot layout this parameter tells pandas how many rows and columns to use for the subplots the means as many as needed while the means to use columns per row figsize markersize to make sure that the figure is big enough we recommend to have a figure height of roughly the number of features and to adjust the markersize accordingly so what does this plot look like each point in this figure is a sample i e a row in our dataset and each subplot represents a different feature the y axis shows the feature value while the x axis is the sample index these kind of plots can give you a lot of ideas for data cleaning and eda usually it makes sense to invest as much time as needed until youre happy with the output of this visualization non numerical features identifying unwanted entries or recording errors on non numerical features is a bit more tricky given that at this point we only want to investigate the general quality of the dataset so what we can do is take a general look at how many unique values each of these non numerical features contain and how often their most frequent category is represented to do so you can use df x describe exclude number datetime there are multiple ways for how you could potentially streamline the quality investigation for each individual non numerical features none of them is perfect and all of them will require some follow up investigation but for the purpose of showcasing one such a solution what we could do is loop through all non numerical features and plot for each of them the number of occurrences per unique value we can see that the most frequent accident i e accident index had more than people involved digging a bit deeper i e looking at the individual features of this accident we could identify that this accident happened on february th at in cardiff uk a quick internet search reveals that this entry corresponds to a luckily non lethal accident including a minibus full of pensioners the decision for what should be done with such rather unique entries is once more left in the the subjective hands of the person analyzing the dataset without any good justification for why and only with the intention to show you the how lets go ahead and remove the most frequent accidents from this dataset conclusion of quality investigation at the end of this second investigation we should have a better understanding of the general quality of our dataset we looked at duplicates missing values and unwanted entries or recording errors it is important to point out that we didnt discuss yet how to address the remaining missing values or outliers in the dataset this is a task for the next investigation but wont be covered in this article content investigation up until now we only looked at the general structure and quality of the dataset lets now go a step further and take a look at the actual content in an ideal setting such an investigation would be done feature by feature but this becomes very cumbersome once you have more than features for this reason and to keep this article as short as needed we will explore three different approaches that can give you a very quick overview of the content stored in each feature and how they relate feature distribution looking at the value distribution of each feature is a great way to better understand the content of your data furthermore it can help to guide your eda and provides a lot of useful information with regards to data cleaning and feature transformation the quickest way to do this for numerical features is using histogram plots luckily pandas comes with a builtin histogram function that allows the plotting of multiple features at once there are a lot of very interesting things visible in this plot for example most frequent entry some features such as towing and articulation or was vehicle left hand drive mostly contain entries of just one category using the mode function we could for example extract the ratio of the most frequent entry for each feature and visualize that information skewed value distributions certain kind of numerical features can also show strongly non gaussian distributions in that case you might want to think about how you can transform these values to make them more normal distributed for example for right skewed data you could use a log transformation feature patterns next step on the list is the investigation of feature specific patterns the goal of this part is two fold can we identify particular patterns within a feature that will help us to decide if some entries need to be dropped or modified can we identify particular relationships between features that will help us to better understand our dataset before we dive into these two questions lets take a closer look at a few randomly selected features in the top row we can see features with continuous values e g seemingly any number from the number line while in the bottom row we have features with discrete values e g but not while there are many ways we could explore our features for particular patterns lets simplify our option by deciding that we treat features with less than unique features as discrete or ordinal features and the other features as continuous features continuous features now that we have a way to select the continuous features lets go ahead and use seaborns pairplot to visualize the relationships between these features important to note seaborns pairplot routine can take a long time to create all subplots therefore we recommend to not use it for more than features at a time given that in our case we only have features we can go ahead with the pairplot otherwise using something like df continuous iloc could help to reduce the number of features to plot there seems to be a strange relationship between a few features in the top left corner location easting osgr and longitude as well as location easting osgr and latitude seem to have a very strong linear relationship knowing that these features contain geographic information a more in depth eda with regards to geolocation could be fruitful however for now we will leave the further investigation of this pairplot to the curious reader and continue with the exploration of the discrete and ordinal features discrete and ordinal features finding patterns in the discrete or ordinal features is a bit more tricky but also here some quick pandas and seaborn trickery can help us to get a general overview of our dataset first lets select the columns we want to investigate as always there are multiple way for how we could investigate all of these features lets try one example using seaborns stripplot together with a handy zip for loop for subplots note to spread the values out in the direction of the y axis we need to chose one particular hopefully informative feature while the right feature can help to identify some interesting patterns usually any continuous feature should do the trick the main interest in this kind of plot is to see how many samples each discrete value contains there are too many things to comment here so lets just focus on a few in particular lets focus on features where the values appear in some particular pattern or where some categories seem to be much less frequent than others and to shake things up a bit lets now use the longitude feature to stretch the values over the y axis these kind of plots are already very informative but they obscure regions where there are a lot of data points at once for example there seems to be a high density of points in some of the plots at the nd latitude so lets take a closer look with an appropriate plot such as violineplot or boxenplot or boxplot for that matter and to go a step further lets also separate each visualization by urban or rural area interesting we can see that some values on features are more frequent in urban than in rural areas and vice versa furthermore as suspected there seems to be a high density peak at latitude this is very likely due to the more densely populated region around london at feature relationships last but not least lets take a look at relationships between features more precisely how they correlate the quickest way to do so is via pandas corr function so lets go ahead and compute the feature to feature correlation matrix for all numerical features note depending on the dataset and the kind of features e g ordinal or continuous features you might want to use the spearman method instead of the pearson method to compute the correlation whereas the pearson correlation evaluates the linear relationship between two continuous variables the spearman correlation evaluates the monotonic relationship based on the ranked values for each feature and to help with the interpretation of this correlation matrix lets use seaborns heatmap to visualize it this looks already very interesting we can see a few very strong correlations between some of the features now if youre interested actually ordering all of these different correlations you could do something like this as you can see the investigation of feature correlations can be very informative but looking at everything at once can sometimes be more confusing than helpful so focusing only on one feature with something like df x corrwith df x speed limit might be a better approach furthermore correlations can be deceptive if a feature still contains a lot of missing values or extreme outliers therefore it is always important to first make sure that your feature matrix is properly prepared before investigating these correlations conclusion of content investigation at the end of this third investigation we should have a better understanding of the content in our dataset we looked at value distribution feature patterns and feature correlations however these are certainly not all possible content investigation and data cleaning steps you could do additional steps would for example be outlier detection and removal feature engineering and transformation and more a proper and detailed eda takes time it is a very iterative process that often makes you go back to the start after you addressed another flaw in the dataset this is normal its the reason why we often say that of any data science project is data preparation and eda take home message be mindful that an in depth eda can consume a lot of time and just because something seems interesting doesnt mean that you need to follow up on it always remind yourself what the dataset will be used for and tailor your investigations to support that goal sometimes it is also ok to just do a quick and dirty data preparation and exploration this will allow you to move on to the data modeling part rather quickly and to establish a few preliminary baseline models and perform some informative results investigation viola jones algorithm and haar cascade classifier complete explanation and mathematics for beginners mrinal tyagi towards data science mrinal tyagi follow published in towards data science min read jul viola jones is a novel approach to rapid detection of objects with running capabilities of frames per second it was the first to achieve real time object detection image by author in this article i talk about viola jones algorithm and it includes the following subtopics viola jones detector what are haar like features what is an integral image calculation of haar like features with integral image boosting and adaboost algorithm deep dive into adaboost algorithm mathematics cascade filter implementation using opencv library viola jones detector a viola jones detector consists of following steps calculating integral image calculating haar like features adaboost learning algorithm cascade filter what are haar like features haar features are the relevant features for face detection it was proposed by alfred haar in they are like convolutional kernels there are various types of haar like features but the most dominant features used are rectangular haar features rectangular haar features rectangular haar features image by author inspired https docs opencv org d d tutorial js face detection html the values of a rectangular feature is the difference between the sum of the pixels within rectangular regions the regions have same shape and size and are horizontally and vertically adjacent a three rectangular feature computes the sum in a centre rectangle finally a four rectangular feature computes the difference between diagonal pairs of rectangles various variations of these regions of different sizes are convolved through the image in order to get multiple filters that will be inputs to the adaboost training algorithm calculation of these features using the standard technique would require a high computation time in order to reduce this time a new approach called the integral image was suggested by the authors of the paper what is an integral image because we have to use haar like features in all possible sizes and locations which eventually result in around k features to calculate which is a really big number the problem with novel calculation of haar features is that we have to calculate the average of a given region multiple times and the time complexity of these operations are o n n we can use an integral image approach to achieve o running time a given pixel in the integral image is the sum of all the pixels on the left and all the pixels above it image by author inspired https www mathworks com help images integral image html image by author inspired https m blog naver com natalliea the sum of all purple boxes in the original image is equal to the sum of green boxes in the integral image subtracted by the purple boxes in the integral image calculation of haar like features with integral image using integral images we can achieve constant time evaluation of haar features edge features or rectangular features requires only memory lookups line features or rectangular features requires only memory lookups diagonal features or rectangular features requires only memory lookups rectangle a b c d e f rectangle a b c d e f g h rectangle a b c d e f h i j technique for calculation of sum of regions for calculation of haar like features in a constant amount of time image by author boosting and adaboost algorithm boosting refers to any ensemble method that can combine several weak learners into a strong learner the general idea of most boosting methods is to train predictors sequentially each trying to correct its predecessor adaboost also known as adaptive boosting is one of the most popular boosting techniques used adaboost one way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor under fitted this results in new predictors focusing more and more on the hard cases this is known as adaptive boosting for example to build an adaptive boosting classifier a first base classifier such a decision tree or svm classifier is trained and used to make predictions on the training set the relative weights of the misclassified predictions are altered and increased in order to lay more emphasis on these predictions while making the next predictor a second classifier is trained using the updated weights and again it makes predictions on the training set weights are updated and so on once all the predictions are trained the ensemble method makes predictions very much like boosting except the predictors have different weights depending on their overall accuracy on the weighted training set the drawback of this type of algorithm is that it cannot be parallelized thereby increasing time required thus after successfully running adaboost on all the features we are left with the most relevant features required for detection therefore this reduces computational time as we dont have to go through all the features and is much more efficient image by author inspired hands on machine learning with scikit learn keras and tensorflow concepts tools and techniques to build intelligent systems deep dive into adaboost algorithm lets take a closer look at the adaboost algorithm each instance weight w i is initially set to m a first predictor is trained and its weighted error rate r is computed on the training set here we take only the misclassified instances and sum up the weights of those instances to get the weighted error rate image by author the predictors weight j is then computed using the formulae given below the more accurate the predictor is the higher its weight will be if it is just guessing randomly then its weight will be close to zero however most often it is wrong and its weight will be negative image by author next the instance weights are updated using the formula provided below in order to boost the misclassified instances image by author then all the predictors are normalized using the formulae provided below image by author finally a new predictor is trained using the updated weights and the whole process is repeated until the desired number of predictors is reached which is specified by the user during inference adaboost simply computed the predictions of all the predictors and weights then using the predictor weight j the predicted class is the one that receives the majority of weighted votes cascade filter strong features are formed into a binary classifier positive matches are sent along to the next feature negative matches are rejected and exit computation reduces the amount of computation time spent on false windows threshold values might be adjusted to tune accuracy lower threshold yield higher detection rated and more false positives image by author inspired https www researchgate net figure cascade classifier illustration fig in simple terms each feature acts as a binary classifier in a cascade filter if an extracted feature from the image is passed through the classifier and it predicts that the image consists of that feature then it is passed on to the next classifier for next feature existence check otherwise it is discarded and next image is checked this thereby decreases computation time as we have to check only some features in windows where the object is not present rather than checking all features this is the main part of the algorithm that allows it to process videos at a rate of approximately frames per second and enables real time implementation implementation using opencv library references rapid object detection using a boosted cascade of simple features https web iitd ac in sumeet viola cvpr pdf detecting faces viola jones algorithm computerphile https www youtube com watch v uej vlummq hands on machine learning with scikit learn keras and tensorflow concepts tools and techniques to build intelligent systems by aurelien geron stay tuned for new research papers explanation like this feel free to connect and give ur suggestions prev tutorial cascade classifier introduction working with a boosted cascade of weak classifiers includes two major stages the training and the detection stage the detection stage using either haar or lbp based models is described in the object detection tutorial this documentation gives an overview of the functionality needed to train your own boosted cascade of weak classifiers the current guide will walk through all the different stages collecting training data preparation of the training data and executing the actual model training to support this tutorial several official opencv applications will be used opencv createsamples opencv annotation opencv traincascade and opencv visualisation note createsamples and traincascade are disabled since opencv consider using these apps for training from branch for cascade classifier model format is the same between and x important notes if you come across any tutorial mentioning the old opencv haartraining tool which is deprecated and still using the opencv x interface then please ignore that tutorial and stick to the opencv traincascade tool this tool is a newer version written in c in accordance to the opencv x and opencv x api the opencv traincascade supports both haar like wavelet features and lbp local binary patterns features lbp features yield integer precision in contrast to haar features yielding floating point precision so both training and detection with lbp are several times faster then with haar features regarding the lbp and haar detection quality it mainly depends on the training data used and the training parameters selected its possible to train a lbp based classifier that will provide almost the same quality as haar based one within a percentage of the training time the newer cascade classifier detection interface from opencv x and opencv x cv cascadeclassifier supports working with both old and new model formats opencv traincascade can even save export a trained cascade in the older format if for some reason you are stuck using the old interface at least training the model could then be done in the most stable interface the opencv traincascade application can use tbb for multi threading to use it in multicore mode opencv must be built with tbb support enabled preparation of the training data for training a boosted cascade of weak classifiers we need a set of positive samples containing actual objects you want to detect and a set of negative images containing everything you do not want to detect the set of negative samples must be prepared manually whereas set of positive samples is created using the opencv createsamples application negative samples negative samples are taken from arbitrary images not containing objects you want to detect these negative images from which the samples are generated should be listed in a special negative image file containing one image path per line can be absolute or relative note that negative samples and sample images are also called background samples or background images and are used interchangeably in this document described images may be of different sizes however each image should be equal or larger than the desired training window size which corresponds to the model dimensions most of the times being the average size of your object because these images are used to subsample a given negative image into several image samples having this training window size an example of such a negative description file directory structure img img jpg img jpg bg txt file bg txt img img jpg img img jpg your set of negative window samples will be used to tell the machine learning step boosting in this case what not to look for when trying to find your objects of interest positive samples positive samples are created by the opencv createsamples application they are used by the boosting process to define what the model should actually look for when trying to find your objects of interest the application supports two ways of generating a positive sample dataset you can generate a bunch of positives from a single positive object image you can supply all the positives yourself and only use the tool to cut them out resize them and put them in the opencv needed binary format while the first approach works decently for fixed objects like very rigid logos it tends to fail rather soon for less rigid objects in that case we do suggest to use the second approach many tutorials on the web even state that real object images can lead to a better model than artificially generated positives by using the opencv createsamples application if you however do decide to take the first approach keep some things in mind please note that you need more than a single positive samples before you give it to the mentioned application because it only applies perspective transformation if you want a robust model take samples that cover the wide range of varieties that can occur within your object class for example in the case of faces you should consider different races and age groups emotions and perhaps beard styles this also applies when using the second approach the first approach takes a single object image with for example a company logo and creates a large set of positive samples from the given object image by randomly rotating the object changing the image intensity as well as placing the image on arbitrary backgrounds the amount and range of randomness can be controlled by command line arguments of the opencv createsamples application command line arguments vec vec file name name of the output file containing the positive samples for training img image file name source object image e g a company logo bg background file name background description file contains a list of images which are used as a background for randomly distorted versions of the object num number of samples number of positive samples to generate bgcolor background color background color currently grayscale images are assumed the background color denotes the transparent color since there might be compression artifacts the amount of color tolerance can be specified by bgthresh all pixels within bgcolor bgthresh and bgcolor bgthresh range are interpreted as transparent bgthresh background color threshold inv if specified colors will be inverted randinv if specified colors will be inverted randomly maxidev max intensity deviation maximal intensity deviation of pixels in foreground samples maxxangle max x rotation angle maximal rotation angle towards x axis must be given in radians maxyangle max y rotation angle maximal rotation angle towards y axis must be given in radians maxzangle max z rotation angle maximal rotation angle towards z axis must be given in radians show useful debugging option if specified each sample will be shown pressing esc will continue the samples creation process without showing each sample w sample width width in pixels of the output samples h sample height height in pixels of the output samples when running opencv createsamples in this way the following procedure is used to create a sample object instance the given source image is rotated randomly around all three axes the chosen angle is limited by maxxangle maxyangle and maxzangle then pixels having the intensity from the bg color bg color threshold bg color bg color threshold range are interpreted as transparent white noise is added to the intensities of the foreground if the inv key is specified then foreground pixel intensities are inverted if randinv key is specified then algorithm randomly selects whether inversion should be applied to this sample finally the obtained image is placed onto an arbitrary background from the background description file resized to the desired size specified by w and h and stored to the vec file specified by the vec command line option positive samples also may be obtained from a collection of previously marked up images which is the desired way when building robust object models this collection is described by a text file similar to the background description file each line of this file corresponds to an image the first element of the line is the filename followed by the number of object annotations followed by numbers describing the coordinates of the objects bounding rectangles x y width height an example of description file directory structure img img jpg img jpg info dat file info dat img img jpg img img jpg image img jpg contains single object instance with the following coordinates of bounding rectangle image img jpg contains two object instances in order to create positive samples from such collection info argument should be specified instead of img info collection file name description file of marked up images collection note that in this case parameters like bg bgcolor bgthreshold inv randinv maxxangle maxyangle maxzangle are simply ignored and not used anymore the scheme of samples creation in this case is as follows the object instances are taken from the given images by cutting out the supplied bounding boxes from the original images then they are resized to target samples size defined by w and h and stored in output vec file defined by the vec parameter no distortion is applied so the only affecting arguments are w h show and num the manual process of creating the info file can also been done by using the opencv annotation tool this is an open source tool for visually selecting the regions of interest of your object instances in any given images the following subsection will discuss in more detail on how to use this application extra remarks opencv createsamples utility may be used for examining samples stored in any given positive samples file in order to do this only vec w and h parameters should be specified example of vec file is available here opencv data vec files trainingfaces vec it can be used to train a face detector with the following window size w h using opencvs integrated annotation tool since opencv x the community has been supplying and maintaining a open source annotation tool used for generating the info file the tool can be accessed by the command opencv annotation if the opencv applications where build using the tool is quite straightforward the tool accepts several required and some optional parameters annotations required path to annotations txt file where you want to store your annotations which is then passed to the info parameter example data annotations txt images required path to folder containing the images with your objects example data testimages maxwindowheight optional if the input image is larger in height then the given resolution here resize the image for easier annotation using resizefactor resizefactor optional factor used to resize the input image when using the maxwindowheight parameter note that the optional parameters can only be used together an example of a command that could be used can be seen below opencv annotation annotations path to annotations file txt images path to image folder this command will fire up a window containing the first image and your mouse cursor which will be used for annotation a video on how to use the annotation tool can be found here basically there are several keystrokes that trigger an action the left mouse button is used to select the first corner of your object then keeps drawing until you are fine and stops when a second left mouse button click is registered after each selection you have the following choices pressing c confirm the annotation turning the annotation green and confirming it is stored pressing d delete the last annotation from the list of annotations easy for removing wrong annotations pressing n continue to the next image pressing esc this will exit the annotation software finally you will end up with a usable annotation file that can be passed to the info argument of opencv createsamples cascade training the next step is the actual training of the boosted cascade of weak classifiers based on the positive and negative dataset that was prepared beforehand command line arguments of opencv traincascade application grouped by purposes common arguments data cascade dir name where the trained classifier should be stored this folder should be created manually beforehand vec vec file name vec file with positive samples created by opencv createsamples utility bg background file name background description file this is the file containing the negative sample images numpos number of positive samples number of positive samples used in training for every classifier stage numneg number of negative samples number of negative samples used in training for every classifier stage numstages number of stages number of cascade stages to be trained precalcvalbufsize precalculated vals buffer size in mb size of buffer for precalculated feature values in mb the more memory you assign the faster the training process however keep in mind that precalcvalbufsize and precalcidxbufsize combined should not exceed you available system memory precalcidxbufsize precalculated idxs buffer size in mb size of buffer for precalculated feature indices in mb the more memory you assign the faster the training process however keep in mind that precalcvalbufsize and precalcidxbufsize combined should not exceed you available system memory baseformatsave this argument is actual in case of haar like features if it is specified the cascade will be saved in the old format this is only available for backwards compatibility reasons and to allow users stuck to the old deprecated interface to at least train models using the newer interface numthreads max number of threads maximum number of threads to use during training notice that the actual number of used threads may be lower depending on your machine and compilation options by default the maximum available threads are selected if you built opencv with tbb support which is needed for this optimization acceptanceratiobreakvalue break value this argument is used to determine how precise your model should keep learning and when to stop a good guideline is to train not further than e to ensure the model does not overtrain on your training data by default this value is set to to disable this feature cascade parameters stagetype boost default type of stages only boosted classifiers are supported as a stage type at the moment featuretype haar default lbp type of features haar haar like features lbp local binary patterns w samplewidth width of training samples in pixels must have exactly the same value as used during training samples creation opencv createsamples utility h sampleheight height of training samples in pixels must have exactly the same value as used during training samples creation opencv createsamples utility boosted classifier parameters bt dab rab lb gab default type of boosted classifiers dab discrete adaboost rab real adaboost lb logitboost gab gentle adaboost minhitrate min hit rate minimal desired hit rate for each stage of the classifier overall hit rate may be estimated as min hit rate number of stages maxfalsealarmrate max false alarm rate maximal desired false alarm rate for each stage of the classifier overall false alarm rate may be estimated as max false alarm rate number of stages weighttrimrate weight trim rate specifies whether trimming should be used and its weight a decent choice is maxdepth max depth of weak tree maximal depth of a weak tree a decent choice is that is case of stumps maxweakcount max weak tree count maximal count of weak trees for every cascade stage the boosted classifier stage will have so many weak trees maxweakcount as needed to achieve the given maxfalsealarmrate haar like feature parameters mode basic default core all selects the type of haar features set used in training basic use only upright features while all uses the full set of upright and degree rotated feature set see for more details local binary patterns parameters local binary patterns dont have parameters after the opencv traincascade application has finished its work the trained cascade will be saved in cascade xml file in the data folder other files in this folder are created for the case of interrupted training so you may delete them after completion of training training is finished and you can test your cascade classifier visualising cascade classifiers from time to time it can be useful to visualise the trained cascade to see which features it selected and how complex its stages are for this opencv supplies a opencv visualisation application this application has the following commands image required path to a reference image for your object model this should be an annotation with dimensions w h as passed to both opencv createsamples and opencv traincascade application model required path to the trained model which should be in the folder supplied to the data parameter of the opencv traincascade application data optional if a data folder is supplied which has to be manually created beforehand stage output and a video of the features will be stored an example command can be seen below opencv visualisation image data object png model data model xml data data result some limitations of the current visualisation tool only handles cascade classifier models trained with the opencv traincascade tool containing stumps as decision trees default settings the image provided needs to be a sample window with the original model dimensions passed to the image parameter example of the haar lbp face model ran on a given window of angelina jolie which had the same preprocessing as cascade classifier files x pixel image grayscale conversion and histogram equalisation a video is made with for each stage each feature visualised skip to content mathworks products solutions academia support community events stateflow search mathworks com search mathworks com stateflow model and simulate decision logic using state machines and flow charts have questions contact sales stateflow provides a graphical language that includes state transition diagrams flow charts state transition tables and truth tables you can use stateflow to describe how matlab algorithms and simulink models react to input signals events and time based conditions stateflow enables you to design and develop supervisory control task scheduling fault management communication protocols user interfaces and hybrid systems with stateflow you model combinatorial and sequential decision logic that can be simulated as a block within a simulink model or executed as an object in matlab graphical animation enables you to analyze and debug your logic while it is executing edit time and run time checks ensure design consistency and completeness before implementation get started design control logic execute and debug charts develop reusable logic for matlab applications schedule simulink algorithms validate designs and generate code what is stateflow video length is what is stateflow free interactive course stateflow onramp get started design control logic model system logic using state machines flow charts and truth tables designing state machines graphically build state machines graphically by drawing states and junctions connected by transitions you can also create functions using flow chart notation simulink subsystems matlab code and truth tables represent operating modes by using states simulink subsystems as states reuse matlab code by defining matlab functions getting started with stateflow stateflow diagram defining the logic for a boiler temperature control system the diagram uses graphical functions right side to implement utility algorithms called by the heater system left side stateflow diagram defining the logic for a boiler temperature control system the diagram uses graphical functions right side to implement utility algorithms called by the heater system left side designing flow charts create flow charts by drawing transitions that are connected at junctions the pattern wizard lets you create commonly used logic flow patterns you can use flow charts to design logic for transitioning between states flow charts in stateflow create flow charts by using pattern wizard stateflow flow chart represent combinatorial logic such as decision trees and iterative loops graphically with flow charts designing logic with tables truth tables in stateflow let you model logic in simulink when the output depends purely on the current input state transition tables provide a structured environment for modeling state machines in simulink reuse combinatorial logic by defining truth tables state transition tables in stateflow state transition tables truth table implementing the logic for selecting a valid sensor reading in a fault detection algorithm truth table implementing the logic for selecting a valid sensor reading in a fault detection algorithm execute and debug charts visualize the behavior of your system for analysis and debugging executing stateflow charts visualize system behavior using state diagram animations to highlight active states and transitions in your charts execution of a stateflow chart types of chart execution understanding complex logic with stateflow video length is understanding complex logic with stateflow debugging stateflow charts stateflow debugging capabilities let you step through chart execution in detail you can set breakpoints monitor data values and step through different functions in your state diagrams set breakpoints to debug charts detect modeling errors during edit time analyzing and debugging logic with stateflow simulation data visualization options in stateflow top left simulink data inspector for comparing specific signals bottom left custom matlab interface for analyzing data right simulink signal selector for comparing specific states simulation data visualization options in stateflow top left simulink data inspector for comparing specific signals bottom left custom matlab interface for analyzing data right simulink signal selector for comparing specific states develop reusable logic for matlab applications use stateflow chart objects to develop reusable logic for matlab applications design state machine and timing logic for a wide range of applications including test and measurement autonomous systems signal processing and communications reusable chart objects create standalone stateflow charts that use the full capabilities of the matlab language in state and transition actions use these charts as matlab objects in your applications that require state machine and timing logic create stateflow charts for execution as matlab objects execute and unit test stateflow chart objects debug a standalone stateflow chart state machine and timing logic accelerate the development of matlab applications by using stateflow to graphically design state machine and timing logic that would be difficult to implement textually design human machine interface logic by using stateflow charts model a communications protocol by using chart objects analog trigger app by using stateflow charts using stateflow to provide the logic for a matlab app video length is using stateflow to provide the logic for a matlab app deploying stateflow applications create matlab applications that include stateflow chart objects and share them without requiring stateflow share matlab applications that include stateflow charts without needing a stateflow license share matlab applications that include stateflow chart objects with users who do not have stateflow schedule simulink algorithms schedule algorithms modeled in simulink periodic and continuous scheduling you can model conditional event based and time based logic in stateflow to invoke simulink algorithms in a periodic or continuous manner orchestrate the execution of components to simulate the scheduling of your real time environment transition between operating modes continuous time modeling in stateflow synchronize model components by broadcasting events you can model logic in stateflow to call simulink and matlab algorithms in a periodic or continuous manner you can model logic in stateflow to call simulink and matlab algorithms in a periodic or continuous manner temporal operators use event based and time based operators such as after and duration to specify state transition logic based on event counts elapsed time and denoised signals without having to create and maintain your own timers and counters control chart execution by using temporal logic control oscillations using the duration operator temporal logic operators video length is temporal logic operators validate designs and generate code validate your design against requirements and generate code for implementation on your embedded system validating designs use stateflow with other simulink products to validate your design against requirements link requirements directly to stateflow objects using drag and drop with requirements toolbox check that your state diagrams comply with standards using simulink check collect model and generated code coverage metrics with simulink coverage detect design errors and generate test vectors using simulink design verifier develop manage and execute simulation based tests with simulink test stateflow and model slicer highlight active logic using model slicer generating code generate code for implementation of your stateflow logic on embedded systems generate c and c code from simulink and stateflow models using simulink coder generate vhdl and verilog code for fpga and asic designs with hdl coder generate iec structured text for plcs and pacs using simulink plc coder generate code to implement stateflow logic generate code to implement stateflow logic product resources get a free trial days of exploration at your fingertips ready to buy get pricing information and explore related products are you a student your school may already provide access to matlab simulink and add on products through a campus wide license whats next panel navigation stateflow onramp learn the basics of simulating physical systems in simscape free tutorials self paced online courses release highlights whats new in the latest release of matlab and simulink mathworks accelerating the pace of engineering and science mathworks is the leading developer of mathematical computing software for engineers and scientists discover explore products matlab simulink student software hardware support file exchange try or buy downloads trial software contact sales pricing and licensing how to buy learn to use documentation tutorials examples videos and webinars training get support installation help matlab answers consulting license center contact support about mathworks careers newsroom social mission customer stories about mathworks select a web siteunited states trust center trademarks privacy policy preventing piracy application status the mathworks inc facebooktwitterinstagramyoutubelinkedinrss join the conversation control of semiconductor switches via pwm technologies control of the semiconductor switches is the most efficient and convenient means to achieve the control of power converters and machine drives they act as the actuators in the implementation of the control systems where the manipulated control inputs in the form of three phase voltage signals are realized by turning on and off the semiconductor switches depending on the application a large variety of power electronic devices different in types of semiconductor switches construction topologies and concepts have been developed the common functionality of these devices is to conduct power flow by varying the on off duration of each switch among them the level voltage source inverter l vsi as shown in figure is the most widely adopted mechanism to control three phase ac machines a similar topology is illustrated in the previous chapter for the power converter see figure based on a dc power supply for controlling ac motors the primary concern of using the semiconductor switches for the applications presented in this book is to create sinusoidal phase voltage signals via the fundamental components of rectangular signals that are produced by varying magnitudes and frequencies through turning on off each power switch for a duration of time a similar operational principle applies to the voltage source power converter see figure the command signal for turning on off each power switch is called a gate signal there are two approaches used in this book to generate the gate signal for the semiconductor switches the first approach uses pulse width modulation pwm based on which the pid controllers see chapters to and the traditional model predictive controllers see chapters to are implemented in the control applications the control signals calculated are the three phase voltage signals that are obtained from one of the controller designs using the model either in the d q reference frame or in reference frame the role of the voltage source inverter with the power electronics devices is to realize the three phase voltage control signals as closely as possible namely the sinusoidal phase voltage signals contained as fundamental components of rectangular wave signals created by turning on off each power switch with the pwm technologies are aimed to be closely matched with the three phase voltage control signals the second methodology features a much simpler approach in the implementation of the control systems that generates such a gate signal by direct optimization of an error function between the desired control signals and those achieved by the semiconductor switches see chapters to in this second approach there is no need to use the pwm technology therefore it significantly reduces the complexity of controlling the semiconductor switches pwm technology originally developed in the telecommunication engineering community has gained wide popularity and has been the subject of intensive research investigations in the control of power pid and predictive control of electrical drives and power converters using matlab simulink first edition liuping wang shan chai dae yoo lu gan and ki ng john wiley sons singapore pte ltd published by john wiley sons singapore pte ltd companion website www wiley com go wang pid pid and predictive control of electrical drives and power converters using matlab simulink electronics over the past several decades for example holtz holmes and lipo with pwm technology and a voltage source inverter the efficient control of the ac motor position speed and torque by variable speed drive vsd becomes possible power converters are also controlled by the pwm technology a wide variety of pwm generation techniques has been developed which could be categorized into two broad classes continuous pwm cpwm and discontinuous pwm dpwm hava et al zhou and wang compared with dpwm cpwm attains superior performance in the low modulation index range which is within the operating conditions of servo drives and the power converters for most of the time therefore it has gained more popularity in servo drive and converter applications these methods could be implemented by two methodologies carrier based pwm and direct digital implementation for a long time the carrier based pwm implementation such as sine triangle intersection techniques has dominated the industrial applications with the development of fast digital signal processors dsps direct digital implementation has also gained popularity in more recent years of which the space vector modulator svm has been well recognized in section the topology of a two level voltage source inverter is introduced where the relationship between the semiconductor power switches and the three phase sinusoidal control signal is established the remainder of this chapter discusses how to manage the semiconductor power switches so that the three phase sinusoidal control signal can be reconstructed in section the six step mode is introduced in section several carrier based pwm techniques are discussed among which zero sequence techniques are used to improve the modulation index section discusses the space vector modulation which has a direct digital implementation in section a simulation study of the effect of pwm is conducted to reveal the current ripples topology of igbt inverter one of the most widely adopted semiconductor power switches for the medium range power converter inverter is the insulated gate bipolar transistor igbt which offers the benefits of both mosfet and bipolar switches however igbt can only allow the current to flow in one direction and hence a freewheeling diode in parallel is required to conduct the current flow in the opposite direction in the three phase l vsi for controlling ac machine as shown in figure each leg of the inverter has two pairs of such a combination consisting of an igbt switch and a freewheeling diode where their middle point is linked to the loads either a passive load or an ac motor here the front end rectifier is replaced by two dc sources connected in series and each supply offers half of the total dc bus voltage vdc for the simplification of analysis it is sufficient to assume that the middle point denoted by o in figure between two dc sources is referred to the ground thus all the voltages can be represented with respect to the ground for example the neutral point voltage of ac motor is referred to as n with respect to the ground vdc vdc c s a s a sb sb s c s c l r vn va vb vc o r r l l figure topology of three phase leg igbt control of semiconductor switches via pwm technologies as seen in figure there are two igbt switches for each of the three legs within each leg of an inverter only one switch is allowed to turn on denoted by while the other is off denoted by at any given time to prevent short circuit thus the switching states of the inverter can be identified by only considering the states of the three upper switches with the states of three upper switches denoted as s i i a b c the states of their corresponding lower switches can be represented by their negation si i a b c as a result there are only eight possible switching states by turning on and off all the switches in the inverter since the states of upper and lower switches within the same leg are complementary to each other all eight switching states can be independently identified by the states of the three upper switches as listed in table among these two switching states v and v which represent the cases where either all the upper or all the lower switches are turned on leading to an open circuit are called zero vector in contrast the other six states that form a closed circuit are called active vector when the ith upper switch is on that is si and si the output of the corresponding phase leg is connected to the top rail of the supplies and thus i v dc conversely when the lower switch is on the output is connected to the bottom rail of the supplies and hence i v dc corresponding to the switching states in table the resulting output voltages a b c are summarized in table equivalently the output voltages could be represented in terms of their switching states i vdcsi v dc i a b c note that the output voltages are expressed with respect to the ground defined before it follows that the three phase voltages with respect to the neutral point of the load are obtained by an a n bn b n cn c n from it is seen that the three voltage signals a b and c generated from the voltage source inverter via the semiconductor switches are in rectangular wave forms with amplitude changes between table switching states of inverter v v v v v v v v s a s b s c table output voltage of inverter v v v v v v v v a v dc v dc v dc v dc v dc v dc v dc v dc b v dc v dc v dc v dc v dc v dc v dc v dc c v dc v dc v dc v dc v dc v dc v dc v dc pid and predictive control of electrical drives and power converters using matlab simulink vdc and therefore their fundamental components are of primary interest in the realization of the three phase control signals six step operating mode one of early technologies to control the power electronic switches is called six step mode see holtz in this approach the maximum magnitude of the fundamental with a l vsi is achieved by sequentially switching the six active vectors that is starting from v to v and the on to v within one electrical cycle for one cycle of the six step operation each output voltage i i a b c will have half of cycle connect to top rail of inverter and the other half bottom rail as an example figure illustrates the rectangular wave of c over one cycle and its associated fundamental where the period of the wave is e since this rectangular signal c is an odd function it can be approximated using a fourier sine series as see kreyszig where c t b sin et b sin et bn sin n et where n is an odd number and the coefficient for the nth index is b n e e v dc sin n etdt n v dc the fundamental component of the fourier sine series b sin et is the approximated sinusoidal output of the voltage source inverter for the third phase of the voltage and is the function to be closely matched with the control signal at this phase the rest of the components in the fourier sine series are the harmonics the fourier coefficient b v dc represents the amplitude of the fundamental component of the rectangular wave c t generated by one cycle of the six step operation this amplitude of the fundamental voltage of six step mode limits the maximum amplitude of the achievable modulation signal in other words the maximum amplitude of the sinusoidal voltage signals that is achievable when using the six step mode is v dc from a control system design point of view this means that if the calculated control signal had an amplitude that exceeded this value then the output of the inverter would not be able to fully realize it one of the drawbacks for the six step mode technique is that the amplitudes of the harmonics decay in a linear way proportional to the index n in particular the third harmonic still has a relatively large amplitude see that only becomes one third of the fundamental component from a control system perspective the harmonics become part of the noise and disturbances of the system therefore their amplitudes should be minimized this means that the six step mode technique will create large noise effects and disturbances in the control system vdc e e vdc figure output c of six step mode control of semiconductor switches via pwm technologies carrier based pwm use of the six step mode to control the power electronic switches is relatively simple in implementation however this approach has large harmonic components relative to the amplitude of the fundamental component of the output voltage signal which is v dc to reduce the effects of the harmonics carrier based pulse width modulation pwm techniques with zero sequence injection are introduced in general the carrier based pwm technique is to compare the amplitude of an input voltage signal with that of a carrier signal usually a periodic triangular signal with frequency fc if the input voltage signal say phase a voltage a is larger than the carrier in amplitude the switch function sa outputs high level logic and otherwise low level logic leading to the actual voltage output calculated as a vdcsa v dc the carrier frequency denoted by fc is usually chosen to be much higher than the fundamental frequency denoted by f of the voltage signal and their ratio is the multiple of for better reduction of total harmonic distortion thd see holmes and lipo for example a ratio fs f could be used for this purpose sinusoidal pwm as one of the earliest pwm generation techniques see bowes the sinusoidal pwm spwm generates the digital pulses to control the igbt switches by directly comparing the three phase voltages with the carrier commonly a periodic triangular waveform it was popular due to its simplicity and feasibility to be implemented by analog circuits as an example to demonstrate the working principle of sinusoidal pwm technique a carrier signal and three phase sinusoidal signals are shown in figure where the fundamental frequency of the sinusoidal signals is f and the carrier frequency is fc and fc f the carrier signal has amplitude of vdc where vdc is the dc voltage of the power supply for the control application of ac drives or vdc as the dc voltage output for the control application of power converter in this illustration there are cycles of the carrier signal and one cycle of the three phase sinusoidal signals see figure a when comparing the three phase sinusoidal signals with the carrier signal the values of s a sb and sc are taken either or depending on whether the corresponding sinusoidal function is greater or smaller than the carrier signal the actual output voltages are in rectangular waveforms see figure b with their magnitudes being vdc and their switches dependent on the values of sa sb and s c i vdcsi v dc i a b c the fundamental components of the three rectangular waves in figure b are found through fourier series analysis and are shown in the corresponding plots it is seen that they have similar characteristics as the input sinusoidal signals shown in figure a however for the modulation to work within the linear range where its fundamental resembles the desired signal it requires that the maximum amplitude of the sinusoidal signal be less than vdc otherwise the exceeding parts will cause the switch states stay at either or and thus the fundamentals lose their linear relationship to the desired reference voltage signals as a result the controller output voltage should limit its value within vdc to avoid the nonlinear modulation region if the sinusoidal pwm technique is used the maximum modulation range could be improved by zero sequence injection which has led to several different modulation schemes as introduced below pid and predictive control of electrical drives and power converters using matlab simulink vdc vdc vdc vdc vdc vdc vdc vdc sine triangle intersection a b figure illustration of sinusoidal pwm three phase voltage signals in a are reconstructed with pulse width modulation b a sine triangle intersection b pulses and fundamental sinusoidal signals from fourier analysis top figure corresponds to solid line wave in a middle figure corresponds to dashed line wave in a bottom figure corresponds to dotted line wave in a carrier based pwm with zero sequence injection to understand the carrier based pwm with zero sequence injection the existence of a zero sequence is examined for a three phase system let a m sin et b m sin et c m sin et control of semiconductor switches via pwm technologies zero sequence s a sb s c v a vb v c figure carrier based pwm with zero sequence injection hava et al denote the desired reference voltages that will be closely approximated by the outputs of the voltage source inverter the neutral point voltage n with respect to ground shown in figure can be represented by n a b c if the desired three phase voltage a b and c are balanced it means that the neutral point voltage n equals because most balanced three phase motors are three wired systems with the isolated neutral point there is the freedom of adding a nonzero value voltage to the neutral point voltage n leading to the modified three desired voltage signals a a n b b n c c n then based on the modified three desired voltage signals the sinusoidal pwm technique explained in the previous section can be applied figure illustrates the operation of the carrier based pwm technique with zero sequence injection which shows that the same zero sequence is added to all three reference voltage signals and these signals are compared with the carrier signal to produce the switching signals s a sb and sc there are many approaches in choosing the zero sequence signal leading to a variety of carrier based pwm schemes in the literatures see hava et al amongst them a commonly encountered selection of the zero sequence signal for injection is the third harmonic injection pwm technique third harmonics injection pwm thipwm exploits the third harmonic component of the desired reference signal as the injection signal there are two types of injections with different amplitudes of the third harmonic with the desired reference voltage defined in the thipwm with one sixth of the reference amplitude is given by n m sin et similarly the thipwm is given by n m sin et pid and predictive control of electrical drives and power converters using matlab simulink figure waveform of thipwm key dashed line original sinusoidal wave dotted line m sin et solid line sinusoidal wave with third harmonic injection with the third harmonics injection the peak value of modified desired sinusoidal is reduced by the injection as the waveform in figure demonstrates it can shown that the maximum amplitude of the three phase voltage signals is reduced to v dc v dc if the thipwm is used and to v dc if the thipwm technique is used the ratio between the maximum amplitude of the three phase voltage signals and the value vdc is called the modulation index which is m max thipwm vdc v dc for the thipwm and m max thipwm vdc v dc for the thipwm both modulation indices indicate that the linear modulation range is larger than the one generated by the original sinusoidal pwm note that thipwm is derived for the purpose of the minimization of the total harmonic distortion thd whereas thipwm is designed based on maximizing the linear modulation range see bowes and lai now with the third harmonic injection technique for the modulation to work within the linear range where its fundamental resembles the desired signal it requires that the maximum amplitude of the sinusoidal signal be less than vdc or v dc depending on whether thipwm or thipwm is being used basically the modulation index will quantify the linear modulation range of the three phase voltage signals with respect to the results from the original sinusoidal pwm the linear modulation range is translated into operational constraints from the perspective of controller design in later chapters control of semiconductor switches via pwm technologies space vector pwm space vector pwm svpwm as its name conveys utilizes the concept of space vector and its geometrical features to derive the on off time duration for each switch similar to the definition of mmf space vector in the space vector of three phase reference voltage is defined as vs an bnej cnej if a balanced three phase voltage is employed then v s is a rotating vector with electrical speed e which is the frequency of sinusoidal signal the modulation of the desired space vector v s is obtained by the time average of its two nearest active vectors and a zero vector either v or v taking the first sector for example as illustrated in figure vs could be modulated with the time average of the active vector v and v within one sampling period ts by ts vs t v t v where t and t are the duration of on time for the active vectors v and v respectively the relationship between the modulated vector v s and two nearest active vectors is obtained by applying the geometric properties of the triangle ts v s sin t v sin t v sin that implies the duty cycle ratio of each active vector is t ts v s v dc sin sin v s v dc sin t ts v s v dc sin sin v s v dc sin v v v v v v vi vdc v v v max vdc t t vs ts vs t v t v t t ts t t figure principle of svm pid and predictive control of electrical drives and power converters using matlab simulink where the length of each active vector vi vdc is used it follows that the duration of the zero vector applied is the remaining time of the sampling period t t ts t t since the hexagon has sixfold symmetry the geometrical method discussed above can be used for the other five sectors as well by rotating the modulation vector by m rads with m denoting the sector in which it locates the conventional svpwm symmetrically distributes the four switching vectors two active vectors and two zero vectors within one sampling time as shown in figure such an arrangement offers the benefits of fixed switching frequency and better harmonics reduction performance figure shows a digital implementation of svpwm within one sampling period for the example illustrated in figure there are four switching vectors denoted by v v v v corresponding to the four on time periods t t t and t calculated using the arrangement of the four switching vectors is shown in the top part of figure together with the three switching states sa sb and s c it is seen that the arrangement begins with the zero vector v and ends with the zero vector v for the first half and symmetrical with the second half of the graph this forms a symmetrical pattern from the center of the graph the individual on time period is also illustrated in the top part of figure the bottom part of figure illustrates how spvpwm is implemented in the direct digital implementation a parameter max is selected and a internal counter is set to count up and down within one sampling period ts to form two straight lines as illustrated which can be described by the linear equations pwm time max ts t t ts pwm time max max ts t ts t t s in the bottom part of figure on the vertical axis marked are the parameter max and the three igbt switching counts pwms a pwmsb pwmsc with the on time periods t t t and t calculated max ts p w msc p w msa p w msb s a sb s c v v v v v v v v t t t t t t t t figure digital implementation of svpwm control of semiconductor switches via pwm technologies the corresponding count at which the igbt switch turn on period is calculated using the following ratios pwm s a max t ts pwm sb max t t ts pwm s c max t t t ts in this example at the beginning of the sample period sa sb sc when pwmtime pwms a and pwm time is increasing the switching state sa similarly when pwmtime pwmsb and pwmtime is increasing the switching state sb the same conditions apply to the switching state sc after the time t reaches ts the operations continue in reverse namely when pwmtime pwms c and pwm time is decreasing the switching state sc the same conditions apply to the switching states sb and sa to complete one cycle of space vector implementation of pwm the maximum amplitude of the three phase voltage signals to be realized by the space vector modulation technique is seen from figure which is vdc thus the modulation index is calculated as m svpwm to ensure that the modulation is within the linear modulation range it requires that voltages in the reference frame satisfy t t vdc this is based on the definition of the voltage space vector in relation to the voltage variables in reference frame see similarly the voltages in the d q reference frame are also required to satisfy d t q t vdc based on the definition of the voltage space vector in relation to those in the d q reference frame see the inequalities and will be used as constraints in the control system design and implementation see chapter note that the maximum amplitude of the three phase voltage signals when using the pwm with the third harmonic injection technique is identical to the case when using the space vector modulation technique which is vdc thus in the applications the limit on the amplitude of the control signal is often taken as vdc simulation study of the effect of pwm since the time varying input voltages for ac drives and power converter are to be realized by the pwm generation and inverter taking the pmsm as an example the complete plant model as shown in figure consists of machine model of pmsm pwm and igbt inverters note that figure is an equivalent representation in d q frame which does not illustrate the real hardware implementation in three phase representation when the modulation signal i works within the linear modulation region of chosen pwm the fundamental of output voltage i from the igbt inverter approximates the desired modulation signal that is i i with the combination of park clarke and its inverse transformation of which their multiplication is an identity matrix it is assumed that d d and q q when using igbt inverters as actuators for controller implementation pid and predictive control of electrical drives and power converters using matlab simulink dq abc pwm igbt abc dq pmsm model id i q vd v q v a v b v c v a vd v b v c v q s a s b s c figure model of pmsm combined with pwm generator to study the impact of carrier frequency in pwm simulation studies are performed using the physical parameters of the pmsm given in table two sets of simulation results are obtained by using the pmsm matlab simulink model illustrated in figure the first set of simulation results uses a smaller pwm carrier frequency where fc khz in contrast with the second set of simulation results where a larger carrier frequency fc khz is utilized the voltage input signals to the pmsm model are set as d v and q v and the dc bus voltage is fixed at v so that the modulation signal is within its linear modulation range figure a shows the three phase currents time s phase current a i a ib i c a frequency khz normalized ampitude b figure spectrum of phase currents fc khz d v and q v control of semiconductor switches via pwm technologies and figure b shows the corresponding spectrum of a phase current for the lower carrier frequency case it is seen that with this lower carrier frequency the ia ib and ic currents contain a large amount of high frequency harmonic noise see figure a and the harmonics mainly occur around the carrier frequency and its multiples see figure b that is n khz with n denoting an integer in this case when the carrier frequency is increased to fc khz the high frequency harmonic noise in the three phase currents is reduced as seen in figure a and the current ripple still occurs at the carrier frequency see figure b but the multiples move to the high frequency region at a certain high frequency region the current ripple is then attenuated by the limited bandwidth of the pmsm which is a first order type of system depending on the values of resistance rs and inductance ld lq in summary the high frequency harmonic ripple with a low carrier frequency is much more severe than the one with a high carrier frequency and the harmonics occur around the carrier frequency and its multiples thus a higher carrier frequency in pwm offers improved performance in terms harmonics attenuation however in practice the resulting high switching loss inherent characteristics of switching devices and limited computational power prevent the use of a very high carrier frequency time s phase current a i a ib i c a frequency khz normalized ampitude b figure spectrum of phase currents fc khz d v and q v a three phase current and b spectrum of ia at steady state pid and predictive control of electrical drives and power converters using matlab simulink summary two popular categories of pwm techniques carrier based pwm and space vector based pwm have also been briefly revisited it has been shown in zhou and wang that both implementation approaches can lead to the same type of pwm such as svpwm thipwm and several variations of discontinuous pwm dpwm schemes see hava et al zhou and wang the implementation of these pwm schemes is achieved by appropriate zero sequence injection in the carrier based pwm approach while its corresponding equivalence is the varieties of the placement of zero vector in the space vector based pwm approach the maximum modulation index of the linear modulation ranges for different pwm is discussed simulation results show that the pwm techniques have a reduced effect on the system when the carrier frequency is increased further reading a book was written on pwm by holmes and lipo an early work on sinusoidal pwm generator was presented in bowes survey papers published on pwm technologies included holtz and holtz pwm schemes based on voltage space vectors were realized and analyzed in van der broeck et al relationships between space vector modulation and three phase carrier based pwm were analyzed in zhou and wang blasko bowes and lai and chai and wang carrier based pwm vsi over modulation strategies were designed analyzed and compared in hava et al graphic methods were discussed for carrier based pwm vsi drives in hava et al space vector modulation implementation with the admcf x was discussed in analog devices inc geyer made comparative studies of control and modulation schemes for medium voltage drives between predictive control concepts and pwm based schemes references analog devices inc implementing space vector modulation with the admcf x blasko v analysis of a hybrid pwm based on modified space vector and triangle comparison methods ieee transactions on industry applications bowes s new sinusoidal pulsewidth modulated invertor electrical engineers proceedings of the institution of bowes s and lai ys the relationship between space vector modulation and regular sampled pwm ieee transactions on industrial electronics chai s and wang l a unified pulse generation approach for l vsi from svpwm to direct switching industrial electronics society iecon th annual conference of the ieee pp geyer t a comparison of control and modulation schemes for medium voltage drives emerging predictive control concepts versus pwm based schemes ieee transactions on industry applications hava a kerkman r and lipo t carrier based pwm vsi overmodulation strategies analysis comparison and design ieee transactions on power electronics hava a kerkman r and lipo t simple analytical and graphical methods for carrier based pwm vsi drives ieee transactions on power electronics holmes d and lipo t pulse width modulation for power converters principles and practice vol wiley holtz j pulsewidth modulation a survey ieee transactions on industrial electronics holtz j pulsewidth modulation for electronic power conversion proceedings of the ieee kreyszig e advanced engineering mathematics th edn john wiley sons inc new york van der broeck h skudelny hc and stanke g analysis and realization of a pulsewidth modulator based on voltage space vectors ieee transactions on industry applications zhou k and wang d relationship between space vector modulation and three phase carrier based pwm a comprehensive analysis ieee transactions on industrial electronics robust control systems introduction robust control systems and system sensitivity analysis of robustness systems with uncertain parameters the design of robust control systems the design of robust pid controlled systems the robust internal model control system design examples the pseudo quantitative feedback system robust control systems using control design software sequential design example disk drive read system summary preview physical systems and the external environment in which they operate cannot be modeled precisely may change in an unpredictable manner and may be subject to significant disturbances the design of control systems in the presence of significant uncertainty motivates the concept of robust control system design advances in robust control design methodologies can address stability robustness and performance robustness in the presence of uncertainty in this chapter we describe five methods for robust design including root locus frequency response itae methods for a robust pid systems internal model control and pseudo quantitative feedback methods however we should also realize that classical design techniques may also produce robust control systems control engineers who are aware of these issues can design robust pid controllers robust lead lag controllers and so forth the chapter concludes with a pid controller design for the sequential design example disk drive read system desired outcomes upon completion of chapter students should appreciate the role of robustness in control system design be familiar with uncertainty models including additive uncertainty multiplicative uncertainty and parameter uncertainty understand the various methods of tackling the robust control design problem using root locus frequency response itae methods for pid control internal model and pseudo quantitative feedback methods section introduction introduction a control system designed using the methods and concepts of the preceding chapters assumes knowledge of the model of the process and controller and constant parameters the process model will be an inaccurate representation of the actual physical system due to parameter changes unmodeled dynamics unmodeled time delays changes in equilibrium point operating point sensor noise unpredicted disturbance inputs the goal of robust control system design is to maintain acceptable closed loop system performance in the presence of model inaccuracies and changes b td s a td s y s g s g c s n s r s controller g c s n s r s y s process g s ea s figure closed loop control system a signal flow graph b block diagram a robust control system maintains acceptable performance in the presence of significant model uncertainty disturbances and noise a system structure that incorporates system uncertainties is shown in figure this model includes the sensor noise n s the disturbance input td s and a process g s with unmodeled dynamics or parameter changes the chapter robust control systems unmodeled dynamics and parameter changes may be significant and for these systems the challenge is to create a design that retains the desired performance robust control systems and system sensitivity designing highly accurate systems in the presence of significant plant uncertainty is a classical feedback design problem the theoretical bases for the solution of this problem date back to the works of h s black and h w bode in the early s when this problem was referred to as the sensitivities design problem a significant amount of literature has been published since then regarding the design of systems subject to large process uncertainty the designer seeks to obtain a system that performs adequately over a large range of uncertain parameters a system is said to be robust when it is durable hardy and resilient a control system is robust when it has low sensitivities it is stable over the expected range of parameter variations and the performance continues to meet the specifications in the presence of a set of changes in the system parameters robustness is the low sensitivity to effects that are not considered in the analysis and design phase for example disturbances measurement noise and unmodeled dynamics the system should be able to withstand these neglected effects when performing the tasks for which it was designed for small parameter perturbations we may use as a measure of robustness the differential sensitivities discussed in sections system sensitivity and section root sensitivity the system sensitivity is defined as s ta t t a a where a is the parameter and t the transfer function of the system the root sensitivity is defined as s ria ri a a when the zeros of t s are independent of the parameter a we showed that s ta a n i s ria s ri for an nth order system for example if we have a closed loop system as shown in figure where the variable parameter is a then t s s a and s ta a s a this follows because r a and s ria a section robust control systems and system sensitivity therefore s ta s ria s a a s a let us examine the sensitivity of the second order system shown in figure the transfer function of the closed loop system is t s k s s k the system sensitivity for k is s s sk t s s s s k a bode plot of the asymptotes of log t jv and log s jv is shown in figure for k critical damping note that the sensitivity is small for lower frequencies while the transfer function primarily passes low frequencies of course the sensitivity s s only represents robustness for small changes in gain if k changes from k within the range k to k the resulting range of step response is shown in figure this system with an expected wide range of r s y s s a figure a first order system r s k y s s s figure a second order system magnitude db log t log s k frequency rad s figure sensitivity and log t jv for the second order system in figure the asymptotic approximations are shown for k y t time s k k k figure the step response for selected gain k chapter robust control systems k may not be considered adequately robust a robust system would be expected to yield essentially the same within an agreed upon variation response to a selected input example sensitivity of a controlled system consider the system shown in figure where g s s and a pd controller g c s kp kds then the sensitivity with respect to changes in g s is sg t g c s g s s s kds kp and t s kds kp s kds kp consider the nominal condition z and vn kp then kd vn to achieve z therefore we may plot log s and log t on a bode plot as shown in figure note that the frequency vn is an indicator on the boundary between the frequency region in which the sensitivity is the important design criterion and the region in which the stability margin is important thus if we specify vn properly to take into consideration the extent of modeling error and the frequency of external disturbance we can expect the system to have an acceptable amount of robustness example system with a right hand plane zero consider the system shown in figure where the plant has a zero in the righthand plane the closed loop transfer function is t s k s s k s k the system is stable for a gain k the steady state error due to a negative unit step input r s s is e ss k k controller process kp k r s d s y s s figure a system with a pd controller section robust control systems and system sensitivity and e ss when k the response is shown in figure note the initial undershoot at t s this system is sensitive to changes in k as recorded in table the performance of this system might not be acceptable for a change of gain of only thus this system would not be considered robust the steady state error of this system changes greatly as k changes magnitude db v v n log t log s figure sensitivity and t s for the secondorder system in figure r s k y s s s g s figure a second order system y t time s figure step response of the system in figure with k table results for example k ess undershoot settling time seconds chapter robust control systems analysis of robustness system goals include maintaining a small tracking error for an input r s and keeping the output y s small for a disturbance td s the sensitivity function is s s g c s g s and the complementary sensitivity function is c s g c s g s g c s g s we also have the relationship s s c s for physically realizable systems the loop gain l s gc s g s is small for high frequencies this means that s jv approaches at high frequencies consider the closed loop system shown in figure an additive perturbation characterizes the set of possible processes as follows g a s g s a s where g s is the nominal process and a s is the perturbation that is bounded in magnitude we assume that ga s and g s have the same number of poles in the right hand s plane if any then the system stability will not change if a jv gc jv g jv for all v this assures stability but not dynamic performance a multiplicative perturbation is modeled as g m s g s m s the perturbation is bounded in magnitude and it is again assumed that gm s and g s have the same number of poles in the right hand s plane then the system stability will not change if m jv gc jv g jv for all v equation is called the robust stability criterion this is a test for robustness with respect to a multiplicative perturbation this form of perturbation is often usedsection analysis of robustness because it satisfies the intuitive properties of being small at low frequencies where the nominal process model is usually well known and being large at high frequencies where the nominal model is always inexact example system with multiplicative perturbation consider the system of figure with gc k and g s s s s s s the system is unstable with k but a reduction in gain to k will stabilize it now consider the effect of an unmodeled pole at rad s in this case the multiplicative perturbation is determined from m s s or m s s s the magnitude bound is then m jv jv jv m jv and kg jv are shown in figure a where it is seen that the criterion of equation is not satisfied thus the system may not be stable if we use a lag compensator g c s s s the loop transfer function is l s gc s g s we reshape the function g c jv g jv in the frequency range v and check the condition m jv gc jv g jv as shown in figure b here the robustness inequality is satisfied and the system is robustly stable the control objective is to design a compensator gc s so that the transient steady state and frequency domain specifications are achieved and the cost of feedback measured by the bandwidth of the compensator gc jv is sufficiently small this bandwidth constraint is needed mainly because of measurement noise in subsequent sections we discuss including a pre filter in a two degree of freedom configuration to help achieve the design goals chapter robust control systems systems with uncertain parameters many systems have several parameters that are constants but uncertain within a range for example consider a system with a characteristic equation sn a n sn an sn g a with known coefficients within bounds ai ai bi and i c n where a n to ascertain the stability of the system we might have to investigate all possible combinations of parameters fortunately it is possible to investigate a limited number of worst case polynomials the analysis of only four polynomials is frequency rad s frequency rad s a b magnitude magnitude g p c jv g jv p m jv m jv p kg jv p figure the robust stability criterion for example section systems with uncertain parameters sufficient and they are readily defined for a third order system with a characteristic equation s a s a s a the four polynomials are q s s a s b s b q s s b s a s a q s s b s b s a q s s a s a s b one of the four polynomials represents the worst case and may indicate either unstable performance or at least the worst performance for the system in that case example third order system with uncertain parameters consider a third order system with uncertain coefficients such that a a b a a b a a b the four polynomials are q s s s s q s s s s q s s s s q s s s s we then proceed to check these four polynomials by means of the routh hurwitz criterion and determine that the system is stable for all the range of uncertain parameters example stability of uncertain system consider a unity feedback system with a process transfer function under nominal conditions g s s s s the nominal characteristic equation is then q s s s s where a a and a using the routh hurwitz criterion we find that this system is nominally stable however if the system has uncertain coefficients such that a a b a a b and a a b chapter robust control systems then we must examine the four polynomials q s s s s q s s s s q s s s s q s s s s using the routh hurwitz criterion q s and q s are stable and q s is marginally stable for q s we have s s s s therefore the system is unstable for the worst case where a minimum a minimum and b maximum this occurs when the process has changed to g s s s s note that the third pole has moved toward the jv axis to its limit at s and that the gain has increased to its limit at k the design of robust control systems the design of robust control systems involves determining the structure of the controller and adjusting the controller parameters to achieve acceptable performance in the presence of uncertainty the structure of the controller is chosen such that the system response can meet certain performance criteria one possible objective in the design of a control system is that the controlled system output should very accurately track the input that is we want to minimize the tracking error in an ideal setting the bode plot of the loop gain l s would be db gain of infinite bandwidth and zero phase shift in practice this is not possible one possible design objective is to maintain the magnitude response curve as flat and as close to unity for as large a bandwidth as possible for a given plant and controller combination another important goal of a control system design is that the effect on the output of the system due to disturbances is minimized consider the control system shown in figure where g s is the plant and td s is the disturbance we then have t s y s r s g c s g s g c s g s section the design of robust control systems and y s td s g s g c s g s note that both the reference and disturbance transfer functions have the same denominator in other words they have the same characteristic equation namely g c s g s l s recall that the sensitivity of t s with respect to g s is sg t g c s g s equation shows that for low sensitivity we desire a high value of loop gain l jv but it is known that a high gain can lead to instability and amplification of the measurement noise thus we seek the following t s with wide bandwidth large loop gain l s at low frequencies small loop gain l s at high frequencies setting the design of robust systems in frequency domain terms we scale a compensator gc s such that the closed loop sensitivity is less than some tolerance value but sensitivity minimization involves finding a compensator such that the closed loop sensitivity is minimized the gain and phase margin problem is to find a compensator to achieve prescribed gain and phase margins the disturbance rejection problem and measurement noise attenuation problem seeks a solution with high loop gain at low frequencies and low loop gain at high frequencies respectively for the frequency domain specifications we seek the following conditions for the bode plot of gc jv g jv shown in figure for relative stability the loop gain must have not more than a db decade slope at or near the crossover frequency vc steady state accuracy and measurement noise rejection achieved by the low gain at high frequency disturbance rejection by a high gain over low frequencies accuracy over a bandwidth vb by maintaining the loop gain above a prescribed level r s y s td s gc s g s figure a system with a disturbance chapter robust control systems using the root sensitivity concept we can state that sa r must be minimized while attaining t s with dominant roots that will provide the appropriate response and minimize the effect of td s as an example let gc s k and g s s s for the system in figure this system has two roots and we select a gain k so that y s td s is minimized sk r is minimized and t s has desirable dominant roots the sensitivity is sk r dr dk k r ds dk s r kr and the characteristic equation is s s k therefore dk ds s since k s s we then obtain sk r s s s s s r when z the roots are complex and r j k then sk r kk the magnitude of the root sensitivity is shown in figure for k to k the percent overshoot to a step is also shown as illustrated in figure select k l yields a near minimum sensitivity while maintaining good performance for the step response to reduce the root sensitivity while simultaneously minimizing the effect of disturbances we can use the design procedure as follows sketch the root locus of the compensated system with gc s chosen to attain the desired location for the dominant roots maximize the gain of gc s to reduce the effect of the disturbance determine sk r and attain the minimum value of the root sensitivity consistent with the transient response required as described in step log gcg minimum performance bounds high gains for good performance command following stable crossover gain and phase margins robustness bounds low gains to reduce sensitivity to sensor noise and model uncertainty v c figure bode plot for log gc jv g jv section the design of robust control systems example sensitivity and compensation consider the system in figure when g s s and gc s is to be selected by frequency response methods therefore the compensator is to be selected to achieve an appropriate gain and phase margin while minimizing sensitivity and the effect of the disturbance thus we choose g c s k s z s p choose k to reduce the effect of the disturbance to attain a phase margin of select z and p the compensated diagram is shown in figure the closed loop bandwidth is vb vc thus we will increase the bandwidth by using the compensator the sensitivity at vc is sg t jvc gc jv g jv v vc k percent overshoot sensitivity percent overshoot srk figure sensitivity and percent overshoot for a second order system log l db v f v uncompensated magnitude db dec compensated magnitude phase margin p m compensated phase angle uncompensated phase angle zero pole v c figure bode plot for example chapter robust control systems to estimate sg t we recall that the nichols chart enables us to obtain t jv gcg jcv jvg gjv jv we can plot points of gc jv g jv on the nichols chart and then read t jv from the chart then we have sg t jv t jv gc jv g jv where v is chosen at a frequency below vc the nichols chart for the compensated system is shown in figure for v vc we have log t jv db and log gc jv g jv db therefore s jv t jv gc jv g jv the design of robust pid controlled systems the pid controller has the transfer function g c s kp ki s kds the popularity of pid controllers can be attributed partly to their robust performance over a wide range of operating conditions and partly to their functional simplicity which allows engineers to operate them in a straightforward manner to implement the pid controller three parameters must be determined for the given process proportional gain integral gain and derivative gain consider the pid controller g c s kp ki s kds kds kps ki s kd s as b s kd s z s z s where a kp kd and b ki kd therefore a pid controller introduces a transfer function with one pole at the origin and two zeros recall that a root locus begins at the poles and ends at the zeros if we have a system as shown in figure with g s s s and we use a pid controller with complex zeros we can plot the root locus as shown in figure as the gain kd of the controller is increased the complex roots approach the zeros the closed loop transfer function issection the design of robust pid controlled systems t s g s gc s gp s g s gc s kd s z s zn s r s r s nr gp s m kdgp s s r g cg g phase of cg loop phase j gcg degrees loop gain gcg decibels g cg g cg g cg g cg magnitude of db loop gain phase diagram versus g db db db db db db db db db db db db db db db db db db db db db db db gg c jv v v v v figure nichols chart for example chapter robust control systems because the zeros and the complex roots are approximately equal r l z setting gp s we have t s kd s r l kd s kd when kd w the only limiting factor is the allowable magnitude of u s figure when kd is large if kd is the system has a fast response and zero steady state error furthermore the effect of the disturbance is reduced significantly in general we note that pid controllers are particularly useful for reducing steady state error and improving the transient response when g s has one or two poles or may be approximated by a second order process the main problem in the selection of the three pid controller parameters is that these coefficients do not readily translate into the desired performance and robustness characteristics that the control system designer has in mind several rules and methods have been proposed to solve this problem in this section we consider several design methods using root locus and performance indices the first design method uses the itae performance index hence we select the three pid coefficients to minimize the itae performance index which produces r s y s ea s u s td s gp s gc s g s figure feedback control system with a desired input r s and an undesired input td s j j j j r z r kd increasing z r figure root locus with z j section the design of robust pid controlled systems an excellent transient response to a step or a ramp the design procedure consists of three steps select the v n of the closed loop system by specifying the settling time determine the three coefficients using the appropriate optimum equation table and the v n of step to obtain gc s determine a prefilter gp s so that the closed loop system transfer function t s does not have any zeros example robust control of temperature consider a temperature controller with a control system as shown in figure and a process g s s if g c s the steady state error is ess and the settling time with a criterion is ts s for a step input we want to obtain an optimum itae performance for a step input and a settling time of ts s using a pid controller we have t s y s r s g c s g s g c s g s kds kps ki s kd s kp s ki where g p s the optimum coefficients of the characteristic equation for itae are s v ns vn s vn we need to select v n in order to meet the settling time requirement since ts zvn and z is unknown but near we set vn then equating the denominator of equation to equation we obtain the three coefficients as kp kd and ki then equation becomes t s s s s s s s j s j s s s the response of this system to a step input has a percent overshoot of p o as recorded in table we select a prefilter gp s so that we achieve the desired itae response with t s g c s g s gp s g c s g s s s s chapter robust control systems table results for example controller g c s pid and gp s pid with gp s prefilter percent overshoot settling time seconds steady state error disturbance error therefore we require that gp s s s in order to eliminate the zeros in equation and bring the overall numerator to the response of the system t s to a step input is indicated in table the system has a small percent overshoot a settling time of ts s and zero steady state error furthermore for a disturbance td s s the maximum value of y t due to the disturbance is of the magnitude of the disturbance this is a very favorable design let us consider the system when the plant varies significantly so that g s k ts where t and k we want to achieve investigate the behavior using the itae optimum system with the prefilter the objective is to have an overshoot of p o and a settling time with a criterion of ts s while g s can attain any value in the range indicated we then obtain the step response for the four conditions t k t k t k and t k the results are summarized in figure this is a very robust system the value of v n that can be chosen will be limited by considering the maximum allowable u t where u t is the output of the controller as shown in figure as an example consider the system in figure with a pid controller g s s s and the necessary prefilter gp s to achieve itae performance if we select v n and the maximum value of u t is as recorded in table if we wish to limit u t we need to limit vn thus we are limited in the settling time we can achieve the robust internal model control system the internal model control system is shown in figure we now consider the use of the internal model design with special attention to robust system performance the internal model principle states that if gc s g s contains r s then y s will track r s asymptotically in the steady state and the tracking is robust section the robust internal model control system consider a simple system with g s s for which we seek a ramp response with a steady state error of zero a pi controller is sufficient and we let k no state variable feedback then we have g c s g s kp ksi s kpss ki note that for a ramp r s s which is contained as a factor of equation and the closed loop transfer function is t s kp s ki s k ps ki table maximum value of plant input v n u t maximum for r s s settling time seconds time s y t k t k t k t k t figure response of the closed loop system in the presence of uncertainty in k and r s gc s y s ea s g s process k x figure the internal model control system chapter robust control systems using the itae specifications for a ramp response we require that t s v ns vn s v ns vn we select v n to satisfy a specification for the settling time for a settling time with a criterion of ts s we select vn then we require kp and ki the response of this system settles in ts s and then tracks the ramp with zero steady state error if this system designed for a ramp input receives a step input the response has a percent overshoot of p o and a settling time of ts s this system is very robust to changes in the plant for example if g s k s changes gain so that k varies by the change in the ramp response is insignificant example design of an internal model control system consider the system of figure with state variable feedback and a compensator g c s we wish to track a step input with zero steady state error here we select a pid controller for g c s we then have g c s kds kps ki s and g s gc s will contain r s s the input command note that we feed back both state variables and add these additional signals after gc s in order to retain the integrator in gc s the goal is to achieve a settling time to within of the final value of ts second and a deadbeat response while retaining a robust response here we assume that the two poles of g s can change by then the worst case condition is gn s s s one design approach is to design the control for this worst case condition another approach which we use here is to design for the nominal g s and one half the desired settling time then we expect to meet the settling time requirement and attain a very fast highly robust system note that the prefilter gp s is used to attain the desired form for t s r s y s process g s x s x s g gp s c s s ka kb s figure an internal model control with state variable feedback and g c s section design examples the response desired is deadbeat so we use a third order transfer function as t s v n s v ns vn s vn and the settling time with a criterion is ts vn for a settling time of ts s we use vn the closed loop transfer function of the system of figure with the appropriate gp s is t s ki s kd kb s kp ka kb s ki we let k a kb kp ki and kd note that t s could be achieved with other gain combinations the step response of this system has a deadbeat response with a percent overshoot of p o and a settling time of ts s when the poles of g s change by the percent overshoot changes to p o and the settling time is t s s this is an outstanding design of a robust deadbeat response system design examples in this section we present two illustrative examples the first example illustrates the design of two degree of freedom controllers that is two separate controllers for an ultra precision diamond turning machine in the second design example we consider the practical problem of designing a controller in the presence of an uncertain time delay the specific problem under investigation is a pid controller for a digital audio tape drive the design process is highlighted with an emphasis on robustness example ultra precision diamond turning machine the design of an ultra precision diamond turning machine has been studied at lawrence livermore national laboratory this machine shapes optical devices such as mirrors with ultra high precision using a diamond tool as the cutting device in this discussion we will consider only the z axis control using frequency response identification with sinusoidal input to the actuator we determined that g s s the system can accommodate high gains since the input command is a series of step commands of very small magnitude a fraction of a micron the system has an outer loop for position feedback using a laser interferometer with an accuracy of micron m an inner feedback loop is also used for velocity feedback as shown in figure chapter robust control systems we want to select the controllers g s and g s to obtain an overdamped highly robust high bandwidth system the robust system must accommodate changes in g s due to varying loads materials and cutting requirements thus we seek a large phase margin and gain margin for the inner and outer loops and low root sensitivity the specifications are summarized in table since we want zero steady state error for the velocity loop we propose a velocity loop controller g s g s g s where g s is a pi controller and g s is a phase lead compensator thus we have g s g s g s kp ksi k s a ka s and choose kp ki k and a we now have g s kp s s s s the root locus for g s g s is shown in figure when kp we have the velocity closed loop transfer function given by t s v s u s s s s s s l s u s r s position command microns v s velocity y s position position controller g s velocity controller g s s actuator and cutter g s laser interferometer tachometer figure turning machine control system table specifications for turning machine control system transfer function specification velocity v s u s position y s r s minimum bandwidth rad s rad s steady state error to a step minimum damping ratio z maximum root sensitivity sk r minimum phase margin minimum gain margin db dbsection design examples imaginary axis real axis kp increasing figure root locus for velocity loop as kp varies table design results for turning machine control system achieved result velocity position transfer position transfer function y s r s closed loop bandwidth rad s rad s steady state error damping ratio z root sensitivity sk r phase margin gain margin infinite db which is a large bandwidth system the actual bandwidth and root sensitivity are summarized in table note that we have exceeded the specifications for the velocity transfer function we propose a phase lead compensator for the position loop of the form g s k k s a ka s and we choose a and k so that g s k s s we then plot the root locus for the loop transfer function l s g s t s s if we use the approximate t s of equation we have the root locus of figure a using the actual t s we get the close up of the root locus shown in figure b we select kp and achieve the actual results for the total system transfer function as recorded in table the total system has a high phase margin has a low sensitivity and is overdamped with a large bandwidth this system is very robust chapter robust control systems example digital audio tape controller consider the feedback control system shown in figure where gd s e ts the exact value of the time delay is uncertain but is known to lie in the interval t t t define g m s e tsg s then g m s g s e tsg s g s e ts g s imaginary axis real axis k increasing a b jv s figure the root locus for k for a overview and b close up near origin of the s plane section design examples or g m s g s e ts if we define m s e ts then we have g m s m s g s in the development of a robust stability controller we would like to represent the time delay uncertainty in the form shown in figure where we need to determine a function m s that approximately models the time delay this will lead to the establishment of a straightforward method of testing the system for stability robustness in the presence of the uncertain time delay the uncertainty model is known as a multiplicative uncertainty representation since we are concerned with stability we can consider r s then we can manipulate the block diagram in figure to obtain the form shown in figure using the small gain theorem we have the condition that the closed loop system is stable if m jv gc jv g jv for all v g c s g s controller e ts time delay r s y s process figure a feedback system with a time delay in the loop g c s g s controller r s y s process m s z e figure multiplicative uncertainty representation m s z e g c s g s g c s g s figure equivalent block diagram depiction of the multiplicative uncertainty chapter robust control systems the challenge is that the time delay t is not known exactly one approach to solving the problem is to find a weighting function denoted by w s such that e jvt w jv for all v and t t t if w s satisfies the inequality in equation it follows that m jv w jv therefore the robust stability condition can be satisfied by w jv gc jv g jv for all v this is a conservative bound if the condition in equation is satisfied then stability is guaranteed in the presence of any time delay in the range t t t if the condition is not satisfied the system may or may not be stable suppose we have an uncertain time delay that is known to lie in the range t we can determine a suitable weighting function w s by plotting the magnitude of e jvt as shown in figure for various values of t in the range t t t a reasonable weighting function obtained by trial and error is w s s s frequency rad s magnitude t s t s t s w jv figure magnitude plot of e jvt for t and section design examples this function satisfies the condition e jvt w jv keep in mind that the selection of the weighting function is not unique a digital audio tape dat stores gigabytes of data in a package the size of a credit card roughly nine times more than a half inch wide reel to reel tape or quarter inch wide cartridge tape a dat sells for about the same amount as a floppy disk even though it can store times more data a dat can record for two hours longer than either reel to reel or cartridge tape which means that it can run longer unattended and requires fewer changes and hence fewer interruptions of data transfer dat gives access to a given data file within seconds on the average compared with several minutes for either cartridge or reel to reel tape the tape drive electronically controls the relative speeds of the drum and tape so that the heads follow the tracks on the tape as shown in figure the control system is complex because motors have to be accurately controlled capstan take up and supply reels drum and tension control the elements of the design process emphasized in this example are highlighted in figure consider the speed control system shown in figure the motor and load transfer function varies because the tape moves from one reel to the other the transfer function is g s km s p s p where nominal values are k m p and p however the range of variation is k m p and p thus the process belongs to a family of processes where each member corresponds to different values of k m p and p the design goal is design goal control the dat speed to the desired value in the presence of significant process uncertainties take up reel supply reel write head guide roller out guide roller in fixed post capstan guide roller fixed post pinch roller fixed post tension post fixed post tape read head rotary drum y t figure digital audio tape driver mechanism chapter robust control systems see equation see equation see figures establish the system configuration obtain a model of the process the actuator and the sensor if the performance meets the specifications then finalize the design if the performance does not meet the specifications then iterate the configuration identify the variables to be controlled establish the control goals topics emphasized in this example write the specifications optimize the parameters and analyze the performance describe a controller and select key parameters to be adjusted control the dat speed to the desired value in the presence of significant plant uncertainties dat speed y s design specifications ds p o and t s s ds robust stability see figures and figure elements of the control system design process emphasized in this digital audio tape speed control design g c s g s controller r s y s motor and load figure block diagram of the digital audio tape speed control system associated with the design goal we have the variable to be controlled defined as variable to be controlled dat speed y s the design specifications are design specifications ds percent overshoot of p o and settling time of ts s for a unit step input ds robust stability in the presence of a time delay at the plant input the time delay value is uncertain but known to be in the range t section design examples design specification ds must be satisfied for the entire family of plants design specification ds must be satisfied by the nominal process km p p the following constraints on the design are given fast peak time requires that an overdamped condition is not acceptable use a pid controller g c s kp ki s kds k mkd when km the key tuning parameters are the pid gains select key tuning parameters kp ki and kd since we are constrained to have k mkd when km we must select kd we will design the pid controller using nominal values for km p and p we then analyze the performance of the controlled system for the various values of the process parameters using a simulation to check that ds is satisfied the nominal process is given by g s s s the closed loop transfer function is t s kds kps ki s kd s kp s ki if we choose kd then we write the characteristic equation as s s s kps ki or kp s ki kp s s s per specifications we try to place the dominant poles in the region defined by zvn and z we need to select a value of t ki kp and then we can plot the root locus with the gain kp as the varying parameter after several iterations we choose a reasonable value of t the root locus is shown in figure we determine that kp represents a valid selection since the roots lie inside the desired performance region we obtain kp and ki tkp the pid controller is then given by g c s s s the step response for the process with nominal parameter values is shown in figure a family of responses is shown in figure for various values of chapter robust control systems real axis imaginary axis performance region selected figure point kp root locus for the dat system with kd and t ki kp time s y t figure unit step response for the dat system with kp kd and ki section design examples km p and p none of the responses suggests a percent overshoot over the specified value of p o and the settling times are all under the ts s specification as well as we can see in figure all of the tested processes in the family are adequately controlled by the single pid controller in equation therefore ds is satisfied for all processes in the family suppose the system has a time delay at the input to the process the actual time delay is uncertain but known to be in the range t s following the method discussed previously we determine that a reasonable function w s which bounds the plots of e jvt for various values of t is w s s s to check the stability robustness property we need to verify that w jv gc jv g jv for all v the plot of both w jv and gc jv g jv is shown in figure it can be seen that the condition in equation is indeed satisfied therefore we expect that the nominal system will remain stable in the presence of time delays up to seconds time s y t figure a family of step responses for the dat system for various values of the process parameters km p and p chapter robust control systems the pseudo quantitative feedback system quantitative feedback theory qft uses a controller as shown in figure to achieve robust performance the goal is to achieve a wide bandwidth for the closedloop transfer function with a high loop gain k typical qft design methods use graphical and numerical methods in conjunction with the nichols chart generally qft design seeks a high loop gain and large phase margin so that robust performance is achieved in this section we pursue a simple method of achieving the goals of qft with an s plane root locus approach to the selection of the gain k and the compensator g c s this approach dubbed pseudo qft follows these steps place the n poles and m zeros of g s on the s plane for the nth order g s also add any poles of gc s starting near the origin place the zeros of gc s immediately to the left of each of the n poles on the left hand s plane this leaves one pole far to the left of the lefthand side of the s plane increase the gain k so that the roots of the characteristic equation poles of the closedloop transfer function are close to the zeros of gc s g s this method introduces zeros so that all but one of the root loci end on finite zeros if the gain k is sufficiently large then the poles of t s are almost equal to the zeros of g c s g s this leaves one pole of t s with a significant partial fraction residue and the system with a phase margin of approximately actually about frequency rad s magnitude w jv g c jv g jv p p figure stability robustness to a time delay of uncertain magnitude section the pseudo quantitative feedback system example design using the pseudo qft method consider the system of figure with g s s p s p where the nominal case is p and p with variation the worst case is with p and p we wish to design the system for zero steady state error for a step input so we use the pid controller g c s s z s z s we then invoke the internal model principle with r s s incorporated within g c s g s using step we place the poles of gc s g s on the s plane as shown in figure there are three poles at s and as shown step calls for placing a zero to the left of the pole at the origin and at the pole at s as shown in figure the compensator is thus g c s s s s r s k gc s g s y s figure feedback system jv s figure root locus for kg c s g s chapter robust control systems we select k so that the roots of the characteristic equation are close to the zeros the closed loop transfer function is t s s s s s s l s this closed loop system provides a fast response and possesses a phase margin of p m when the worst case conditions are realized p and p the performance remains essentially unchanged pseudo qft design results in very robust systems robust control systems using control design software in this section we investigate robust control systems using control design software in particular we will consider the commonly used pid controller in the feedback control system shown in figure note that the system has a prefilter gp s the objective is to choose the pid parameters kp ki and kd to meet the performance specifications and have desirable robustness properties unfortunately it is not immediately clear how to choose the parameters in the pid controller to obtain certain robustness characteristics an illustrative example will show that it is possible to choose the parameters iteratively and verify the robustness by simulation using the computer helps in this process because the entire design and simulation can be automated using scripts and can easily be executed repeatedly example robust control of temperature consider the feedback control system in figure where g s s c and the nominal value is c and gp s we can design a compensator based on c and check robustness by simulation our design specifications are a settling time with a criterion ts s and an optimum itae performance for a step input for this design we will not use a prefilter to meet specification but will instead show that acceptable performance i e low percent overshoot can be obtained by increasing a cascade gain the closed loop transfer function is t s kds kps ki s kd s kp s ki section robust control systems using control design software the associated root locus equation is kn s sas b where kn kd a kp kd and b ki kd the settling time requirement ts s leads us to choose the roots of s as b to the left of the s zvn line in the s plane as shown in figure to ensure that the locus travels into the required s plane region we have chosen a and b to ensure the locus travels past the s line we select a point on the root locus in the performance region and using the rlocfind function we find the associated gain kn and the associated value of vn for our chosen point we find that kn then with kn a and b we can solve for the pid coefficients as follows kd kn kp a kd ki b kd to meet the overshoot performance requirements for a step input we will use a cascade gain k that will be chosen by iterative methods using the step function as illustrated in figure the step response corresponding to k has an imaginary axis real axis c d pwo cd fgp u u vh pwo fgp tnqewu u u tnqe pf u u selected roots s triple pole figure root locus for the pid compensated temperature controller as kn varies chapter robust control systems acceptable percent overshoot of p o with the addition of the gain k the final pid controller is g c s k kds kps ki s s s s we do not use the prefilter instead we increase the cascade gain k to obtain satisfactory transient response now we can consider the question of robustness to changes in the plant parameter c the investigation into the robustness of the design consists of a step response analysis using the pid controller given in equation for a range of plant parameter variations of c the results are displayed in figure the script is written to compute the step response for a given c it can be convenient to place the input of c at the command prompt level to make the script more interactive the simulation results indicate that the pid design is robust with respect to changes in c the differences in the step responses for c are barely discernible on the plot if the results showed otherwise it would be possible to iterate on the design until an acceptable performance was achieved the interactive capability of the m file allows us to check the robustness by simulation u c d u c d pwoie fgpie u uie vh pwoie fgpie pwoi fgpi u ui vh pwoi fgpi u uq ugtkgu u uie u ui u u hggfdcem u uq uvgr u u amplitude time s gain from uncompensated root locus increase system gain to reduce overshoot pid gains k k k figure step response of the pid temperature controller section sequential design example disk drive read system sequential design example disk drive read system in this section we design a pid controller to achieve the desired system response many disk drive head control systems use a pid controller and use a command signal r t that utilizes an ideal velocity profile at the maximum allowable velocity until the head arrives near the desired track when r t is switched to a step type input thus we want zero steady state error for a ramp velocity signal and a step signal examining the system shown in figure we note that the forward path possesses two pure integrations and we expect zero steady state error for a velocity input r t at t the pid controller is g c s kp ki s kds kd s z s zn s the motor field transfer function is g s s l amplitude time s e pwoi fgpi e e pwoie fgpie u ui vh pwoi fgpi u uie vh pwoie fgpie u uq ugtkgu u uie u ui u u hggfdcem u uq uvgr u u c specify process parameter figure robust pid controller analysis with variations in c chapter robust control systems the second order model uses g s and the design is determined for this model we use the second order model and the pid controller for the s plane design technique illustrated in section the poles and zeros of the system are shown in the s plane in figure for the second order model and g s then we have the loop transfer function l s gc s g s g s kd s z s zn s s we select z j and determine kd so that the roots are to the left of the line s if we achieve that requirement then ts and the percent overshoot to a step input is ideally p o since z of the complex roots is approximately of course this sketch is only a first step as a r s y s pid controller motor coil g s td s load g s gc s s s kd s z s z s figure disk drive feedback system with a pid controller j j j j j j j j poles z jv s figure zn a sketch of a root locus at kd increases for estimated root locations with a desirable system response section summary second step we determine kd we then obtain the actual root locus as shown in figure with kd the system response is recorded in table the system meets all the specifications summary the design of highly accurate control systems in the presence of significant plant uncertainty requires the designer to seek a robust control system a robust control system exhibits low sensitivities to parameter change and is stable over a wide range of parameter variations the pid controller was considered as a compensator to aid in the design of robust control systems the design issue for a pid controller is the selection of the gain and two zeros of the controller transfer function we used three design methods for the selection of the controller the root locus method the frequency response method and the itae performance index method an operational amplifier circuit used for a pid controller is shown in figure in general the