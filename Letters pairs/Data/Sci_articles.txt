Machine Learning Enabled Team Performance
Analysis in the Dynamical Environment of Soccer
SHITANSHU KUSMAKAR 1
, (Member, IEEE), SERGIY SHELYAG 1
, YE ZHU 1
, (Member, IEEE),
DAN DWYER 2
, PAUL GASTIN 3
, AND MAIA ANGELOVA 1
1School of Information Technology, Deakin University, Geelong, VIC 3125, Australia
2School of Exercise and Nutrition Sciences, Deakin University, Geelong, VIC 3125, Australia
3La Trobe Sport Exercise Medicine Research Centre, La Trobe University, Melbourne, VIC 3086, Australia
Corresponding author: Maia Angelova (maia.a@deakin.edu.au)
This work was supported by the DSI Collaborative Research (Intelligent Sensor Processing for Enhancing Defence Decision Support)
under Grant RM35517.
ABSTRACT Team sports can be viewed as dynamical systems unfolding in time and thus require tools and
approaches congruent to the analysis of dynamical systems. The analysis of the pattern-forming dynamics
of player interactions can uncover the clues to underlying tactical behaviour. This study aims to propose
quantitative measures of a team’s performance derived only using player interactions. Concretely, we
segment the data into events ending with a goal attempt, that is, ‘‘Shot’’. Using the acquired sequences
of events, we develop a coarse-grain activity model representing a player-to-player interaction network. We
derive measures based on information theory and total interaction activity, to demonstrate an association with
an attempt to score. In addition, we developed a novel machine learning approach to predict the likelihood of
a team making an attempt to score during a segment of the match. Our developed prediction models showed
an overall accuracy of 75.2% in predicting the correct segmental outcome from 13 matches in our dataset.
The overall predicted winner of a match correlated with the true match outcome in 66.6% of the matches
that ended in a result. Furthermore, the algorithm was evaluated on the largest available open collection of
soccer logs. The algorithm showed an accuracy of 0.84 in the classification of the 42, 860 segments from
1, 941 matches and correctly predicted the match outcome in 81.9% of matches that ended in a result. The
proposed measures of performance offer an insight into the underlying performance characteristics.
INDEX TERMS Dynamical systems, network science, distribution entropy, football, Kolmogorov complexity, machine learning, performance analysis, Shannon entropy, support vector machines, soccer.
I. INTRODUCTION
Improving comprehension of strategic performance and success in team competition is an important goal in sports science [1]. Data-driven methods can effectively overcome the
subjective limitations (manual analysis) of the match and
offer better results for football clubs. Quantitative analysis
can provide players and coaches with such insight, by allowing them to improve their match and assessment of the event
beyond what personal observation can accomplish [2]. Traditionally, methods of performance analysis push the study of
one-dimensional and discrete performance indicators towards
probabilistic and correlational approaches [3]. However, this
results in somewhat limited functional information as it lacks
The associate editor coordinating the review of this manuscript and
approving it for publication was Paul D. Yoo .
the understanding of the player-to-player interactions that
support the actions of players and overall team behaviour.
It is reasonable to expect an analysis of such one-versusone dynamics in team sports to be insufficient as multiplayer interactions are important in determining success and
failure [4]. Therefore, in order to quantify and explain performance, it has been advocated that performance analysis
in team sports must also focus on the interactions between
players that sustain the overall team behaviour [5], [6]. From
the dynamical systems view, the understanding of how the coordination emerges from the interaction among the system
components, that is, the player-to-player interaction, is the
key to performance analysis [7], [8]. In team sports, performance analysis approaches that consider the interactions
of the players in many multiplayer team competitions like
football are not well explored [9].
90266 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
Inspired by empirical studies of networked systems,
researchers have recently developed a variety of techniques
and models to help us understand player interaction network
in sports [10]–[13]. Interaction or passing networks can be
constructed from the observation of ball transfer between
players. A key challenge is to leverage the interaction networks to gain a functional understanding of the underlying
team strategies. For example, by examining the structure of
interaction networks, recurrent pass sequences can be identified and linked to a team’s playing style [14], [15]. When
the emphasis is put at the player level, Duch et al. [16]
used the interaction networks to quantify and rank player’s
contribution relative to the overall team activity.
Due to dissimilarity and diversity in real-world sports data,
there is no systematic program for predicting network structure. In addition, there are no particular subsets of diagnostics that are universally accepted [17]. Since team networks
are intrinsically subjective and dynamic objects, it is often
hard to determine a suitable way of network characterisation
that governs team formation [18]. In team sports like football, quantifying player-to-player interaction is the key for
understanding the dynamic patterns that generate a scoring
opportunity [19]. This motivated us to develop an approach
that quantitatively characterises players’ interaction in team
sports. In this study, a data-driven approach to the study of
complex player interactions from event stream data generated
during football matches (henceforth referred to as soccer) is
employed. The proposed framework can be used to quantify
player interactions and connect that with the outcome using
a machine learning approach.
Data-driven approaches for soccer analytics are given
importance with the availability of the event stream data
(e.g., Opta, Wyscout, STATS, SecondSpectrum, SciSports,
and StatsBomb). Cintia et al. [15] in their work, extracted
pass-based performance measures to learn the correlation to
match outcome using a machine learning approach. More
recently, Pappalardo et al. [20] in their work employed a
machine learning approach to rank players. Their approach is
based on computing statistical features from the event stream
data for each player, which are then utilised to learn feature
weights in a supervised learning framework i.e., relative to
the match outcome. The authors then use the learned weights
to compute the rating of a player. In another recent study
by Decroos et al. [21], the authors have performed a segmental analysis of different match states to extract several
associative features of player performance, which are then
used to determine the scoring or conceding probability using
an ensemble classifier. In contrast to the above-mentioned
studies that consider individual player’s actions or cumulative
team statistics, the proposed study describes a segment of a
match using a set of activity and entropy-based quantifiable
markers that capture both inter- and intra-player interactions.
To quantify interaction among players in team sports conceived as dynamical systems unfolding in time, it is important
to use appropriate measures [22], [23]. The proposed study
considers the behaviour of multiple players and the emergent
nature of performance to develop pattern-forming dynamics,
that is, the dynamic physical relationships that a player may
establish with the teammates and opponents to make a goal.
We developed a coarse-grain activity model of player-toplayer interaction from the possession chain data, that can be
used to quantify the dynamic patterns underlying the interaction among players. We used the concepts of information
theory retrieval to quantify the complexity of a pattern representing player interactions during sub-segments of the match.
Another key challenge from the analytics perspective is the
format of the soccer log data, as different vendors use different data formats [21]. Therefore, an analyst has to develop
complex pre-processors specific to a dataset. To tackle the
challenges posed by the variety of event stream formats and to
benefit the data-science community, we propose an approach
that uses only a limited amount of information. The proposed approach only uses the possession information, such
as player, team, action type, and result from the event stream
data. The segmental analysis was thus performed using only
the possession information to quantify the team performance
and stability in team-dynamics during a specific module,
that is, a match segment. Furthermore, based on the derived
performance measures we developed a machine learningenabled decision support system for automated prediction of
a team’s likelihood of a successful attempt at goal.
II. APPROACH
A. DATASET
In this study we have analysed the dataset from a season
of Major League Soccer division of the United States and
Canada. The dataset consists of the possession chain data
from 13 matches. The interaction information (possession
chain) comprises of time and duration of all ball passes
and tackles between players. The dataset also includes the
nature of the interaction which can be categorised as being
between teammates or between opposing players (Table 1).
The positional information includes the x-y position of all
individuals throughout the entire match (∼90 minutes).
B. COARSE-GRAIN PLAYER INTERACTION MODEL
Given: A set of possession chain information for each match,
representing a set of events (pass, shot etc) between players
and the game outcome.
A match is split into a number of segments, where each
segment represents a phase of the match that begins with
either the start of the match or after an attempt (Shot) at
the goal and ends with a ‘‘SHOT ’’ (see Fig. 1). Further,
throughout the text the teams in an adversarial relationship
during a match were denoted by team-1 and team-2 for each
match in the dataset. Using the possession information corresponding to every segment in the match, we propose a coarsegrain model to find quantifiable measures of performance
that demonstrate an associationship with the outcome of that
segment, that is, which team (team-1 or team-2) makes an
attempt to score by taking a ‘‘SHOT’’ at the opposition’s goal.
VOLUME 8, 2020 90267
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
TABLE 1. An example of ball possession chain data. The table shows a part of a ball possession chain dataset, which represents events in the 1st half of
a match.
FIGURE 1. Segmentation of the possession chain data. A match is split into different segments of varying length (or the number of events in a
segment) ending with a ‘‘SHOT ’’. The red and blue shaded cells represent possession by different teams. Each segment was individually
evaluated for measures of performance.
1) COARSE-GRAIN MODELS DERIVED FROM POSSESSION
CHAIN DATA
Each of the match segments was studied separately. The segments represent a sequence of ball possession change events
leading to an attempt to score. Each team in a soccer match
has 11 players with 3 allowed replacements. Based on the
sequence of events in each segment we define two types of
coarse-grain models. The first model weighs all the events
(e.g. pass, shot at goal, ball lost) equally, whereas the second
model weighs events based on their type. More specifically, a
higher weight of an event denotes a higher relevance. As we
are interested in measures that quantify a successful attempt
to score, we assign higher weights to shots and recoveries and
lower weights to events like ball lost and faults.
For each segment we first generate a pairwise player
matrix, Mi,j
, where i, j = {1, . . . , 28}, each element of which
was initialised to zero. The matrix M contains players of both
teams (i, j = {1, . . . , 14} and i, j = {15, . . . , 28} for team-1
and team-2, respectively) and any element Mi,j represents the
interaction of the i
th player with j
th player in the segment.
The value of the Mi,j element denotes the number of times
the players interacted or the number of times the players
interacted weighed by the type of event. For example, if
player 1 of team-1 passes the ball to player 5 of team-1 the
element M1,5 of the matrix is incremented by 1 (i.e., M1,5 =
M1,5 + 1). Similarly, if player 3 of team 2 recovers the ball in
a tackle from player 5 of team 1, then the element M14+3,5 of
the matrix is incremented by 1 (i.e., M14+3,5 = M14+3,5 +1).
Therefore, the diagonal 14×14 blocks of the matrix M denote
interactions of the players within a team whereas, the offdiagonal blocks represent the inter-team player interactions.
Thus, the matrix M (such that, Mi,j ≥ 0 ∀i, j) was termed as
the interaction matrix. The matrix M represents the connections on the network of players (agents), related to activitybased decision-making to the directed transfer of information
(ball) from one agent to another. This coarse-grain interaction
model (M), represents the network of connections, accumulated over a sequence of events during a segment of the match.
We analysed the interaction between players based on the
following approach:
a: UNIT INCREMENT
Each element Mi,j of the interaction matrix is incremented
by 1 for an interaction between the i
th and j
th player of the
same team (ball passed) or players of the different team (ball
recovery, tackle, ball lost etc.) as follows:
Mi,j
7→ Mi,j + 1 (1)
b: WEIGHTED INCREMENT
Each element Mi,j of the interaction matrix is weighted by the
type of the event Et
. More specifically, we assign a weight to
each event for evaluation of its contribution. Given that we
are mostly interested in goal attempts, we introduce a higher
weight for shots in comparison to passes, ball losses and other
events that lead to loss of ball possession.
Mi,j
7→ Mi,j + WEt
, (2)
90268 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
where WEt
is the weight corresponding to the event Et
. As
we are interested in the likelihood of a successful attempt at
goal, we assign a high weight Wshot = 2 to shots, a low weight
Wpass = 0.5 to passes, and an average weight Wshot,pass = 1
to all other event types as suggested by Decroos et al. [24].
C. QUANTIFICATION OF TEAM PERFORMANCE FROM
COARSE-GRAIN MODELS
To quantify the performance of a team in each segment of the
match four measures were proposed:
1) TOTAL ACTIVITY INDEX (TAI)
To quantify the interaction in a segment the matrix M is
further divided into four blocks by summing all the elements
(Mi,j) in top left (team-1 ∀ i, j in {1, . . . , 14}), and the bottom
right (team-2 ∀ i, j in {15, . . . , 28}), which represent the
overall activity of each team that is obtained by summing the
activity of all players in a team. The off-diagonal elements
represent the interaction between players of both teams. Any
row in the top left (team-1 ∀ i in {1, . . . , 14}) or bottom right
(team-2 ∀ i in {15, . . . , 28}) block of matrix M represents the
interaction of the i
th player with the rest of his team. Similarly,
any row in the off-diagonal blocks of the matrix M represents
player i (∀ i ∈ {1, . . . , 14}) of team 1 losing the ball to player
j (∀ j ∈ {15, . . . , 28}) of team 2 and vice versa. We introduce
the average 2 × 2 team activity matrix T as follows:
T =

T11 T12
T21 T22
(3)
where each element of matrix T represents the average activity of each block in M, as follows:
T11 =
X
N
i,j=1
Mi,j (4)
T22 =
X
2N
i,j=N+1
Mi,j (5)
T12 =
X
N
i=1
X
2N
j=N+1
Mi,j, (6)
T21 =
X
2N
i=N+1
X
N
j=1
Mi,j, (7)
where N = 14, and PN
i,j=1 Mi,j represent a player’s activity
(team-1 ∀ i ∈ {1, . . . , 14}, and team-2 ∀ i ∈ {15, . . . , 28},
respectively). The overall activity (Ac) of each team in a
segment is then calculated as:
Ac1 =  × (T11 + T21 − T12) (8)
Ac2 =  × (T22 + T12 − T21) (9)
where  =
P
i,j=1..2
Ti,j
is a normalisation constant.
The total activity index (TAI) of the match is then computed as follows:
TAI = Ac1 − Ac2 (10)
2) INFORMATION ENTROPY AS A MEASURE OF
PERFORMANCE
It has been advocated that performance analysis in a team
sports should consider the dynamical nature of the match and
must consider player-to-player interaction [5], [8]. The stability and consistency of interaction between different players of
a team have been considered as a measure of performance
in soccer matches [25]. Entropy quantifies the uncertainty
coming from the random aspect of the dynamics. Entropy as a
measure can be utilised to quantify the consistency of patterns
representing player-to-player interaction in the match.
a: SHANNON ENTROPY
Previously, Shannon entropy has been used as a measure of
uncertainty in team sports to quantify the variability associated with the movements of players in a match [26]. In this
work, we have used Shannon entropy to quantify the patterns
representing player-to-player interaction during a segment of
the match. Shannon entropy is a measure of the uncertainty or
unpredictability in the estimate of the information content of
a random variable [27]. The Shannon entropy (H) is defined
as follows:
H = −X
N
i=1
pi
ln(pi), (11)
where, pi
is the probability of the i
th element in the sequence.
b: KOLMOGOROV COMPLEXITY
As an alternative to the probabilistic notion of information
content, the Kolmogorov complexity is based on the concept
of recursive function [28]. Kolmogorov complexity allows
the characterisation of chaotic motion in dynamical systems and the analysis of spatiotemporal patterns [28]. The
Kolmogorov complexity c(N) of a sequence with N samples
is the length of the shortest binary program that can generate
that sequence as output [28], [29]. An appropriate measure of
Kolmogorov complexity can be defined by h(N) as follows:
h(N) =
c(N)
b(N)
(12)
where b(N) = N log2 N. In this work, Kolmogorov complexity of the signal was calculated following Kaspar et al. [28].
c: DISTRIBUTION ENTROPY
Distribution entropy (DistEn) computes the complexity
of a time-varying sequence using the distribution of the
inter-vector distances [30]. Unlike approximate and sample entropy, DistEn offers high robustness for short length
sequences and reduced dependence on pre-determined
parameters [30]. DistEn has been previously used in many
biomedical applications to quantify the complexity of short
length signals [30], [31]. In the context of soccer, DistEn
can be used to characterise the complexity of the dynamical
network patterns representing the player-to-player interaction
VOLUME 8, 2020 90269
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
during a segment. The DistEn of a vector can be defined as:
DistEn(m, τ, β) =
1
log2
(β)
X
β
i=1
pi
log2
(pi) (13)
where β = 64 is the number of bins in the probability
distribution, obtained from the data with the lag τ = 1
and embedding dimension m = 2. These parameter values
are selected based on common recommendations from literature [30].
d: ENTROPY DERIVED PERFORMANCE INDEXES
To quantify the complex behaviour in which players interact
during a soccer match, three entropy measures were calculated. Based on the type of the entropy three indexes were
defined: (1) Shannon entropy index (SEI), (2) Kolmogorov
complexity index (KCI), and (3) Distribution entropy index
(DEI). Let s(N) denote the entropy for a sequence of length
N. We calculate s(N) for each row in each of the four blocks
of the interaction matrix M. We introduce a 2 × 2 matrix S
to represent the team entropy/complexity matrix as follows:
S =

S11 S12
S21 S22
(14)
Here, the elements of matrix S represent the averaged
entropy/complexity of player-to-player interaction in each
block of matrix M, as follows:
S11 =
1
N
X
N
i=1
h(Mi,j=1...N ) (15)
S22 =
1
N
X
2N
i=N+1
h(Mi,j=(N+1)...2N ) (16)
S12 =
1
N
X
N
i=1
h(Mi,j=(N+1)...2N ) (17)
S21 =
1
N
X
2N
i=N+1
h(Mi,j=1...N ), (18)
where N = 14.
The overall complexity for each team in a segment is given
by:
s1 = S11 + S21 − S12 (19)
s2 = S22 + S12 − S21, (20)
The three entropy derived indexes (SEI, KCI, and DEI) of
a segment in the match are then computed as follows:
DerivedIndex = s1 − s2, (21)
where the DerivedIndex is SEI, KCI, DEI for s denoting
Shannon entropy, Kolmogorov complexity, and distribution
entropy, respectively.
D. MACHINE LEARNING APPROACH
The possession chain data from each segment in a match
was quantified using the proposed measures, which were
then used as features for predicting the team that makes the
‘‘SHOT ’’ during the segment. In the model training phase, the
predictive model was trained using a supervised framework,
where each segment ending in a ‘‘SHOT ’’ was given a label
‘‘1’’ if team-1 makes the shot and a label ‘‘2’’ if the opposition
makes the shot. During the testing and validation phase, the
learned model was then used to predict the team making
the ‘‘SHOT’’ in a segmental manner. The outcome of the
game (i.e. team winning the match) was determined based
on the classification of the segments (team-1/team-2) where
the ‘‘SHOT ’’ ends in a goal. For each game, we report the
segmental performance and the predicted match outcome (i.e.
winner of the match). We now describe the classifier and the
learning procedure.
1) SUPPORT VECTOR MACHINE
Support vector machines (SVM) are state-of-art binary state
classifiers, which are suited for pattern recognition and classification problems with good robustness to overfitting. Given
an i.i.d. learning set {(x1, y1), (x2, y2), . . . ,(xi, yi)}, where
x ∈ <N , y ∈ {−1, 1}, the kernel function maps the input
feature space to a high-dimensional space where the data
is linearly separable, offering the ability to learn non-linear
functions and decision boundaries. The decision function
separating the two classes is learned as a hyperplane. The
optimisation problem can be formulated as:
min
ω,b,ξ
1
2
kωk
2 +
C
n
Xn
i=1
l(ξ ) (22)
subject to yi(ω·φ(x)+b) ≥ 1−ξi
, ∀ i ∈ 1, . . . , n, where C
is a positive regularisation constant and ξ is the slack term.
By using the Lagrange multiplier techniques, the optimisation problem in SVM is reduced to a dual optimisation
problem:
max
αk
W(α)=
Xn
i=1
αi −
1
2
Xn
i=1
Xn
j=1
αiyiαjyjKhxi
·xji (23)
subject to Pn
i=1
αiyi = 0 and αi ∈ [0,C] ∀ i = 1, . . . , n.
The learned decision function can then be represented as:
f (x) = sgn Xn
i=1
αiyiKhxi, xi + b
!
, (24)
where Khxi, xji represents the kernel function. In this study
we have used the Gaussian radial basis kernel function.
2) LEARNING CLASSIFICATION MODELS
The classification models were trained using a leave-oneout cross-validation approach [32]. Let N represents the
total number of matches. In leave-one-out cross-validation
approach, the dataset corresponding to a match is left out
while the dataset from the remaining matches (N − 1) is
90270 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
FIGURE 2. (a) Player interaction matrix M, computed for the unit
increment for a segment of match G3
. (b) Same as (a) for the weighted
increment. (c) Team activity matrix T , computed for the unit increment for
a segment of match G3
. (d) Same as (c) for the weighted increment. The
self-interaction (main diagonal of the matrix M) has been saturated in the
left panel to reveal the interaction between the players. Every element
Mi,j
represents the ball originator and receiver for an event. The main
diagonal blocks in (a) and (b) represent the interaction between players
of the same team, whereas the top-right and bottom-left corner blocks
represent interactions with the players of the opposition team. The lighter
the color, the higher the value of activity between the players. For the
shown segment, team-1 made an offensive attack against the team-2,
which is also evident in the higher activity (lighter color) of team-1 as
shown in (c) and (d).
used for training the SVM classifier. A feature selection using
Lasso technique was applied on the training set for finding the
least correlated and most discriminating features [33], thus
ensuring the test data (left-out match) was not a part of feature
selection and model learning procedure.
III. RESULTS AND DISCUSSION
In this study, we have analysed each match by segmenting
into sequences that end with a ‘‘SHOT ’’. The possession
chain data in each segment was first mapped onto a matrix
M representing match-integrated ball possession activity of
players (Fig. 2). To calculate the estimate of complexity and
non-linear dynamics in a match of soccer using the proposed
coarse-grain model of teams’ activity, we introduced four
quantitative measures of team performance (TAI, SEI, KCI,
and DEI). In addition, a machine learning approach was
presented, where we developed machine learning models to
predict the outcome of a segment based on the proposed
quantitative measures of performance.
We first explain the quantitative measures of performance
derived from the proposed coarse-grain model of player interactions network, (A) total activity index (TAI), (B) Shannon
entropy index (SEI), (C) Kolmogorov complexity index
(KCI), and (D) distribution entropy index (DEI), followed
FIGURE 3. Match G3
: Atlanta United FC (team-1) vs. San Jose
Earthquakes (team-2), season 2018 (final results: 4-3). Temporal
evolution of the proposed quantitative markers of performance (a) Total
activity index (TAI), (b) Shannon entropy index (SEI), (c) Kolmogorov
complexity index (KCI), and (d) Distribution entropy index (DEI), derived
using the weighted network of connections represented by matrix M. The
vertical dashed lines indicate the moments at which a goal was scored in
the match (the red (− − −) and blue (− − −) lines represent the goal
scored by team-1, and team-2, respectively). Each interval on the timeline
represents the time stamp of the segment ending with a ‘‘SHOT ’’.
by (E) the performance of the proposed machine learning
approach and future work.
A. TOTAL ACTIVITY INDEX (TAI )
The total activity index (TAI) is a measure of a team’s activity
relative to the other during a segment. Based on the definition
of TAI, a positive value of TAI indicates that team-1 is likely
to take the ‘‘SHOT ’’ at the end of the segment, while a negative value indicates team-2 (Table 2, Fig. 3 (a)). The underlying hypothesis was that the more frequently or longer the
players of a team interact during a segment, the more likely it
is that this team scores in the particular segment of the match.
This was further corroborated by the minimum and the maximum values of TAI as seen, for example, in match G3 (Atlanta
United FC (team-1) vs. San Jose Earthquakes (team-2),
season 2018 (final result: 4-3)) that correspond to the segments when first team-2 was trying to score and then team-1
was trying to equalise by maintaining a higher possession of
the ball (the segment ending at 12th and 25th minutes of the
match G3, Fig. 3 (a)). When plotted with respect to the ground
truth (i.e the outcome of the segment w.r.t to the team taking
the shot) the distribution of TAI is close to normal for both the
teams (Fig. 4a, Fig. 4b). The descriptive statistics relating to
the performance of TAI are shown in Table 2. Results showed
significantly different (p < 0.05) means for both teams
(Table 2). Although a certain overlap could be seen among the
TAI value ranges derived using the unit and weighted increment matrices (Table 2, Fig. 4a, and Fig. 4b), the area under
the receiver operator characteristics curve (AUC) values of
0.79 and 0.81 for TAI derived from the unit and weighted
increment matrices show a good class separability.
VOLUME 8, 2020 90271
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
FIGURE 4. A distribution of the proposed measures of performance on
‘‘SHOTS’’ with respect to the true segmental outcome, shown for all
matches in the dataset; (a-b) TAI, (c-d) SEI, (e-f) KCI, and (g-h) DEI,
derived using unit and weighted network of connections represented by
the interaction matrix M. The distribution of the derived quantitative
measures (TAI, SEI, KCI, and DEI) was close to normal, with both teams
having a significantly different means (p < 0.05).
The better performance of the weighted increment matrix
shows the introduced bias towards the segment outcome
(‘‘SHOT ’’) due to the higher weights given for the events
that are likely to result in goal attempts in comparison to
normal passes. Furthermore, the use of weights provides an
alternative evaluation function that offers the opportunity to
consider the types of events appearing in a pattern, and the
pattern’s support to determine its relevance. Finally, the TAI
derived from the coarse-grain activity model shows good
potential as a quantitative measure of performance in a team
sport like soccer.
B. SHANNON ENTROPY INDEX (SEI )
Shannon entropy gives a measure of uncertainty to quantify the randomness associated with a time-varying signal.
The Shannon entropy index (SEI) quantifies the underlying
variability in player-to-player interaction for a team relative to
the other team. The Shannon entropy of a team in a segment
would be low (≈ 0) if only few players interact with each
other, thus minimising the randomness and the associated
unpredictability, whereas it would be high (≈ 1) if different
players are continuously interacting with each other. A higher
entropy indicates that there is more uncertainty in patternforming dynamics governing the interaction among players.
Alternatively, a higher entropy represents that players are not
constrained to a specific role and assume a higher tactical role
(e.g. players moving both forward, backward, and through
the sides of pitch, thus forging more player-to-player interactions). In team sports, a longer possession of the ball is likely
to forge more player-to-player interactions especially, during
a strategy leading to an offensive on the opposition more
players are likely to be involved (e.g. in a match of soccer
midfielders, centre forwards, wing forwards can be a part
of an attack). Therefore, we hypothesised that the Shannon
entropy for the team that is attacking would be higher relative
to the other. Therefore, as defined in section II-C.2.d, SEI
would be > 0 if team-1 is attacking and < 0 if team-2
is attacking (Table 2, Fig. 3 (b)). The minimum and the
maximum values of SEI denote an offensive behaviour by
team-2 and team-1, respectively (segments ending at 12th and
25th minute in Fig. 3 (b)).
Thus, SEI can be a good marker indicating when a team
makes an offensive against the opposition. The SEI index
correlated with the segment outcomes, that is whether team-1
or team-2 takes the shot (highest AUC: 0.80, Table 2). The
mean SEI for team-1 and team-2 were significantly different for both unit and weighted increment matrix (Table 2).
A similar observation was made from the distribution when
SEI was plotted with respect to the true outcome of the
segment (Fig. 4c, and Fig. 4d). Based on the descriptive
statistics (Table 2), the SEI index can be used as a potential
marker of a team’s performance derived from a coarse-grain
network model representing player-to-player interaction.
C. KOLMOGOROV COMPLEXITY INDEX (KCI )
The use of Kolmogorov complexity was motivated by the
presumption that interaction among players during a segment can be both random or synchronised (if certain players interact more frequently). Let us consider two vectors sx = {0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0}, and sy =
{0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0} of length 14 each (only
a maximum of 11 players of each team were active during
any segment of the match; however, a pattern length of 14
was considered as a soccer match can have a maximum of 3
substitutes.), that represents interaction pattern of players sx
and sy. The value of the i
th element in vectors sx and sy represents the number of times the i
th player (∀i ∈ {1, . . . , 14})
interacted with player sx or sy, including any self-interaction.
Both the sequences sx and sy have the same Shannon entropy
of 2 and DistEn of 0.143 (m = 2, β = 64), whereas
both have a different Kolmogorov complexity (0.81 and 1.63,
90272 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
TABLE 2. Mean, standard deviation, and area under the receiver operator characteristic curve statistics of the indexes TAI, SEI, KCI, and DEI derived using
two different types of interaction matrix M over all matches in the dataset. The statistics show that the proposed indexes have significantly different
values corresponding to the team taking the shot at the goal. A positive value of the derived indexes denotes that team-1 takes the shot, while a negative
value indicates team-2, relative to whom the indexes are computed.
respectively). Sequence sx has a pattern composed of units
{0, 0, 1} in recursion, whereas sequence sy has no obvious
pattern, thus sy has a higher complexity. In the context of
soccer, if the players are interacting in a synchronised manner,
that is, few particular players are part of a strategy (offensive
or defense), such patterns would be represented by simpler
sequences with lower complexity or unpredictability. From a
coaches point of view, it is important to assess the dynamics of pattern formation occurring in each segment of the
match to decode the underlying strategy [22]. The proposed
Kolmogorov complexity index derived from the player-toplayer interaction network (matrix M) gives a quantitative
measure of local numerical relations in which the dynamics
of a teams pattern formation varies relative to the other team.
For example, if certain players are only restricted to particular
parts of the playing pitch as in the formation 4:4:3, the playerto-player interactions in such a segment would be represented
by a less complex patterns like sx . On the other hand, if a team
allocates more players in sub-segments of a match to prevent
opposition’s attacking move (i.e., a defensive strategy) or to
create an offensive move at opposition’s goal, the playerto-player interactions would be represented by more complex patterns without any recursive sub-patterns as shown by
sy. Therefore, Kolmogorov complexity derived (KCI) index
captures the complexity of patterns that is different from
Shannon entropy derived index or SEI. KCI showed a good
correlation when plotted with the segmental outcome of a
match (AUC (0.73), Table 2). A KCI value > 0 favoured
team-1, while a KCI < 0 indicates a shot taken by team-2
(Fig. 3 (c)). For match G3, the segment ending at the 54th
minute represents a case when team-1 is making an offensive
against the opposition to level the scores at 2 − 2, which is
shown by the maximum value of KCI at the 54th minute
(Fig. 3 (c)). The KCI index followed a distribution close
to normal, when plotted on the true segment outcome, that
is, with respect to the team taking the ‘‘SHOT’’ (Fig. 4e,
and 4f), with a significantly different mean values for both the
teams (Table 2). KCI is a measure to quantify the regularity
of complex patterns in which players interact during team
sports. It gives a numerical relation in which the dynamics
of a team’s pattern formation varies over the segments of
a match. KCI can allow coaches to discover, identify and
quantify segments during a match, when a team interacts in
more complex or rather synchronised patterns.
D. DISTRIBUTION ENTROPY INDEX (DEI )
The distribution entropy (DistEn) measures the complexity
of patterns governing player-to-player interactions by taking
into account the hidden information in the state-space via
estimating the probability density of inter-vector distances.
A chaotic sequence has the maximum DistEn, thus patterns
of player-to-player interaction with high variability would
be characterised by a high DistEn (≈ 1) and vice versa.
The distribution entropy derived index (DEI) quantifies the
chaotic patterns underlying player-to-player network of interaction for a team relative to the other. Similar to Shannon
entropy-derived index, a DEI > 0 would indicate a higher
variability (associated with an attacking move) in patterns
governing player-to-player interaction for team-1, when computed relative to team-2 (Fig. 3 (d)). However, a particular
advantage of DEI over SEI is that SEI can be affected by
the variance of the sequences representing player interaction
patterns, while DEI is derived using a probability density
function with fixed bin number (β). Thus, DEI is more
robust as it considers inter-vector distances. Let us consider
two vectors sx = {0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 0}, and
sy = {0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 5, 0, 1, 0} that represent the
interaction patterns of player x and player y during a segment
of the match, and having the same number of total interactions
(
Psx =
Psy = 10). The 5 in sy represents the number
of times the 11th player of the team interacted with player
y during the segment. The Shannon entropy for sx , and sy is
2.84, and 1.96, respectively, whereas DistEn is 0.27, and 0.36,
respectively. Pattern sy represents that player y interacts more
frequently (5 times) with the 11th player, which results in high
inter-vector distances thus leading to a higher DistEn value.
The interaction pattern represented by sy indicates the events,
when player y is continuously interacting with a particular
player (11th player in the team). This pattern might signify an
underlying strategy where the players’ (defender/midfielder)
VOLUME 8, 2020 90273
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
FIGURE 5. Match G3
: Atlanta United FC (team − 1) vs. San Jose Earthquakes (team − 2), season 2018 (final results: 4-3). The
segmental analysis for match G3
, showing the predicted outcomes for every segment compared to the true outcomes; (a)
segmental likelihoods, (b) predicted outcomes, and (c) the true match outcome, which is the team taking the ‘‘SHOT ’’ at the
oppositions goal at the end of the segment. The vertical bars on the match timeline show the segments where a team scored a
goal. Red bars indicate team-1 and blue bars indicate goals scored by team-2. The intervals on the timeline indicate the time
stamp corresponding to the segments ending with a ‘‘SHOT ’’.
are interacting with a particular player (forward) as a part of
a strategy to generate a scoring opportunity. DistEn provides
an ability to encode such patterns, thus distribution entropyderived index (DEI) can be a good marker to characterise the
complexity of player-to-player interaction patterns, such that
the underlying strategy can be quantified as a measure of a
team’s performance during a segment of the match. When
plotted with respect to the outcome of the segment, the DEI
values were normally distributed (Fig. 4g, and 4h). The mean
DEI values for the teams in the adversarial relationship were
significantly different (Table 2), and a good class separability
was achieved with an AUC of 0.79, and 0.81 for DEI derived
using unit and weighted interaction network of players.
E. PERFORMANCE OF THE MACHINE LEARNING
APPROACH
We developed an automated machine learning model to predict the outcome of a match segment using the proposed
measures of performance quantification (TAI, SEI, KCI,
and DEI). Our machine learning approach showed a mean
sensitivity of 78.3% (95% confidence interval (CI): 70.3%
- 85.3%), a specificity of 73.8% (95% CI: 69% - 80.2%)
and an overall accuracy of 75.2% in predicting the segmental
outcomes of the matches. Although our dataset comprised of
only 13 matches, it should be noted that we performed a segmental analysis on segments of different duration (segments
ending with ‘‘SHOT ’’), resulting in a sizeable number of
samples (241 temporal segments) for training and validating
the machine learning classifier. In addition, our approach is
based on a robust cross-validation approach that ensures no
bias of the learned model to the ground truth.
The predicted segmental outcomes for all the matches in
the database are shown in Table 3. One match G13 ended
in draw. Among the rest, the predicted outcome correlated
with the ground truth (i.e., the winner of the match) in 8
(66%) of 12 matches. Furthermore, the application of the
automated segmental analysis is not limited to the overall
match outcome. It also helps to analyse the underlying local
prediction statistics. The outcome of our developed prediction
model on a complete match is shown in Fig. 5. The prediction models give the segmental likelihood of an attempt to
90274 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
TABLE 3. The predictive performance of our developed machine learning models. Shown are the segments predicted in favour of a team with the overall
prediction accuracy, the predicted winner, and the true match results.
TABLE 4. Validation of the proposed approach on the largest open collection of soccer logs from 7 major competitions [34]. Shown are the total number
of matches, matches ending in result, the number of analysed segments, mean performance measures with 95% [CI: confidence interval] over all matches
of each competition, and the accuracy depicting the percentage of matches where the predicted winner correlated with the true match outcome.
goal for both the teams (Fig. 5 (a)). In the particular match
shown in Fig. 5, team-1 (shown in red) won the match 4-3.
The segments where a goal was scored are marked with ∗
(segments 3, 11, 13, 17, 19, 20, and 22 shown in Fig. 5).
It can be seen in Fig. 5 that segments where team-1 has
scored a goal (segments 13, 19, 20, and 22) have a higher
likelihood. Similarly, the likelihood for team-2 is higher in
the segments where they scored a goal. Segments 9, 14, 18,
and 19 show where both the teams are engaged in gaining the
possession of the ball as they want to equalise. The segments
where the predicted outcome (Fig. 5 (b)) did not match the
true outcome (Fig. 5 (c)) are the ones, where the possession
of the ball is continuously changing between the teams. To
quantify the minority of segments, where the model does not
provide a sufficient agreement with the ground truth data,
in future we would incorporate more sophisticated measures
by introducing player labels (forwards, mid-fielders, defence)
to understand player-to-player interaction using concepts of
mutual information retrieval [36].
F. VALIDATION AND COMPARISONS ON PUBLIC DATASET
To elaborate on the efficacy of the proposed approach a thorough performance evaluation was carried out on the largest
available public dataset of soccer logs [34]. This dataset comprised event logs (possession chain data) from 1, 941 matches
of 7 major competitions (Table 4). The proposed machine
learning approach showed an overall sensitivity of 83.5%, a
VOLUME 8, 2020 90275
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
specificity of 83.4%, F1 score of 83.7%, and an AUC of 0.84
in classifying a total of 42, 860 segments ending in ‘‘SHOT ’’
(i.e., whether team-1 or team-2 makes the ‘‘SHOT’’ at the
goal) (Table 4). The match outcome (segments leading to a
goal) correctly correlated in 1, 202 (81.9%) of 1, 467 matches
that ended in a result. The performance measures with the
95% CI (calculated over all matches in a competition) for
each competition are also reported (Table 4). The European
cup 2016, and the Italian first division had the lowest and the
highest AUC for segmental analysis among the 7 competitions. Overall the AUC of the segmental performance was
close to the overall AUC for each of the 7 competitions, which
shows the consistent performance of the proposed approach
across the competitions. The Italian first division had the
highest number of segments ending in ‘‘SHOT’’ (8, 758) from
the 380 matches. The proposed machine learning approach
resulted in an AUC of 0.85 (95% [CI]: 0.84 - 0.86) in correctly
classifying the segments. Furthermore, the predicted match
winner correlated with the ground truth in 254 (85.5%) of
the 297 matches of the Italian first division that ended in a
result. The overall performance of the proposed approach on
1, 941 matches and 42, 860 segments shows the efficacy of
the proposed quantitative markers (TAI, SEI, KCI, and DEI)
of a team’s performance.
Furthermore, to elaborate the efficacy of the proposed
quantifiable markers of team performance, we compared
the results of the proposed approach with studies that
employ a machine learning approach for evaluating performance [15], [20], [21]. A direct comparison of the proposed
segmental analysis approach can be done with the study by
Decross et al. [21]. They employed a segmental analysis to
learn the importance of players actions based on the outcome
of a match state (e.g. success in taking a ‘‘SHOT ’’ at opponents goal). On the contrary, Pappalardo et al. [20] defined
a feature vector for each team and modelled the outcome of
the match (Win/Loss) using a linear support vector machines
classifier. As both the studies [20], [21] use different datasets,
therefore, to ensure a direct comparison of the proposed
approach the algorithms by Decross et al. [21], and Pappalardo et al. [20] are run on the soccer logs from 1, 941
matches of 7 competitions [34]. The model estimation and
the learning task was performed using a leave-one-out crossvalidation approach as explained in section II-D. Additionally, the results on the public dataset were compared with the
study by Cintia et al. [15], who analysed the match outcome
using pass-based performance indicator (H-indicator) and
evaluated the performance on the German, Spanish, Italian,
and English division leagues.
For a comparison of the segmental performance, the algorithm by Decross et al. [21] was used to model the segments
ending in a ‘‘SHOT’’ with an XGBoost classifier [37]. The
algorithm by Decross et al. [21] showed an overall AUC
of 0.83 (Table 5). In comparison, the proposed approach
showed a similar performance with an overall AUC of 0.84 on
42, 860 analysed segments (Table 5). Further, for a comparison of the correctly predicted match outcome, the algorithm
TABLE 5. Comparisons of the proposed approach with some recent
machine learning-based studies.
by Pappalardo et al. [20] was employed. The algorithm by
Pappalardo et al. [20] could correctly classify the match
outcome in 1176 (77.5%) of 1467 matches that ended in a
result among the 7 competitions (Table 5). In comparison, the
proposed approach could correctly classify the match-winner
in 1, 202 of 1, 467 matches. Furthermore in comparison to
the study by Cintia et al. [15] who reported a mean accuracy
0.55 in correctly predicting the match outcome, the proposed
approach showed a higher mean accuracy of 0.82 (German:
0.81, Spanish: 0.78, Italian: 0.85, and English division: 0.86)
in correctly predicting the match outcome. The improved performance of the proposed approach shows the robustness and
efficiency of the proposed quantitative markers (TAI, SEI,
KCI, and DEI) in capturing a team’s underlying performance
characteristics (Table 5).
The performance of the proposed approach can be
attributed to the use of kernelised classifier and the nonlinearity of the proposed indices like SEI, KCI, and DEI that
can quantify the underlying non-linear dynamics of player
interaction. Based on the performance validation on external dataset and comparison with recent studies, it can be
concluded that the proposed approach offers a data-driven
framework for evaluating a team’s performance in a segmental manner, offering the potential for predictive analytics in
sport sciences using data science research.
90276 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
FIGURE 6. The feature histograms showing players activity, Shannon entropy (ShnEn), Kolmogorov complexity (KolCmp), and Distribution
entropy (DistEn) derived from the player interaction matrix M. The derived parameters are normalised to ensure feature commensurablity.
Shown here for a segment of match G3
(a) team-1, and (b) team-2. The dotted horizontal lines in subplots (a), and (b) represent the mean
across all the players of a team. The rectangular box in histogram plot for team-1 (subplot (a)) indicates the players (P2
, P5
, P8
, P9
, P10,
and P11) who maintain a ball possession activity that is higher than the team’s mean. Player P10, who makes the shot at goal had the
highest interaction with the rest of the players during team-1 ball possession activity. In contrast, the ball possession activity of team-2 is
mainly among players P1
, P5
, P7
, and P8
.
G. INTERPRETABILITY FOR SPORTS ANALYSTS
Our analysis shows that the interaction between players is
essential for generating a scoring opportunity. To outline the
applicability of the proposed features, we use histogram plots
representing each player’s feature values that are derived from
the player interaction matrix M for a segment of the match
(Fig. 6).
The histograms illustrate the level of interaction of each
player, when their team has possession of the ball. The players
that are more frequently involved in the ball possession have
feature values above the team’s mean value, which is represented by the dashed line (−−) as shown in Fig. 6. Across this
match segment, team-1 performs better than team-2, because
the segment ends with team-1 having a successful attempt at
scoring i.e., a ‘‘SHOT’’ at goal. Six players of team-1 (P2, P5,
P8, P9, P10, and P11) maintain a level of interaction (as indicated by the rectangular box in 6a) above the team’s mean,
which is higher than team-2, where only three players are
above the team’s mean (as indicated by the rectangular box
in Fig. 6b).
The feature Activity shows the players that are more frequently involved in a team’s ball possession activity. The
remaining features (ShnEn, KolCmp, and DistEn) were also
above average for more players in team-1 than for team-2.
Sports analysts can interpret this as an association between
the complexity of passing between players and the likelihood
of having a shot at goal. In other words, when a team has
possession of the ball, there may be a benefit in making a relatively large number of passes between a large proportion of
the team, as they move the ball towards their opponent’s goal
post.
The usual analysis of an opponent’s tactics is a resourceintensive procedure, as most tactical analyses are performed
by manually reviewing the match videos or scouting matches
in-person to identify the players that are constantly part of
the ball possession activity and are involved in generating
scoring opportunities [24]. The features used in the present
analysis may enable the automatic identification of such players using a data-driven approach. For example, player P10 in
team-1 is one such player who had the highest ball possession
activity during the shown segment of the match (indicated
by an ∗ in Fig. 6a). Identifying the players that are more
frequently involved in match states that end with an attempt
at scoring i.e., a ‘‘SHOT ’’ at goal, may assist sports analysts
and team staff to develop strategies suited to an opponent’s
playing style.
The proposed study presents different characteristics of a
team’s performance during a segment of a match that ends
with a ‘‘SHOT ’’ on the goal. Although, there are different
ways to define match segments (e.g. a segment ending with
the ball going out, a foul etc.) the purpose of the study was to
identify the characteristics leading to an attempt at scoring a
goal. Therefore, in this study, we analysed segments ending
with a ‘‘SHOT ’’ on the goal, which is also a limitation of the
study. Furthermore, the influence of match location, quality
of opposition, match type etc. were not controlled for while
developing the predictive models. Thus, further research is
required to investigate the effects of these variables to further enhance the understanding of teams and players performances.
IV. CONCLUSION
Our study proposes information theory-derived quantifiable
measures of performance that can uncover the dynamic patterns underlying team sports like soccer. The study provides
first evidence of a machine learning-enabled approach for
VOLUME 8, 2020 90277
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
automated predictive analysis of performance in a segmental
manner, offering the potential for uncovering local numerical markers of team performance. Our developed predictive
models show a mean accuracy of 75.2% in predicting the segmental outcome of the likelihood of team making a successful
attempt to score a goal on our dataset comprising 13 matches.
In addition, the segmental outcomes could predict the correct
overall winner in 66.6% of the matches that resulted in a winner. Furthermore, the validation on an external dataset comprising 42, 860 segments from 1, 941 matches showed the
robustness of the approach. Finally, the study demonstrates
that the analysis we present can help uncover the pattern
dynamics of a team’s network derived using possession chain
data, by quantitatively analysing measures of performance
that have a specific distribution and that can be used to predict
the performance of a team.
ACKNOWLEDGMENT
The authors would like to thank Dr A. Kalloniatis and Dr T.
Wilkin for the useful and constructive comments during the
development of the material for this article.
AUTHOR CONTRIBUTIONS STATEMENT
S.K. performed the analysis and wrote the manuscript.
S.S., Y.Z., and M.A. contributed to the analysis of the results.
D.D. provided the dataset. D.D. and P.G. helped in formulating the study’s significance from a sports perspective. All
authors contributed to and reviewed the manuscript.
REFERENCES
[1] T. McGarry, ‘‘Applied and theoretical perspectives of performance analysis
in sport: Scientific issues and challenges,’’ Int. J. Perform. Anal. Sport,
vol. 9, no. 1, pp. 128–140, Apr. 2009.
[2] I. Franks, ‘‘Use of feedback by coaches and players,’’ in Proc. Sci. Football
III, 1997, pp. 267–278.
[3] C. Wright, C. Carling, and D. Collins, ‘‘The wider context of performance
analysis and it application in the football coaching process,’’ Int. J. Perform. Anal. Sport, vol. 14, no. 3, pp. 709–733, Dec. 2014.
[4] D. Araújo, K. Davids, and R. Hristovski, ‘‘The ecological dynamics
of decision making in sport,’’ Psychol. Sport Exerc., vol. 7, no. 6,
pp. 653–676, Nov. 2006.
[5] P. S. Glazier, ‘‘Game, set and match? Substantive issues and future directions in performance analysis,’’ Sports Med., vol. 40, no. 8, pp. 625–634,
Aug. 2010.
[6] L. Vilar, D. Araújo, K. Davids, and C. Button, ‘‘The role of ecological
dynamics in analysing performance in team sports,’’ Sports Med., vol. 42,
no. 1, pp. 1–10, Jan. 2012.
[7] K. Davids, D. Araújo, and R. Shuttleworth, ‘‘Applications of dynamical
systems theory to football,’’ in Proc. 5th World Congr. Sci. Football Sci.
Football V, J. Cabri, T. Reilly, and D. Araújo, Eds. Abingdon, U.K.:
Routledge, 2005, pp. 537–550.
[8] B. Travassos, K. Davids, D. Araújo, and T. P. Esteves, ‘‘Performance
analysis in team sports: Advances from an ecological dynamics approach,’’
Int. J. Perform. Anal. Sport, vol. 13, no. 1, pp. 83–95, Apr. 2013.
[9] H. Folgado, K. A. P. M. Lemmink, W. Frencken, and J. Sampaio,
‘‘Length, width and centroid distance as measures of teams tactical performance in youth football,’’ Eur. J. Sport Sci., vol. 14, pp. S487–S492,
Jan. 2014.
[10] P. Passos, K. Davids, D. Araújo, N. Paz, J. Minguéns, and J. Mendes,
‘‘Networks as a novel tool for studying team ball sports as complex social systems,’’ J. Sci. Med. Sport, vol. 14, no. 2, pp. 170–176,
Mar. 2011.
[11] C. Braham and M. Small, ‘‘Complex networks untangle competitive advantage in Australian football,’’ Chaos, Interdiscipl. J. Nonlinear Sci., vol. 28,
no. 5, May 2018, Art. no. 053105.
[12] J. M. Buldú, J. Busquets, J. H. Martínez, J. L. Herrera-Diestra,
I. Echegoyen, J. Galeano, and J. Luque, ‘‘Using network science to analyse
football passing networks: Dynamics, space, time, and the multilayer
nature of the game,’’ Frontiers Psychol., vol. 9, p. 1900, Oct. 2018.
[13] J. Ramos, R. J. Lopes, and D. Araújo, ‘‘What’s next in complex networks?
Capturing the concept of attacking play in invasive team sports,’’ Sports
Med., vol. 48, no. 1, pp. 17–28, Jan. 2018.
[14] L. Gyarmati and X. Anguera, ‘‘Automatic extraction of the passing
strategies of soccer teams,’’ 2015, arXiv:1508.02171. [Online]. Available:
http://arxiv.org/abs/1508.02171
[15] P. Cintia, F. Giannotti, L. Pappalardo, D. Pedreschi, and M. Malvaldi,
‘‘The harsh rule of the goals: Data-driven performance indicators for
football teams,’’ in Proc. IEEE Int. Conf. Data Sci. Adv. Analytics (DSAA),
Oct. 2015, pp. 1–10.
[16] J. Duch, J. S. Waitzman, and L. A. N. Amaral, ‘‘Quantifying the performance of individual players in a team activity,’’ PLoS ONE, vol. 5, no. 6,
2010, Art. no. e10937.
[17] R. Hristovski, K. Davids, D. Araujo, and P. Passos, ‘‘Constraints-induced
emergence of functional novelty in complex neurobiological systems:
A basis for creativity in sport,’’ Nonlinear Dyn.-Psychol. Life Sci., vol. 15,
no. 2, p. 175, 2011.
[18] J. M. Buldú, J. Busquets, I. Echegoyen, and F. Seirullo, ‘‘Defining a
historic football team: Using network science to analyze Guardiola’s F.C.
Barcelona,’’ Sci. Rep., vol. 9, no. 1, pp. 1–14, Dec. 2019.
[19] P. Silva, R. Duarte, P. Esteves, B. Travassos, and L. Vilar, ‘‘Application
of entropy measures to analysis of performance in team sports,’’ Int. J.
Perform. Anal. Sport, vol. 16, no. 2, pp. 753–768, Aug. 2016.
[20] L. Pappalardo, P. Cintia, P. Ferragina, E. Massucco, D. Pedreschi, and
F. Giannotti, ‘‘PlayeRank: Data-driven performance evaluation and player
ranking in soccer via a machine learning approach,’’ ACM Trans. Intell.
Syst. Technol., vol. 10, no. 5, pp. 1–27, Nov. 2019.
[21] T. Decroos, L. Bransen, J. Van Haaren, and J. Davis, ‘‘Actions speak louder
than goals: Valuing player actions in soccer,’’ in Proc. 25th ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining, Jul. 2019, pp. 1851–1861.
[22] L. Vilar, D. Araújo, K. Davids, and Y. Bar-Yam, ‘‘Science of winning soccer: Emergent pattern-forming dynamics in association football,’’
J. Syst. Sci. Complex., vol. 26, no. 1, pp. 73–84, Feb. 2013.
[23] R. Duarte, D. Araújo, H. Folgado, P. Esteves, P. Marques, and K. Davids,
‘‘Capturing complex, non-linear team behaviours during competitive football performance,’’ J. Syst. Sci. Complex., vol. 26, no. 1, pp. 62–72,
Feb. 2013.
[24] T. Decroos, J. Van Haaren, and J. Davis, ‘‘Automatic discovery of tactics
in spatio-temporal soccer match data,’’ in Proc. 24th ACM SIGKDD Int.
Conf. Knowl. Discovery Data Mining, Jul. 2018, pp. 223–232.
[25] K. Davids, C. Button, D. Araújo, I. Renshaw, and R. Hristovski,
‘‘Movement models from sports provide representative task constraints for
studying adaptive behavior in human movement systems,’’ Adapt. Behav.,
vol. 14, no. 1, pp. 73–95, Mar. 2006.
[26] P. Silva, R. Duarte, J. Sampaio, P. Aguiar, K. Davids, D. Araújo, and
J. Garganta, ‘‘Field dimension and skill level constrain team tactical
behaviours in small-sided and conditioned games in football,’’ J. Sports
Sci., vol. 32, no. 20, pp. 1888–1896, 2014.
[27] C. E. Shannon, ‘‘A mathematical theory of communication,’’ Bell Syst.
Tech. J., vol. 27, no. 3, pp. 379–423, Jul./Oct. 1948.
[28] F. Kaspar and H. G. Schuster, ‘‘Easily calculable measure for the complexity of spatiotemporal patterns,’’ Phys. Rev. A, Gen. Phys., vol. 36, no. 2,
pp. 842–848, Jul. 1987.
[29] A. Lempel and J. Ziv, ‘‘On the complexity of finite sequences,’’ IEEE
Trans. Inf. Theory, vol. IT-22, no. 1, pp. 75–81, Jan. 1976.
[30] P. Li, C. Liu, K. Li, D. Zheng, C. Liu, and Y. Hou, ‘‘Assessing the
complexity of short-term heartbeat interval series by distribution entropy,’’
Med. Biol. Eng. Comput., vol. 53, no. 1, pp. 77–87, Jan. 2015.
[31] P. Li, C. Karmakar, J. Yearwood, S. Venkatesh, M. Palaniswami, and
C. Liu, ‘‘Detection of epileptic seizure based on entropy analysis of shortterm EEG,’’ PLoS ONE, vol. 13, no. 3, 2018, Art. no. e0193691.
[32] G. C. Cawley, ‘‘Leave-one-out cross-validation based model selection
criteria for weighted LS-SVMs,’’ in Proc. IEEE Int. Joint Conf. Neural
Netw. Proc., Jul. 2006, pp. 1661–1668.
[33] R. Tibshirani, ‘‘Regression shrinkage and selection via the lasso,’’ J. Roy.
Stat. Soc., B, (Methodol.), vol. 58, no. 1, pp. 267–288, Jan. 1996.
90278 VOLUME 8, 2020
S. Kusmakar et al.: Machine Learning Enabled Team Performance Analysis in the Dynamical Environment of Soccer
[34] L. Pappalardo, P. Cintia, A. Rossi, E. Massucco, P. Ferragina, D. Pedreschi,
and F. Giannotti, ‘‘A public data set of spatio-temporal match events in
soccer competitions,’’ Sci. Data, vol. 6, no. 1, pp. 1–15, Dec. 2019.
[35] J. A. Hanley and B. J. McNeil, ‘‘A method of comparing the areas under
receiver operating characteristic curves derived from the same cases,’’
Radiology, vol. 148, no. 3, pp. 839–843, Sep. 1983.
[36] D. R. Brillinger, ‘‘Some data analyses using mutual information,’’ Brazilian J. Probab. Statist., vol. 18, pp. 163–182, Dec. 2004.
[37] T. Chen and C. Guestrin, ‘‘XGBoost: A scalable tree boosting system,’’
in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,
Aug. 2016, pp. 785–794.
SHITANSHU KUSMAKAR (Member, IEEE)
received the master’s degree in clinical engineering from IIT Madras, India, in 2012, and the
Ph.D. degree in electrical and electronic engineering from The University of Melbourne, Australia,
in 2019. He was a Melbourne India Postgraduate
Scholar with The University of Melbourne. He
is currently a Research Fellow of complex data
analytics with the School of Information Technology, Deakin University, Australia. His doctoral
research focused on developing an artificial intelligence (AI)-based system
for automated detection of epileptic seizures using a wearable sensing device.
His research interests include artificial intelligence, machine learning, cognitive computing, health-AI, and time-series analysis.
SERGIY SHELYAG received the Ph.D. degree
from the University of Göttingen, Germany, in
2004. He worked as a Research Fellow with
the Department of Applied Mathematics, University of Sheffield, U.K., and with the Department
of Mathematics and Physics, Queen’s University,
Belfast, U.K. He was a Future Fellow with the
Department of Mathematical Sciences, Monash
University, Australia. He worked as a Senior Lecturer with the University of Northumbria, Newcastle, U.K. He is currently a Senior Lecturer with the School of Information Technology, Deakin University, Geelong, VIC, Australia. His current
research interests are in mathematical and computational modelling of complex physical processes, data analytics, machine learning, and dynamical
systems.
YE ZHU (Member, IEEE) received the Ph.D.
degree in artificial intelligence with a Mollie
Holman Medal for the best doctoral thesis of
the year from Monash University, Australia, in
2017. He joined Deakin University, in 2017, as a
Research Fellow of complex system data analytics.
He has been a Lecturer with the School of Information Technology, Deakin University, Australia,
since 2019. His research works focus on clustering
analysis, anomaly detection, and their applications
for pattern recognition and information retrieval.
DAN DWYER received the Ph.D. degree from
Griffith University, Queensland. He worked as a
Research Active-Academic with the University of
Tasmania, and the University of Newcastle. He
worked as a Sport Scientist with the Victorian
Institute of Sport in Melbourne. He is currently
a Senior Lecturer of applied sport science with
Deakin University. His research focuses on the
measurement, analytics, and prediction in sport.
He is a member of the Centre for Sport Research.
PAUL GASTIN is currently a Professor and the
Head of Sport and Exercise Science, La Trobe
University. His teaching and research focuses
on innovation in sport science and coaching to
enhance the performance of people and organisations across the sport participation spectrum. He
worked in Olympic/Paralympic and professional
sport in Australia and overseas holding senior
positions at the Victorian Institute of Sport, the
UK Sports Institute, and UK Sport. He is an ESSA
Fellow and an accredited level 2 Sport Scientist and High Performance
Manager.
MAIA ANGELOVA received the Ph.D. degree
from the University of Sofia. She was a Lecturer
of physics with the Somerville College, University of Oxford, and a Professor of mathematical
physics with Northumbria University, U.K., until
2016. She is currently a Professor of data analytics
and machine learning with the School of Information Technology, Deakin University, Geelong,
VIC, Australia. She is also the Director of the Data
to Intelligence Research Centre and leads Data
Analytics Research Lab, Deakin University. Her research interests include
mathematical modelling, data analytics, time series, machine learning, and
dynamical systems with applications to health.


A Long-Range Generalized Predictive
Control Algorithm for a DFIG
Based Wind Energy System
J. S. Solís-Chaves, Lucas L. Rodrigues, C. M. Rocha-Osorio, and Alfeu J. Sguarezi Filho, Senior Member, IEEE
Abstract— This paper presents a new Long-range generalized
predictive controller in the synchronous reference frame for a
wind energy system doubly-fed induction generator based. This
controller uses the state space equations that consider the rotor
current and voltage as state and control variables, to execute the
predictive control action. Therefore, the model of the plant must
be transformed into two discrete transference functions, by means
of an auto-regressive moving average model, in order to attain a
discrete and decoupled controller, which makes it possible to treat
it as two independent single-input single-output systems instead of
a magnetic coupled multiple-input multiple-output system. For
achieving that, a direct power control strategy is used, based on
the past and future rotor currents and voltages estimation. The
algorithm evaluates the rotor current predictors for a defined
prediction horizon and computes the new rotor voltages that must
be injected to controlling the stator active and reactive powers. To
evaluate the controller performance, some simulations were made
using Matlab/Simulink. Experimental tests were carried out with
a small-scale prototype assuming normal operating conditions
with constant and variable wind speed profiles. Finally, some conclusions respect to the dynamic performance of this new controller are summarized.
Index Terms—Direct power control, doubly-fed induction generator, flux oriented control, generalized predictive control, long-range
predictive control, wind energy systems.
I. Introduction
RENEWABLE energy production is worldwide accepted by academics and industries like an interesting alternative to traditional generation, mainly due to the growing
scarcity of fossil fuels and the dreadful effect of the greenhouse gases on the climate of Earth, a phenomenon known as
global warming [1], [2].
Of all possible arrangements for wind energy systems, the
most commonly used is the double-fed induction generator
(DFIG) based configuration [3], [4]. Mainly due to its control
stability, since it is possible to access to the stator and rotor
windings [5], to the partial power electronic converter sizing,
as the Back-to-Back converter only processes the 30% of the
rated power [6], but keep in mind that the total cost of the
system is much higher, due primarily to the constant
maintenance of the slip rings and the continuous replacement
of the rotor brushes [7].
On the other hand, the generalized predictive control (GPC)
is a part of the model-based predictive controllers (MBPC)
family. This theory was formulated by Clarke et al. in the late
80s [8]. Its basic algorithm, its properties and interpretations
[9], its possible applications and extensions [10] were
explained for himself and his co-workers in the following
years. In the next decade, for 1997, Zhang et al. presented an
interesting GPC application in the synchronous reference
frame for regulating the stator current of an induction machine
(IM) with experimental results, but without a good dynamic
response in steady state [11]. Kennel et al. in 2001 developed
a practical optimized GPC with flux oriented control (FOC)
for an IM, using a speed control strategy, the prototype can
supply the demand for processing power, considering the realtime conditions in a digital processor application and the
consequent great calculation effort of the GPC algorithms
[12]. In 2016, an interesting paper revisited the operating
principle of MBPCs and identifies its constitutive elements:
the prediction model, the cost function, and the optimization
algorithm, summarizing the most recent research and
providing details about the different proposed solutions [13].
Then, in 2019, Mahmoud and Oyedeji presents a special
survey for Adaptive and Predictive control strategies for wind
turbines [14] which include the nonlinear MBPCs, but without
delving into the GPC strategy, which is also part of that
broader theory of the predictive controllers.
The GPC MBPC proposed here, uses a new DFIG’s
dynamic model based on a discrete transfer function and a
controlled auto-regressive integrated moving average
(CARIMA) model equation to make a stator power control by
means of a long-range rotor current prediction, which will
have an implicit integral term [15]. Although this is a wellknown stochastic model, its application to the DFIG remained
unpublished until now. Therefore, the GPC control law must
minimize a quadratic cost function that uses the past states of
the rotor currents, the N step-ahead rotor current predictors
and the rotor current references, to estimate the next rotor
voltages and thus controlling the stator powers, at real time.
Manuscript received May 30, 2019; accepted July 21, 2019. This work was
supported by UFABC, CNPQ and CAPES. Recommended by Associate Editor
Yanjun Liu. (Corresponding author: J. S. Solís-Chaves.)
Citation: J. S. Solís-Chaves, L. L. Rodrigues, C. M. Rocha-Osorio, and A.
J. Sguarezi Filho, “A long-Range generalized predictive control algorithm for
a DFIG based wind energy system,” IEEE/CAA J. Autom. Sinica, vol. 6, no.
5, pp. 1209–1219, Sept. 2019.
All the authors are with the Department of Energy Engineering at the
Federal University of ABC-UFABC, Santo André, SP, Brazil (e-mail: sebastian.chaves@ufabc.edu.br; lucaslrodri@gmail.com; cmrochaos@hotmail.com;
alfeu.sguarezi@ufabc.edu. br).
Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JAS.2019.1911699
IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019 1209Some GPC formulations across the time has been focused
on different control problems for wind energy systems such as
the pitch angle control, the MPPT control, the system stability,
etc. [14]. Also, the use of different types of generators like the
permanent magnet (PMSG) [16], [17], the squirrel cage
induction generator (SQIG) [12], and even the switching
reluctance generator (SRG) [18] were considered. For all the
above mentioned, the main motivation of this research is to
demonstrate that a long-range GPC formulation with direct
power control (DPC) strategy, that uses a control horizon
equal to the prediction horizon can govern the DFIG’s stator
powers, when generator is working under normal operating
conditions and constant and variable wind speed profiles are
assumed.
λ
From all of these GPC formulations mentioned below, the
work of Vieira Dias et al. proposes a GPC robust strategy, that
forcing to the control signal to follows its reference very
quickly and without overshoot, that rejecting disturbances and
using for tuning the set point only a single parameter [19], as
is done in this paper. The parameter that must be tuned via a
heuristic method is called the controller weighting factor ( ).
An MPPT with a GPC for a PMSG based stand-alone wind
system was proposed by Mohamed Amine Bouzid et al. with
a better performance than the classic PI controller. However,
the proposed controller was evaluated only by simulation
results [16]. Thus, for the long-range GPC proposed here, a
set of experimental results are present to ensure its correct
performance. A Predictive Hysteresis Current Control for a
SRG Drive was presented by Rù et al. in 2016. This algorithm
predicts the plant behavior considering the finite set of the
power converter states and choosing the most suitable voltage
vector so as to reduce the current amplitude error and to
minimize a cost function [18]. A multiple-vector predictive
power control direct model for the grid-side converter control
of a wind energy system PMSG based, using a full field
programmable gate array was presented in [17]. In 2017,
another predictive controller proposed by Kou et al. [20]
describes an optimized control action that can be directly
applied to the power converter, yielding a better controller
performance and achieving a short computation time, using a
finite control set of variables with longer prediction horizons.
In addition, new nonlinear controllers can deal MIMO
systems in the non-triangular form, as is the case of the DFIG
state space model, using a fuzzy logic controller and a state
filter [21], or, combining the back-stepping recursive design
with Lyapunov function theory [22] and thus resolve this
stochastic finite-time control problem. Therefore, this new
strategies could be used to improve the time response and the
performance of the DFIG based wind energy systems.
∆vrdq
The long-range GPC proposed here, performs its control
action, estimating the rotor current predictors and then
applying the proper voltage vectors in the DFIG rotor
terminals, after the minimization of the quadratic cost function
composed of the rotor currents and its references, as well as
the increment in the rotor voltage . This long-range
GPC-dq, also requires constant switching frequency and
constant sampling time, these control parameters are shown in
Appendix B. Moreover, some classical controllers as PI [23],
or nonlinear controllers like fuzzy [24], [25], adaptive and
robust [19], [14] controllers, etc. precise of a big set of rules
or a tricky matrix calculation; in counter-position, this longrange GPC-dq only requires one weighting factor to be
adjusted via a heuristic procedure. However, what it can be an
advantage, can also be a disadvantage, since the iterative trial
and error procedure cannot ensure optimal value for this
factor. Therefore, the approach proposed by Rodriguez et al.
[26], which consists of an analytical method using the Fake
Algebraic Ricatti Equation for the optimal tuning of this
weight factor, could be used to improve the dynamic response
of this GPC-dq.
For demonstrate all above mentioned about this long-range
GPC-dq, authors decides the following paper structure: the
DFIG model based on CARIMA model and the equations for
calculating the generator discrete transfer functions are
described in Section II. Next, in Section III, the GPC-dq
algorithm is explained in all detail, including the rotor current
predictors calculation, the estimation of the past states of the
system, the cost function minimization and its corresponding
predictive control law to evaluate the next component of the
rotor voltage. Then, simulation tests are depicted in Section
IV for fixed and variable rotor speed, also including a
comparison with a PI controller and a parameter variation test.
Experimental test are presented in Section V to corroborate
the performance of this GPC strategy. Finally, some
conclusions about the dynamic performance of the new GPCdq are written in Section VI.
II. The Dfig Model in the Stator-Flux-Oriented
Reference Frame
v
sq |⃗vs,dq|
vsd = 0 ψsq |ψ⃗s,dq| = ψs ψsd = 0
The long-range GPC for DFIG uses the synchronous
reference frame (also named the dq frame). Therefore, the
dynamic model for the controller must be in the same
reference as the plant i.e., the long-range GPC-dq must be
fixed respect to the stator flux space vector, just as the DFIG
model is. This last is also known as the Stator Flux Oriented
Control (SFOC) strategy. Therefore the dynamic model of the
plant meets the following orientation conditions: = ,
, = and [27].
x˙1(t) = d⃗ird(t)/dt x˙2(t) = d⃗irq(t)/dt x1(t) =⃗ird(t)
x2(t) =⃗irq(t) u1(t) =⃗vrd(t) u2(t) =⃗vrq(t)
According to [28], [29], a state space feedback model for
DFIG can be written in dq frame, considering the stator and
rotor currents and voltages. Due to the GPC algorithm
explained below is focused on the rotor side converter (RSC),
as is depicted in Fig. 2, the rotor currents and rotor voltages
are chosen as state and control variables, respectively. These
can be written using the standard nomenclature for state
variables: , , ,
. For control variables: , .
Therefore, the space state equations for DFIG, as shown as
follows:
x˙1(t) = −
Rr
σLr
x1(t)+ωslx2(t)+
1
σLr
u1(t) (1)
x˙2(t) = −ωslx1(t) −
Rr
σLr
x2(t)+
1
σLr
u2(t) −
ωslLm
σLsLr
ψs (2)
Rr Lr
σ = 1 − L2
m/LsLr Lm
ωsl
where , are the rotor resistance and rotor inductance,
is the total coupling factor, is the mutual
inductance and is the DFIG slip angular speed. In Ap-
1210 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019pendix B, the DFIG parameters are summarized.
r s
However, since an expression for the rotor voltage for this
state feedback model is required, in this paper, authors decide
to maintain the commonly used notation with “ ” and “ ”
subscripts for the rotor and stator variables and also due to the
need to use, for obtaining the DFIG model, other variables of
the machine such as stator and rotor fluxes. Therefore, the
above mentioned expression for rotor voltage can be derived,
according with [28], as follows:
⃗vr
,dq = Rr⃗ir,dq + dψ⃗r,dq
dt
+ jωslψ⃗r,dq. (3)
The use of the stator flux orientation method for the closed
control loops allows the independent control of the active and
reactive power of the stator by regulating the rotor currents.
Since the DFIG is connected to the mains by means of a Backto-Back converter, the control goal is to command the stator
powers by applying the right voltage to the rotor terminals.
Replacing the quadrature rotor flux component from [28] in
(3) its possible to write an expression for the rotor voltages in
terms of rotor currents, in this way:
⃗vrdq =Rr⃗irdq + d
dt (Lm⃗isdq + Lr⃗irdq)
+ jωsl(Lm⃗isdq + Lr⃗irdq). (4)
Isolating in the expression for direct stator flux component,
the stator current vector [28]; considering that the stator flux is
constant and its derivative equal to zero, due to the plant is
oriented by the SFOC; it is possible to rewrite the expression
(4) into two separate state equations. Therefore, for d axis:
dird(t)
dt
= −
Rr
σLr
ird(t)+ωslirq(t)+
1
σLr
vrd(t) (5)
And for q axis:
di
rq(t)
dt
= −ωslird(t)−
Rr
σLr
i
rq(t)+
1
σLr
v
rq(t)−
ωslLm
σLsLr
ψs(t). (6)
These equations are equivalent to (1) and (2) and it is
possible rewrite it in two independent transfer functions by
means of the Laplace transform. One for the d axis and
another one for the q axis. In this case, the transfer functions
become orthogonal due to the SFOC. This is an additional
advantage for the proposed GPC controller, because a
multiple-input multiple-output system (MIMO) can be treated
by two independent and totally decoupled single-input singleoutput (SISO) systems.
III. The GPC-Dq Control Algorithm
N
The long-range GPC-dq derived for DFIG power control is
described in following sections. A GPC algorithm gives an
integral action of the closed loop system and the present state
is estimated using the model and some old known input and
output data [30]. This nonlinear controller works with a
constant sample time step equal to 0.1 ms and its prediction
horizon is = 3. For a better comprehension of the longrange GPC-dq, a fully explanation is done, starting with the
DFIG CARIMA model and the estimation of its associated
polynomials, in Section III-A. Then, the rotor current
predictors estimation and the past states of the system are
addressed in Section III-B. Finally, the cost function
minimization and the predictive control law estimation are
explained in Sections III-C and III-D, respectively.
A. Obtaining the DFIG CARIMA Model Coefficients
For digital control applications, a continuous transfer
function must be discretized using, for instance, the ZTransform. Therefore, since that the plant is oriented by
SFOC, (5) and (6) can be discretized. A simplified transfer
function for d axis given by [29] is:
G(s) =
1
Rr + s(σLr)
= (σ1Lr )

1
s+(σRLrr )

=
Ird(s)
Vrd(s)
. (7)
ωslσLr
Graphically, the previous transfer function is presented in
Fig. 1 and its possible to see the cross coupling terms .
Applying the Z-Transform to this continuous transfer
function (7), two discrete transfer functions using the
standards Z-transform tables [31] can be obtained (only a
transfer function for the d axis is shown):
H(z−1) = (σTLr )[1−e−(Rr1/σLrT)z−1 ] (8)
A(z−1)
B(z−1)
A˜(z−1) A(z−1)
B(z−1)
By means of the CARIMA model (6.1) from [15] and the
discrete transfer function (8), a set of coefficients for
and polynomials can be derived. Normally, the
prediction is calculated using instead of [11],
[15] and since to the fact that for this GPC the
polynomial is affected by two discrete time delays, the
coefficients for both polynomials can be estimated as follows:
A˜(z−1) = 1+a˜1z−1 +a˜2z−2 (9)
and
B˜(z−1) = b˜0z−1 +b˜1z−2. (10)
For is possible to write, from (8) and (9): A˜(z−1)
A˜(z−1) = 1−(1+e−(Rr/σLrT))z−1 +e−(Rr/σLrT)z−2 (11)
A˜(z−1) = ∆A(z−1) ∆ = (1−z−1)
A˜(z−1)
a˜0 = 1 a˜1 = −(1+e−(Rr/σLrT)) a˜2 = e−(Rr/σLrT)
where , with . It can be seen that
comparing (11) with (9), the coefficients for polynomial are: , and .
v
rq
+−
−
vrd
+
+
ωsl Ls φs
L
m
ωslσLr
ωslσLr
R
r + s(σLr)
R
r + s(σLr) irq
ird
1 1
Fig. 1. A simplified FT for DFIG.
SOLÍS-CHAVES et al.: A LONG-RANGE GENERALIZED PREDICTIVE CONTROL ALGORITHM FOR A DFIG BASED WIND ENERGY SYSTEM 1211B˜(z−1)
vrd vrq
b˜0 = T/σLr b˜1 = −T/σLr
Owing to that is a polynomial related with the rotor
voltage components and , as can be seen in (5) and (6),
its coefficients can be estimated from (8) and (10), as follows:
and .
With the and coefficients already calculated, A˜(z−1) B˜(z−1)
it is now possible to determine the GPC-dq rotor current
predictors. Fig. 2 and Fig 3 resume the above estimation.
Additionally, in these figures, the DFIG variables necessary to
realize this estimation are shown.
B. The N-Step Ahead Current Predictor Calculation
t
vrd vrq
N T
When the DFIG CARIMA model is already done, and
supposing that at an arbitrary instant of time , the rotor
voltage components and will be well-defined, because
the GPC-dq operates in an incremental form, as is
demonstrated in [15], these voltages will be kept constant for
future steps of sample time . Therefore, the GPC-dq must
N
ˆird(k) ˆirq(k) k
be predicted these steps ahead and give the current
predictors and , with as the sample period. This is
presented in the light grey box of Fig. 3.
∆vrd(k) ∆vrq(k)
(k +1) N
The general form for the rotor current predictors are
commonly expressed by two parts: the past states of the
system and the part related to the future controls of the plant,
that is and [11]. Hence, the rotor currents at
sample time can be written for sample steps in the
future.
For the first future sample step : (N = 1)
ˆirdq(k +1) = g0∆vrdq(k)± s0∆vrqd(k)+irdq(k)+ Frdq(k) (12)
And for the sample step:N ˆi
rdq(k + N) =gn−1∆vrdq(k + N −1)
± sn−1∆vrqd(k + N −1)+ˆirdq(k + N −1)
+ Frdq(k + N −1). (13)
DFIG
Gearbox
Back to Back Coverter
RSC GSC
SVM Grid Side
Control
Control
Law
Rotor
Current
Predictors
Estimation
of
Parameters
Rotor Side Control
Discretization
min J
Transformer
Grid
Ir
, abc
i
r, αβr(t)
i
r, dq(t)
i
r, dq(k − 1)
v
r, dq(k − 1)
Δv
r, dq(k + N)
i
r, dq(k + N)
Vs
, abc, Is, abc
φs, ωsl, Ps, Qs
ω
s, ωm
i
r, dq(t)
v
r, dq(t)
i
r, dq(k)
v
r, dq(k)
i
r, dq(k)
vrd(k)
v
rq(k)
θsl
θsl
v
r, αβr(t) vr, αβr(k)
v
r, abc
v
s, abc Is, abc
abc
αβr
αβ
dq dq
αβr
z−1
z−1
*
*
Fig. 2. Block diagram for the GPC-dq with DFIG based wind energy system.
(MTM + λI)−1MT
GPC Control
Law
z−1
Past States
of the
System
M
z−1
DFIG
Plant
A(z−1)
Rotor Current
Prediction
i
rqd(k)
DFIG
CARIMA Model
Polynomials
ˆ
z−1
F
rqd(k + N)
ˆ i
rqd(k + N − 1)
F
rqd(k + N − 1)
i
rqd(k + N)
v
Δvrqd(k) rqd(k) irdq(k)
v
irqd(k + N) rdq(k)
Power Estimator & Rotor
Currents Generator
B(z−1)
Ps
, ref
Qs, ref
*
+ +
+ +
+
+
+
−
−
+
λ
Fig. 3. Control law and rotor current predictors estimation for the GPC-dq.
1212 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019nu
nv nu = nv = N
For this GPC-dq, the control horizon is equal to the
prediction horizon , it means that .
g0 g1,...,gm−1
ωslσLr
G(z−1) = [g0,g1,...,gm−1]T s0
s1,...,sm−1 G(z−1)
ωslσLr
S (z−1) = [s0,s1,...,sm−1]T
Terms , are derived from the step response of
the model and the cross-coupling term . With these
terms it is possible built a parameter vector
. Correspondingly, terms ,
are parameters obtained from combining
and . Thus, another parameter vector
can be obtained.
Both terms and can be calculated as follows [ g s 11]:
g0 = b0,g1 = b1 −a1b0,...
gm = −a1gm−1 −a2gm−2 −b0ςsm−2 −b1ςsm−3 (14)
s0 = 0,s1 = b0ςg0,s2 = −a1s0 +b1ςg0,...
sm = −a1sm−1 −a2sm−2 +b0ςgm +b1ςgm−1 (15)
where . ς = ωslσLr
Terms and are the past states of the system. Since Frd Frq
an incremental form is used and the system is in steady state
they are initially equal to zero. Therefore, the expressions for
these past states for the first sample instant can be estimated
as follows:
Frd(k +1) =−a1Frd(k)−a2Frd(k −1)+b0ςFrq(k)
+b1ςFrq(k −1) (16)
F
rq(k +1) =−a1Frq(k)−a2Frq(k −1)−b0ςFrd(k)
−b1ςFrd(k −1) (17)
And for the sample instant, in this way:N F
rd(k + N) =−a1Frd(k + N −1)−a2Frd(k + N −2)
+b0ςFrq(k + N −1)+b1ςFrq(k + N −2) (18)
F
rq(k + N) =−a1Frq(k + N −1)−a2Frq(k + N −2)
−b0ςFrd(k + N −1)−b1ςFrd(k + N −2) (19)
For this long-range GPC-dq, the past states are the DFIG
rotor currents.
Finally, it is possible rewrite (13) separating d and q
components in a matrix form to obtain the definitive
expression for rotor currents predictors:

ˆi
rq(k +1)
...
ˆi
rq(k + N)
ˆird(k +1)
...
ˆird(k + N)

= [ GS G −S ][ ∆∆vvrd rq((kk)) ]+

F
rq(k)
...
F
rq(k + N −1)
Frd(k)
...
Frd(k + N −1)

+

ˆi
rq(k)
...
ˆi
rq(k + N −1)
ˆird(k)
...
ˆird(k + N −1)

(20)
The first element of the is equal to zero because the S (z−1)
cross-coupling effect in DFIG does not appear until the
second sample instant occurs [11]. The rotor current
predictors calculation and the estimation of the past states are
graphically presented in Fig. 3.
C. Estimating the Cost Function J
The GPC-dq must be applying a control sequence that
minimizes a quadratic cost function of the form [11]:
J =
N∑m=1
[ˆirqd(k +m)−i∗ iqd(k +m)]T
[ˆirqd(k +m)−i∗ rqd(k +m)]+∆vT rqd(k)λI∆vrqd(k) (21)
N
I n×n λ
i∗
rqd(k +m)
where is the finite number of output prediction sample instants, is the Identity matrix, is a weighting factor that
must be adjusted by heuristic procedure and are the
future reference currents.
The cost function is shown in Fig. 3, and it can be seen the
input variables to make possible the long-range predictive
control action and the output control signals (rotor voltages
increments) which are necessary for the rotor current
predictors calculation. Moreover, in Fig. 3 also appears the
boxes representing the DFIG CARIMA polynomials and the
past states of the system, explained in Sections III-A and IIIB, respectively.
D. Obtaining the Control Law
∆vrdq(k)
When the cost function of the future current errors is
minimized, the long-range GPC-dq provides a relationship for
the next rotor voltage inputs based on the estimation of the
control voltage increments .
[ ∆∆vvrd rq((kk)) ] = (MT M +λI)−1MT


i∗
rd(k +1)
...
i∗
rd(k + N)
i∗
rq(k +1)
...
i∗
rq(k + N)

−

Frd(k)
...
Frd(k + N)
F
rq(k)
...
F
rq(k + N)

−

ird(k)
...
ˆird(k + N −1)
i
rq(k)
...
ˆi
rq(k + N −1)


(22)
where
M = [ GS G −S ]. (23)
i∗
r,dq(k + N)
Fr,dq(k + N) k
(k + N −1)
Note that the predictive control law needs the rotor current
references , the past states of the system
and the rotor currents in instant sample and the
rotor current predictors in instant sample, making
up an extended column vector. This is depicted in Fig. 3 and
presented in (22).
Therefore, the long-range predictive control law estimates
the new control rotor voltages for the GPC-dq as follows:
v
rq(k) = vrq(k −1)+∆vrq(k)
vrd(k) = vrd(k −1)+∆vrd(k) (24)
Detailed block diagrams for the long-range GPC-dq, for its
control law and for the rotor current predictors are depicted in
Figs. 2 and 3 for a better comprehension of the algorithm
developed here and explained in previous Sections.
SOLÍS-CHAVES et al.: A LONG-RANGE GENERALIZED PREDICTIVE CONTROL ALGORITHM FOR A DFIG BASED WIND ENERGY SYSTEM 1213Additionally, a flowchart illustrating this control algorithm is
present in Fig. 17 in Appendix A.
IV. Simulation Results
Computational simulations were developed using
Matlab/Simulink to evaluate the performance of the GPC-dq.
Four types of tests on normal operating conditions are
considered: Fixed rotor speed, variable rotor speed,
performance comparison, and parameters variation.
A. Fixed Rotor Speed
The rotor angular speed was fixed at 1690 rpm and step
response for both powers were made. These responses are
presented in Fig. 4.
Red signal is the reactive stator power and the dotted blue
line represent its power reference.
The green signal and the dotted orange signal are the active
stator power and its reference. The settling time is very short
and a detailed view of this is shown in Fig. 5. The steady state
error is equal to zero for both stator powers.
In Fig. 5 the settling time for the active stator power is equal
to 3.2 ms.
B. Variable Rotor Speed
For doing this test, a variable wind speed profile are
simulated. The mechanical speed is increased from 1700 rpm
(sub-synchronous speed) through 1800 rpm (synchronous
speed) to 2200 rpm (super-synchronous speed value). Red
signal in Fig. 6 shown this speed profile. Green and blue
signals are the power responses. Dotted lines are the power
references.
Similarly to the precedent test, the long-range GPC-dq can
respond to the power references, with a little and short
overshoot and zero steady state error.
In the reference frame, the rotor current for the αβr α
component is also shown. Its response and the phase
inversions are in concordance with the speed profile. This is
depicted in Fig. 7.
Near to 6 s and to 9 s, two phase inversions can be
observed. Frequency changes are also observed between 7 s
and 7.8 s.
C. Performance Comparison
To demonstrate the superior performance of the GPC, a
comparative simulation is conducted between it and a PI
controller, using the same plant, that is, the same circuit model
with the DFIG parameters resumed in Appendix B. PI gains
are adjusted following the procedure stated in [23].
i
rq
i∗
rq
In Fig. 8 it can be seen the time responses for both
controllers. Settling times for when a step variation is done
in its reference at 1.55 s (red line) are presented. For the PI
controller, the settling time is equal to 1.822 s (grey dot-line)
and for the long-range GPC-dq is equal to 1.5521 s (black line).
While the PI controller takes 0.272 seconds to stabilize, the
long-range GPC-dq just takes 2.1 milliseconds. Therefore, this
predictive controller is faster than the PI controller, despite its
small oscillation around its reference in the steady state, less
than 1%.
Qs
Q*s
P
s
P*
s
1500
1000
500
0
−500
−1000
−1500
5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0
Tempo (s)
P (W) Q (var)
Fig. 4. Fixed-Speed stator power responses and . Ps(t) Qs(t)
53932 S
P
s
P*
s
Tempo (s)
5.3900 5.3905 5.3910 5.3915 5.3920 5.3925 5.3930 5.3935 5.3940
Magnitude (A)
−300
−350
−400
−450
Fig. 5. Detailed view for step response for . Ps(t)
Qs
ω
r
Q*s
P
s
P*
s
1700 rpm
1800 rpm
2200 rpm
5.0 6.5 7.0 7.5 8.0 8.5 9.0 5.5 6.0
Tempo (s)
1500
2000
1000
500
0
−500
−1000
−1500
P (W) Q (var)
Fig. 6. Power responses and for the variable rotor speed test. Ps(t) Qs(t)
5.0 6.5 7.0 7.5 8.0 8.5 9.0 5.5 6.0
Tempo (s)
Magnitude (A)
5
−5
4
−4
3
−3
2
−2
1
−1
0
Fig. 7. Alpha component for the rotor current for the variable rotor irαr
speed test.
1214 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019D. Parameter Variation Analysis
Rr Lm
Lm σ
To verify the robustness of the proposed long-range GPCdq algorithm, a 50% of increment in the and parameters
of the DFIG is performed. The values of the stator and the
rotor leakage inductance are modified and consequently the
values of and are affected too.
The power response for the q component rotor current is
presented in Fig. 9. Again, the GPC shows a faster response, a
detail of this settling time (3 ms) is shown in Fig. 10.
Usually, in DFIG based wind systems, an increase of more
than 5% in its parameters values, is an alert to take out the
generator to maintenance [27].
V. Experimental Results
Some tests are carried out to validate the GPC-dq in an
experimental setup. A 3.0 kW rated power DFIG was used,
coupled to a DC motor to emulate the mechanical wind
power, its parameters are present in the Appendix B.
(v) (i) ωm
The GPC-dq was implemented using a Digital Signal
Processor (DSP) model TMS320F28335 and electronic boards
to acquire voltage , current and speed ( ) signals.
The main components and its configuration are presented in
Fig. 11.
A photograph for the DFIG small-scale system used for the
experimental purposes is depicted in Fig. 12.
λ 0.0045
10kHz
N
The weighting control factor is equal to , the PWM
commutation frequency is equal to and the control
horizon is equal to 3, for all tests.
Two types of different tests in normal operation conditions
were made to validate this new controller: fixed rotor speed
0.5 1.0 1.5 2.0
Time (s)
2.5
1.54 1.545 1.55 1.555 1.56
8 7 6 5 4 3
i*
rq
i
rqGPC − dq irqPI
1.55 1.60 1.65 1.70 1.75 1.80 1.85
8 7 6 5 4 3 2 1
1.5521 s 1.822 s
1.5521s
−2
8 6 4 2 0
10
12
14
16
Amplitude (A)
i*
rq
i
rqGPC − dq
i
rqPI
Fig. 8. Comparison between a PI controller and the GPC–dq.
i*
rq
i
rq
5.0 6.5 7.0 7.5 8.0 8.5 9.0 5.5 6.0
Tempo (s)
3.5
3.0
2.5
2.0
1.5
1.0
Magnitude (A)
i
Fig. 9. Step response for . Parameter variations test. rq(t)
i*
rq
i
rq
Tempo (s)
7.9000 7.9005 7.9010 7.9015 7.9020 7.9025 7.9030 7.9035
3.75
3.70
3.65
3.60
3.55
3.50
3.45
3.40
3.35
3.30
Magnitude (A)
i
Fig. 10. Detailed view for the step response of . rq(t)
DFIG
RSC GSC
DC
MOTOR
C
A/D
Converter
PWM
DSP TMS320F28335
D/A
Converter
Scope
PWM Pulses
Hall Effect Sensors for V
and I Signals
-Interface Boards
v
s
v, i, P, Q, ω$
i
s
iR
ω
$
vR
Fig. 11. Connection diagram for experimental setup.
SOLÍS-CHAVES et al.: A LONG-RANGE GENERALIZED PREDICTIVE CONTROL ALGORITHM FOR A DFIG BASED WIND ENERGY SYSTEM 1215and variable rotor speed. Both of these are described in detail
in the next sections.
A. Fixed Speed Test
ωm = 1690 rpm
i∗
rd(t) i∗ rq(t)
The rotor mechanical speed was fixed at .
Both current references and are varying in a series
of steps, this steps are shown in Fig. 13.
ird(t) irq(t)
i∗
rd(t)
i∗
rq(t)
In this test, both currents and follows its
references, even when a little noise due to the Back-to-Back
Converter PWM commutation is observed. The GPC-dq step
response is tested generating step signals that varies from 1.13
A to 2.25 A and from 2.25 A to 3.37 A for and from
3.37 A to 1.13 A for .
ird(t)
For a better comprehension of these responses, a detailed
zoom for is shown in Fig. 14, being possible to see the
settling time equal to 3.35 ms when its reference rises from
1.13 A to 2.15 A in 53.35 ms approx.
i
rq(t)
ird(t) irq(t)
Another detailed response, this time for is shown in
Fig. 15. While remains constant in 2.15 A, varies
from 1.13 A to 3.37 A, the same settling time is achieved in
50 ms.
isa(t) ird(t) irq(t)
Analyzing the precedent figures it can be concluded that the
GPC-dq step response is fast enough for its application in
DFIG based wind power systems. In both Figs. 14 and 15 it
can be noted a change in the stator current phase A response
, when and varies. The corresponding
isa(t)
vsa(t)
r/s ms
amplitude for rises from 1.09 A to 2.83 A in Fig. 15
while the stator voltage amplitude for phase A remains
invariant. Its period changes from 448.6 at 15 to 348.1
r/s at 67 ms in Fig. 14, remaining invariant the phase A stator
voltage frequency.
B. Variable Speed Test
ωm irαr
αβr
ird(t) irq(t)
For this test a variable wind speed profile was emulated
using a variable DC Motor mechanically coupled to the DFIG.
A variation in the rotor speed was made from the subsynchronous speed at 1701 rpm, passing by synchronous
speed (1800 rpm) and achieving a super-synchronous speed at
2000 rpm. In Fig. 16 four signals can be seen: the mechanical
rotor speed ( ), the rotor current for A phase ( ) in its
rotor reference frame ( ) and the rotor current components
in synchronous reference frame , .
irαr(t)
ird(t)
i
rq(t)
When the mechanical speed rises from 1701 rpm and pass
up to its synchronous speed until 2000 rpm, approximately at
2 s, a frequency inversion in the A phase rotor current
can be observed, while the rotor current components ,
still remains constant. When the speed descends again to
1701 rpm. Another little phase inversion can be seen when the
speeds rise to sub-synchronous to synchronous speed in the
left side of the Fig. 16 approx. in 1 s.
VI. Conclusion
A new DFIG discrete model was presented, based on the
CARIMA model. With this dynamic model, a new long-range
generalized predictive controller was achieved. This new
GPC-dq presented satisfactory experimental results when it
DFIG
DC
Motor
AC Sources
Signal Acquisition Boards
AC Source
AC Source +
Rectifier
Scope
DSP
Back-to-Back
Converter
Fig. 12. Small–Scale laboratory setup.
1.13 A
2.25 A
3.37 A
3.37 A
1.13 A
i
rq
ird
i*rd
i*
rq
1 500mv/ 2 500mv/ 3 500mv/ 4 500mv/ 0.0 s 50.00 Parar ms/
2 1
33
1 4
Fig. 13. Rotor fixed-speed test — Step response for and . ird(t) irq(t)
448.6
rad/s
348.1
rad/s
311 V
v
sa
i
sa
ird
i
rq
3.37 A
1.13 A 2.15 A
3.35
ms
2.83 A
1 1.00 V/ 1.00 V/ 2 3 500mv/ 4 500mv/ 723.0 Parar ms 10.00ms/
1 1
2 2
3 44
Fig. 14. Rotor fixed-speed test — A detailed step response for . ird(t)
i
sa
V
sa
ird
i
rq
3.35 ms
2.83 A
1.09 A
1.13 A
3.37 A
2.15 A
311 V
1.74 A
1 1.00 V/ 1.00 V/ 2 3 500mv/ 4 500mv/ 327.0ms 10.00 Parar ms/
1 1
2 2
3 44
i
Fig. 15. Rotor fixed-speed test — A detailed step response for . rq(t)
1216 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019N
was tested by means of a DSP system and a small-scale
laboratory experimental setup. The dynamic responses for the
power steps imposed to the system satisfied the requirement
for a grid-connected DFIG wind energy system working under
normal operating conditions, and confirm that the GPC-dq is
totally applicable to these kinds of renewable energy systems.
The proposed algorithm had been explained in all detail,
although for simulation and experimental purposes, the
prediction horizon is restricted to a value no greater than 3,
but a flowchart for the general case is presented in the
λ
Appendix A. An additional advantage of this GPC strategy,
relative to other controllers such as PI, lies in the fact that a
single weight parameter needs to be adjusted. However, its
value could not be the optimal one. In simulation scenarios,
the DFIG parameter variations were also tested. Again, the
GPC remains a null steady-state error and a little and short
overshoot. The settling time for the power responses is similar
to presented in the fixed and variable speed tests. For the fixed
step response test both, the settling time and the steady-state
error are fast enough with a few of milliseconds of time (near
to 3 ms) and a percentage less than 1%. Also, when GPC-dq is
compared with a PI controller, a faster settling time is
observed. In variable rotor speed conditions, the GPC-dq
performance is adjusted to the requirements, due to the fact
that the rotor currents, and therefore the stator powers, remain
constants and following the references imposed. In addition,
the generator can provide active and reactive power to the grid
when it passes from sub-synchronous to super-synchronous
speed. The PWM commutation noise present in experimental
results is contemptible when DFIG is linked to the power
network and additional filters are not needed.
APPENDIX A
Flowchart for the GPC-Dq
The flowchart for the long-range GPC-dq is presented as a
help to understand the iterative estimation of the rotor current
predictors and the predictive control voltages for the rotor
terminals.
N
This flowchart was thought for the general case, it means,
when the control and prediction horizons are equal to .
APPENDIX B
DFIG Parameters
For experimental tests a 3 kW DFIG was used, its
parameters are shown in Table I. These parameters were
determined according to the IEEE 112 standard [32].
Additionally, the weighting factor parameter and the λ
PWM switching frequency are listed too. Same parameters are
used for the simulation scenarios.
TABLE I
DFIG and Test Bench Parameters
Parameter Value
Stator resistance per phase ( ) Rs 1 Ω
Stator inductance per phase ( ) Ls 0.2010 H
Rotor resistance per phase ( ) Rr 3.122 Ω
Rotor inductance per phase ( ) Lr 0.2010 H
Mutual inductance ( ) Lm 0.1917 H
Synchronous stator speed ( ) ωs 120π rad/s
Pole pairs ( ) PP 2
Nominal active power ( ) Ps 3 kW
Nominal stator voltage ( ) Vs 220/380 ∆−Y V
Nominal rotor voltage ( ) Vr 440 Y V
PWM sample reriod ( ) T 0.1 ms
Control horizon ( ) N 3
Weighting control factor ( ) λ 0.0045
1701 rpm 1800 rpm 1701 rpm
2000 rpm
1 A
2.61 A
ird
i
rq
i
rαr
ω
m
2 A
1
1
2
2 2
3
3 3
2.00 V/ 2.00 V/ 1.00 V/ 4 100 2.400 s mv/ 500.0 Parar ms/
4 4
Fig. 16. Rotor variable speed test — Rotor currents response.
Start
Choose N
N = n + 1
is
N = n + 1?
is
N = n + 1?
Calculate
G(z−1) and S(z−1)
Calculate
the n × n matrix
(MTM + λI)−1
Calculate
Δvrd(t) = Δvrq(t)
Yes
No
Yes
No
End
While t = 0
ird(1) and irq(1)
Δvrd(t) = Δvrq(t) = 0
^ ^
^ ^
While t ≤ ts
ird(t) and irq(t) with
i
rq(1) and ird(1)
^ ^
^ ^
ird(t)
ird(t)
^*
^*
Calculate
v
−r, dq(t) = Δvrdq(t − 1)
Fig. 17. Flowchart for the GPC–dq.
SOLÍS-CHAVES et al.: A LONG-RANGE GENERALIZED PREDICTIVE CONTROL ALGORITHM FOR A DFIG BASED WIND ENERGY SYSTEM 1217References
GWEC, “Global wind report - annual market update 2017, ” Tech. Rep.,
Global Wind Energy Council, 2018.
[1]
GWEC, “Wind power is crucial for combating climate change., ” Tech.
Rep., Global Wind Energy Council, 2008.
[2]
W. Cao, Y. Xie, and Z. Tan, Wind Turbine Generator Technologies.
INTECH Open Access Publisher, 2012.
[3]
F. Blaabjerg and K. Ma, “Future on power electronics for wind turbine
systems,” IEEE Journal of Emerging and Selected Topics in Power
Electronics, vol.1, no.3, pp.139–152, 2013.
[4]
R. Datta and V. T. Ranganathan, “Variable-Speed wind power
generation using a doubly fed wound rotor induction machine: a
comparison with alternative schemes,” IEEE Power Engineering
Review, vol.22, pp.52, July. 2002.
[5]
F. Blaabjerg and K. Ma, “Wind energy systems,” in Proc. of the IEEE ,
2017.
[6]
J. A. Baroudi, V. Dinavahi, and A. M. Knight, “A review of power
converter topologies for wind generators,” Renewable Energy, vol.32,
no. 14, pp.2369–2385, 2007.
[7]
D. W. Clarke, C. Mohtadi, and P. Tuffs, “Generalized predictive control
part I. the basic algorithm,” Automatica, vol.23, no.2, pp.137–148,
1987.
[8]
D. Clarke, C. Mohtadi, and P. Tuffs, “Generalized predictive control
part II. extension and interpretations,” Automatica, vol. 23, no.2,
pp. 149–160, 1987.
[9]
D. W. Clarke and C. Mohtadi, “Properties of generalized predictive
control,” Automatica, vol. 25, no.6, pp.859–875, 1989.
[10]
L. Zhang, R. Norman, and W. Shepherd, “Long-Range predictive
control of current regulated PWM for induction motor drives using the
synchronous reference frame,” IEEE Transactions on Control Systems
Technology, vol.5, no.1, pp.119–126, 1996.
[11]
R. Kennel, A. Linder, and M. Linke, “Generalized predictive control
(GPC)-ready for use in drive applications, ” in Proc. PESC. IEEE 32nd
Annual Power Electronics Specialists Conf. , vol. 4, pp. 1839–1844,
2001.
[12]
S. Vazquez, J. Rodriguez, M. Rivera, L. G. Franquelo, and M.
Norambuena, “Model predictive control for power converters and
drives: advances and trends, ” IEEE Transactions on Industrial
Electronics, vol. 64. pp. 935-947, Feb. 2017.
[13]
M. S. Mahmoud and M. O. Oyedeji, “Adaptive and predictive control
strategies for wind turbine systems: a survey,” IEEE/CAA Journal of
Automatica Sinica, vol.6, pp.364–378, March. 2019.
[14]
A. Linder, R. Kanchan, R. Kennel, and P. Stolze, Model-Based
Predictive Control of Electric Drives. Cuvillier, 2010.
[15]
M. A. Bouzid, A. Massoum and S. Zine, “Generalized predictive control
of standalone wind energy generation system,” International Journal of
Renewable Energy Research, vol.6, no.1, pp.220–228, Feb. 2016.
[16]
Z. Zhang, F. G. Hui Fang, J. Rodrguez, and R. Kennel, “MultipleVector model predictive power control for grid-tied wind turbine system
with enhanced steady-state control performance, ” IEEE Transactions
On Industrial Electronics, vol. 64, pp. 6287–6298, Aug. 2017.
[17]
D. D. Rú, M. Morandin, S. Bolognani, and M. Castiello, “Model
predictive hysteresis current control for wide speed operation of a
synchronous reluctance machine drive, ” Industrial Electronics Society,
IEEE, pp. 2845–2850, Oct. 2016.
[18]
S. V. Dias, W. A. da Silva, L. L. dos Reis, and J. C. T. Campos,
“Robust generalized predictive control applied to the rotor side
converter of a wind power generator system based on DFIG, ” in Proc.
11th IEEE/IAS Int. Conf. on Industry Applications (INDUSCON), pp. 1-
6, IEEE, 2014.
[19]
P. Kou, D. Liang, J. Li, L. Gao, and Q. Ze, “Finite-Control-Set model
predictive control for DFIG wind turbines,” IEEE Transactions on
Automation Science and Engineering, 2017.
[20]
S. Sui, C. L. P. Chen, and S. Tong, “Fuzzy adaptive finite-time control
design for nontriangular stochastic nonlinear systems,” IEEE
Transactions on Fuzzy Systems, vol.27, pp.172–184, Jan. 2019.
[21]
S. Sui, S. Tong, and C. L. P. Chen, “Finite-Time filter decentralized
control for nonstrict-feedback nonlinear large-scale systems,” IEEE
Transactions on Fuzzy Systems, vol.26, pp.3289–3300, Dec. 2018.
[22]
A. L. L. F. Murari, J. A. T. Altuna, R. V. Jacomini, C. M. RochaOsorio, J. S. Solís-Chaves, and A. J. S. Filho, “A proposal of project of
PI controller gains used on the control of doubly-fed induction
generators,” IEEE Latin America Transactions, vol.15, pp. 173–180,
Feb. 2017.
[23]
C. Rocha-Osorio, J. S. Solís-Chaves, I. R. Casella, C. Capovilla, J. A.
Puma, and A. S. Filho, “GPRS/EGPRS standards applied to DTC of a
DFIG using fuzzy PI controllers,” International Journal of Electrical
Power & Energy Systems, vol.93, pp.365–373, 2017.
[24]
C. M. Rocha-Osorio, J. S. Solís-Chaves, L. L. Rodrigues, J. A. Puma,
and A. S. Filho, “Deadbeat fuzzy controller for the power control of a
doubly fed induction generator based wind power system,” ISA
Transactions, vol.88, pp.258–267, 2019.
[25]
L. L. Rodrigues, O. A. C. Vilcanqui, A. L. L. F. Murari, and A. J. S.
Filho, “Predictive power control for DFIG: a fare-based weighting
matrices approach,” IEEE Journal of Emerging and Selected Topics in
Power Electronics, vol.7, pp.967–975, June. 2019.
[26]
G. Abad, J. López, M. Rodríguez, L. Marroyo, and G. Iwanski,
Dynamic Modeling of the Doubly Fed Induction Machine, pp. 209–239.
Wiley IEEE Press, 2011.
[27]
A. J. S. Filho, A. L. L. F. Murari, a nd, et al, “A state feedback DFIG
power control for wind generation,” Eletrônica de Potência -
SOBRAEP, vol.20, pp.151–159, March. 2015.
[28]
A. L. L. F. Murari, “Proposta de projeto de ganhos de controladores pi
empregados no controle de geradores de indução com rotor bobinado
aplicados a sistemas eólicos, ” M.S. thesis, UFABC, Santo André, SP,
Brazil, 2015.
[29]
D. R. David, “Model predictive control with integral action: a simple
mpc algorithm, ” Modeling, Identification and Control, vol. 34, pp.
119– 129, 2013.
[30]
I. Dogan., Microcontroller Based Applied Digital Control. John Wiley
& Sons, Ltd., 2006.
[31]
“IEEE standard test procedure for polyphase induction motors and
generators, ” IEEE Std 112-2017(Revision of IEEE Std 112-2004), pp.
1-115, Feb. 2018.
[32]
1218 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 6, NO. 5, SEPTEMBER 2019J. S. Solís-Chaves received the B.S. and M.S. degrees in electrical engineer in 2006 and industry
automation in 2009 from National University of
Colombia, respectively. He received the Ph.D. degree in energy engineering from Federal University
of ABC-UFABC in 2017. He is a full-time Professor,
teaching in the areas of electrical machines, renewable energies, industry automation, and power electronics. His research interests has been focused on
the control of doubly-fed induction generators for
wind energy systems, smart grids, and GIS software for renewable energy applications using the sustainable development perspective and applying the following political and ethical principles: The Energy Democratization, the Society Decarbonization and the Technological Innovation for the benefit of the
poorest people.
Lucas L. Rodrigues received the bachelor’s degree
in electrical engineering from Federal University of
Bahia, in Salvador, Brazil, in 2017. In 2019, he received the Master degree in renewable energy and
predictive control from Federal University of ABCUFABC, in Santo André, Brazil. Nowadays he is
working at his Ph.D. degree in the same area of
knowledge. His research interests are power electronics, predictive control, renewable energy, doubly-fed
induction generators, and Meta-heuristic optimization.

Babichenko Anatolii,
NTU «KhPI», Department of TSA and EM,
Kharkiv, Ukraine
Kravchenko Yana,
NTU «KhPI», Department of TSA and EM,
Kharkiv, Ukraine
Babichenko Juliya,
UkrSURT, Department of Heat engineering and heat engines,
Kharkiv, Ukraine,
Velma Volodymyr,
NUPh, Department of Pharmaceutical Preparation Technologies,
Kharkiv, Ukraine
German Eduard,
NTU «KhPI», Department of TSA and EM,
Kharkiv, Ukraine
Е-mail: kravchenko_y_o@ukr.net
ASSESSMENT OF THE FORECAST OF THE OPERATING MODE
OF THE COMPLEX OF AMMONIA SECONDARY CONDENSATION
PRODUCTION IN UNCERTAINTY CONDITIONS
Abstract. The article describes the analysis of conditions of secondary condensation complex
functioning. There, the software algorithm for estimating the forecasts of functioning mode
is developed, which makes it possible to prepare the complex operator in conditions of existing
uncertainty for making decisions in supervisor control mode.
Keywords: secondary condensation, ammonia production, forecast evaluation, uncertainty conditions.
One of the most crucial stages of separation of
ammonia synthesis of AM-1360 series units operating in Ukraine is secondary condensation, which
performs the final removal of commercial ammonia
from the synthesis cycle. Previous studies have improved the energy efficiency of the secondary condensation stage by creating an optimal structure that
ensures the exclusion of electrically driven turbocompressor refrigeration unit from the operational
scheme and reducing the thermal load on low-temperature evaporators (LTE) [1; 2]. Technological
complex of this structure is formed by additional
heat exchanger (AH), condensing column (CС),
high-temperature evaporator (HTE) with steam-jet
refrigerating unit (SJRU) and two LTEs with waterammonia refrigerating units (WACU).
Functioning of the secondary condensation
complex is done under conditions of uncertainty
[3], which is mainly due to constant changes in
the external heat load due to the application of aircooled circulating gas (СG) units at the previous
stage of primary condensation and the lack of posSection 3. Technical sciences
32
sibility of continuous automatic control of ammonia
concentration in CG [4]. At the same time, the range
of changes at the inlet of the complex temperature
of the CG and ammonia concentration in the CG is
35 ÷ 45 °C and 8.6 ÷ 12% respectively. Under such
circumstances, the CG temperature at the HTE inlet,
which is included in the SJRU operational scheme,
will also change. Therefore, it is necessary to apply
a control system to stabilize the CG temperature at
the СС inlet at 30 °C, and therefore stabilize the CG
temperature at the HTE inlet at 9.2 °C, which was
provided by the functional scheme of secondary
condensation process control [5].
However, the process equipment of the complex
of secondary condensation is characterized by significant inertia, which is due to its large metal intensity.
Only the mass of CC is about 290 tons [6]. At the
same time, SJRU is a rather complicated technological system in which change of refrigerating capacity
and, consequently, expenses of the coolant to HTE
are in general performed at the expense of changing
the number of working ejectors [7]. Therefore, for the
purpose of increasing control reliability and possibility of preparation of the operator for such changes in
the supervisor control mode, which is provided by the
functional control scheme, a subsystem of decision
making support is necessary. It should provide assessment of forecasts of possible changes in CG temperature at the inlet of HTE. Setting of this temperature
will condition the determination of forecast of necessary SJRU refrigerating capacity and, consequently,
of coolant costs to HTE and monoethanolamine
(MEA) solution to SJRU steam generator, which
provides necessary supply of working steam/vapor
for ejection of the coolant from HTE.
In the process of algorithm development there
have been used equations of mathematical description of AH heat exchange, HTE evaporator, subroutine of calculation of heat transfer coefficients, heat
transfer and ammonia concentration in AH at the
inlet and outlet of secondary condensation complex
a
NH
IN
3
and aNH P
3 according to the developed algorithms
stated in [4; 8] and located in STAB and STOCH
files. The algorithm contains convergence cycles,
which provide alignment of heat flows from the side
of the pipe and inter-pipe space and in the process of
heat exchange AH to determine the temperature of
CG at the outlet of its inter-pipe space. Then according to this temperature the necessary cooling capacity of SJRUs is sequentially determined, which will
condition the stabilization of AH temperature at the
outlet of HTEs at the level of 30 °C, coolant consumption in HTEs for its provision, consumption of
working ammonia vapor for ejection and consumption of IEA solution for obtaining this vapor in SJRUs
steam generator. At that the algorithm contains the
following main functional blocks.
Block 1. Call of the task to be solved in a certain
period of time or by operator’s command.
Block 2. Open the PROG file that serves this task.
Block 3. A sub-program for reading the necessary
DANI file information, which receives and stores
the current information about inlet and outlet variables and structural characteristics of the object, obtained from the information management complex
TDC-3000.
Block 4. STAB data reading sub-program for determining heat transfer coefficients of actual K ЕA
and calculated according to formulas accepted at
design K DA .
Block 5. STOCH sub-program on fulfillment of
stationary conditions, process reproducibility and
hypothesis on normality of empirical distribution
and determination of functional dependencies for
numerical estimation of volume concentrations of
ammonia in CG at inlet a
NH
IN
3
and outlet a
NH
P
3
of secondary condensation complex using MATLAB (Optimization Toolbox) package, which according to
research [4] should be calculated using equations:
a f NH IN PPC PC
3
= ( ) , ; �Θ (1)
a f ТР NH V V NHM ICG P P LTE a P NH IN CG
3 3
= ( ) , , � � Θ , ,� , (2)
where P
PC , PCG are primary condensation pressure
and CG at the AH inlet, MPa; ΘPC , ΘP LTE are primaryASSESSMENT OF THE FORECAST OF THE OPERATING MODE OF THE COMPLEX OF AMMONIA SECONDARY CONDENSATION PRODUCTION IN UNCERTAINTY CONDITIONS
33
condensation temperature and CG respectively at
the outlet of LTE, °C;VNHM,VIP CG are volume flow rate
of nitrogen-hydrogen mixture (NHM) and CG at
the inlet of AH, nm3/s.
Block 6. Determination of condensed ammonia
flow rate M
CD
AH in CG flow of inter-pipe space AH and
total thermal resistance R
T
EA is done using the following equations:
M V a a
CD
AH
IP
CG
NH
IN v
NH
OUT v
= − 0. ; 771� ( ) ( ) 3 3 ( ) (3)
a
P
NH P
IN v NH
IN
3 CG
( ) 3
= ; (4)
a
P
NH P
OUT v NH
OUT
3 CG
( ) 3
= ; (5)
R
T K
EA
AЕ
AP
IP
= − + A


1 1 1
α α
, (6)
where a
NH
IN v
3
( )
, aNH
OUT v
3
( ) are, respectively, the concentration of ammonia vapour in the CG inlet and outlet
of AH, volume ratio; PNH IN
3
, PNH OUT
3
is the partial pressure
of ammonia vapor in CG correspondingly at inlet
and outlet of inter-pipe space of AH, MPa; αPA , αIP A –
are the coefficients of heat transfer correspondingly
from the side of pipe and inter-pipe space, calculated
by Krausold equations, W/(m2 · K) [1].
Block 7. Subprogram of calculating the functional
dependence on the numerical estimate of uncertainty
R
Т
EA using the MATLAB (Optimization Toolbox) software, which should be searched by equation [1]:
R f ТEA = ( ) MCD AH (7)
Block 8. Flow rate of CG M
AР
(kg/s) at CC outlet
is determined by the following formulas:
M
V a a
a
CD
CC IP
A
NH
IN
NH
P
NH
= P
−( ) −
3 3
3
0 771
100
� .
; (8)
G V
a
a
EV
S
NHM
NH
P
NH
= P
−
3
3
0 771
100
� .
; (9)
M M G
V
CC
CD
CC
EV
S
= − ; (10)
M M M
AP
IP
A
V
CC
= − , (11)
where M
CD
CC
, GEV S , MVCC is the flow rate of ammonia,
respectively, condensed into CC, obtained by evaporation during the heat exchange with NHM and
liquid ammonia from CC, kg/s; VIPA, VNHM are, respectively, the volume flow rate of CG at the AH
inlet and nitrogen-hydrogen mixture at the CC inlet, nm3/s.
Block 9. Setting the initial temperature approximation Θ Θ ∆Θ
P
CG
P
CC
= + at the outlet of the AE pipe
space with determination of the heat flow ФPA from
the AE pipe space by the following formula:
Ф
AP
P
AG
P
AG
P
CG
P
CC
L
AIN
L
AIN
V
= − M C� ( ) Θ Θ + − M i ( ) i AOUT ,(12)
where M
P
AG
, M LAIN are respectively the mass flow rate of
CG gas mixture at the outlet and liquid ammonia at the
outlet of the AE pipe space, kg/s; C� ТP AG is the average
heat capacity of CG gas mixture, kJ/(kg·K); iLAIN, iVAOUT
are respectively the enthalpy of liquid ammonia at the
outlet and ammonia vapor at the outlet of the AE pipe
space, kJ/kg; ΘCC P =17.5 °C is the CG temperature at
inlet of AE pipe space, which is provided by CG temperature at CC inlet at the following level Θ2HT CGE = 30 °C
and CG temperature at LTE outlet at the following
level Θ
P
LTE
= −5 °C; ∆Θ – approximation step, °C.
Block 10. Setting the initial temperature approximation Θ Θ ∆Θ
1CG
HTE
IP
CG
= − at the outlet of the AE inter-pipe space with determination of the amount of
condensed ammonia M
CD
A according to equations
(3–5) and heat flow ФIP A from the AE inter-pipe space
according to the following formula:
Ф
IP
A
IP
AG
IP
ACG
IE
CG
CG
HTE
CD
A
IN
= − M C� � ( ) Θ Θ1 + + M r A
+ − ( ) M M LA 0 5 . � � CD A CLA ( ) Θ Θ CG IP − 1HT CGE � (13)
where M
IP
AG
, MCD A , M LA – are respectively the amount
of gas mixture at the outlet of the AE inter-pipe
space, condensed and liquid ammonia space in the
AE inter-pipe space, kg/s; C� IP ACG, C� LA are respectively the average heat capacity of gas mixture of AE
inter-pipe space and liquid ammonia, kJ/(kg·K);�rIP A
is specific heat of condensation in the AE inter-pipe
space, kJ/kg; ΘCG IP is the temperature of CG at the
inlet of the AE inter-pipe space, °C.
Block 11. Estimation of error margin of convergence condition δ1A of heat flows ФPA and ФIP A and
transition in case of its fulfillment to calculation of
heat flow Ф
AT
due to heat exchange according to the
following formulas:Section 3. Technical sciences
34
Ф
AT
AE
A MN
= K F�� ∆ΘA ; (14)
K
R
AE
AP
IP
A T
EA
=
+ +
1
1 1
α α
, (15)
where F
A�= 1150 is the surface of heat exchange of
AE, m2; ∆ΘMN A is the log mean temperature difference of AE, °C.
Block 12. Estimation of error margin of convergence condition δ2A of heat flows ФPA, ФIP A and ФTA
and in case of its fulfillment, given determined temperature�Θ1HT CGE transition to the calculation of M IN HTE
(kg/s) at the inlet of HTE inter-pipe space according
to the equation:
M
M C M r M M
IN
HTE P
HTE
P
CG
CG
HTE
CG
HTE
CD
HTE
CD L
HTE
C
=
� � ( ) Θ Θ 1 2 − + + −0 5 . � HT D E LHTE HT CGE HT CGE
CL CL CL
C
r
( ) ( ) −
− − ( )
�
�
Θ
Θ
Θ
Θ
1 2
1 2
, (16)
where M
P
HTE
, MCD HTE, M LHTE is the flow rate of gas mixture, condensed ammonia and liquid ammonia respectively in the pipe space of the HTE, kg/s; C� PCG,
C�
L
HTE are the average heat capacity of gas mixture and
liquid ammonia respectively in the pipe space of the
HTE, kJ/(kg·K); rCD , rCL is heat of ammonia condensation and vaporization in pipe and inter-pipe space
of HTE, kJ/kg; ΘCL1 is coolant (ammonia) temperature at HTE inlet, °C; ΘCL2 = 24 is the coolant boiling
point in inter-pipe space of HTE, °C.
Block 13. Calculation of the flow rate of working
ammonia vapor MV to SJRU ejectors, MEA solution
flow M
МЕА to produce this vapor, as well as the total
amount of coolant vapor and MTTL working vapor to
air condensers according to the following formulas:
M M r
МЕА C
V V
МЕА МЕА МЕА
=
�( ) Θ Θ �1 2 − ; (17)
M M
u
V
IN
HTE
= ; (18)
M M M
TTL = + V МЕА, (19)
where r
V is the specific heat of ammonia vaporization
at 65 °C and pressure of 3 MPa, kJ/kg;C� МЕА is the
specific heat capacity of MEA solution, kJ/(kg·K);
Θ
МЕА1 = 85 °C, ΘМЕА2 = 75 °C is the temperature of
MEA solution at inlet and outlet respectively; u = 0 4 .
– ejection coefficient [7].
Block 14. Formation of PSPR current data array
of the decision-making subsystem, in particular MV,
M
МЕА, MTTL and formation of results.
Block 15. Closing the PROG file and task termination.
Table 1 shows, as an example, separate results of
calculations according to the given algorithm, implemented in the MATLAB R2014a software.
Table 1.– Separate results of assessment of forecasts of the subsystem
of support for management decision making in the conditions of
changes in CG temperature at the secondary condensation complex
inlet with ammonia concentration in CG at the inlet of 10.1% vol
Performance data
CG temperature at the secondary condensation
complex inlet��ΘCG IP , °C
37 41 45
1 2 3 4
Additional heat exchanger (AH)
Heat flow Ф
AT
, MW 3.31 4.25 5.19
Heat transfer coefficient�K
AE
, W/(m²·K) 304.86 343.83 376.04
Temperature at the outlet of the inter-pipe space
Θ
1CG
HTE, °C 30.40 32.76 35.17
Temperature at the outlet of the pipe space ΘCG P , °C 30.32 33.79 37.31ASSESSMENT OF THE FORECAST OF THE OPERATING MODE OF THE COMPLEX OF AMMONIA SECONDARY CONDENSATION PRODUCTION IN UNCERTAINTY CONDITIONS
35
1 2 3 4
Amount of condensed ammonia in the inter-pipe
space MCD A , t/h 5.33 7.15 9.14
Log mean temperature difference ∆ΘMN A , °C 9.45 10.74 11.99
High Temperature Evaporator and SJRU
Heat flow Ф
HTE , MW 0.19 1.36 2.57
Amount of condensed ammonia in the pipe space
M
CD
HTE , t/h 0.30 2.11 4.07
Coolant quantity at the inlet of the inter-pipe space
M
IN
HTE , t/h 0.64 4.44 8.38
Quantity of working ammonia vapor to SJRU ejectors�M
V, t/h 1.59 11.09 20.96
MEA solution quantity for working vapor M МЕА, t/h 41.06 285.84 540.29
The developed algorithmically-programmed
software of the decision support subsystem allows
to predict the modes of its functioning for such a
complex inertial object with high metal intensity
as a complex of secondary condensation. It gives
an opportunity to prepare the complex operator
in conditions of existing uncertainty for decisionmaking in the supervisor control mode in order to
stabilize the temperature conditions of circulating
gas cooling.
References:
1. Babichenko A., Velma V., Babichenko J., Kravchenko Y., Krasnikov I. System analysis of the secondary condensation unit in the context of improving energy efficiency of ammonia production. Eastern–
European Journal of Enterprise technologies. 2017. – Vol. 2/6(86). – P. 18–26. Doi 10.15187/1729–
4061.2017.96464

FUZZY INVERSE MODEL CONTROL (InvMC)
OF AN UNDERWATER VEHICLE
Andrzej Piegat, Marcin Pluciñski
Technical University of Szczecin, Institute of Computer Science and Information Systems
ul. ¯ o³nierska 49, PL-71210 Szczecin, Poland, phone: +4891-4876485, fax:+4891-4876439
email: andrzej_piegat@ii.tuniv.szczecin.pl, marcin_plucinski@ii.tuniv.szczecin.pl
ABSTRACT: The IMC-structure, often used in fuzzy and neural control, requires both a model of the plant and its
inversion. The paper presents how a more parsimonioues structure using only the model inversion (InvMC) can be
designed and how the inversion can be found. The InvMC-structure was applied for control of the underwater vehicle
(UV) Krab II.
1. INTRODUCTION
For control of nonlinear systems the IMC-structure, described eg. in [4] and shown on Fig. 1a, is often used.
✂✁ ✄ ☎ ✆✞✝ ✁ ✟✡✠✡✆✞✝☞☛✂✁ ✌✍✟ ✎✑✏✒✄ ✓✞✟✡☎
ψ✔ ✕ ψ✖ ψ✗ ✘ ✔ ψ
✕
✙ ✚
✆ ✖ ✛ ψ ✜ ψ✚ ✢ ✌✡✎✣✆✤✄
✥
✥
✦ ✧ ✙ ★ ✩ ✪ ✙
✚
✏✣✝✫✆ ✜✭✬ ✌✍✟✣☎ ✝✫✌✡✄ ✄ ✆✞✝ ✁ ✟✡✠✡✆✞✝☞☛✂✁ ✌✍✟ ✎✑✏✒✄ ✓✞✟✡☎
ψ✔ ✕ ✆ ✆ ✖ ✘ ✔ ψ
✆ ✖ ✛ ψ ✜ ψ✚ ✢ ✌✡✎✣✆✤✄
✥
✦ ✮ ✙ ★ ✩ ✪ ✙
✚
✯✱✰ ✲✴✳
a) b) (A-WP: anti-windup)
Figure 1. The IMC (a) and the InvMC (b) structure.
Its advantage is internal stability of the whole system when both the plant and the IMC-controller (separately) are
stable. It refers also to nonlinear plants and controllers as fuzzy and neural ones [3]. The IMC-structure enables to
approach the so called "perfect" control [4]. The same can also be shown for the InvMC-structure, which is simpler. The
common advantage of both structures is a small number of parameters to be tuned. In the IMC it is usually only the
filter time constant and in the InvMC the pre-controller gain. In the PID-control 3 gains must be tuned, what
considerably complicates the synthesis. In the literature not many application examples of the pure InvMC-structure can
be found. Usually the model inversion is combined with the model [2,3,5,8] or with other controller types as eg. with a
predictive [1] or PD-controller [9]. Below, an algorithm (shortly, because of the paper limitation) for the InvMC-system
will be presented. It is described precisely in the book "Fuzzy Modeling and Control" by Piegat being prepared for
printing (in polish).
2. DESIGN ALGORITHM OF InvMC-SYSTEMS
Designing of InvMC-systems consists of two main steps:
I. design of the inversion F*inv of the controlled plant F,
II. structure choosing and tuning of the pre-controller.
Let’s assume the plant realizes the nonlinear input/output mapping (1).
ψ(✵ ) = ✶ [✷ ✺ (✵ − ✸ + ✹ )✼ ✽ ✼ ✷ ✺ (✵ − ✸ )✼ ψ(✵ − ✻ )✼ ✽ ψ(✵ − ✸ )] (1)
The ideal inversion F*inv would be the one realizing dynamic requirements determined by the reference model ✾ ✿❃ ❄❆❅ ❀❂❁
= 1, Fig. 2. Because in real dynamic systems it is impossible, as real reference models can be assumed eg. inertia ones
as given by (2).( ) ( )
( )
( )
 ✁
✁
✂ ✁
✄
✁ ✄
☎ ✆✞✝
✟ ✠☛✡ ☎ ✆✞✝
✠ ☞
= =
− −


−
ψ
✌
✍
(2)
✎ ✏✒✑ ✂✔✓✖✕ ✎ ✗✘✏ ✙✛✚✢✜✣✏✒✤
✥ ✦ ✧✩★✫✪ ✬ ✭ ✮✰✯✘✱ ψ✧✩★✫✪ ε→✲
✳ ✴
ψ✵ ✶✩✷ ✧✩★✫✪
✸ ✹ ✺✩✻
✼
 ✽ ✾❀✿
✸
❁ ❂✢❃
Figure 2. Inversion design in the system with reference model  ☎ ✆❄✝
✟ ✠☛✡
.
For c=0 one gets "fast" inversion, for 0<c<1 "slow" one. The "fast" inversion has usually strong differential action
and generates great control signals M0, often greater than saturations of the setting unit. With 0<c<1 we can decrease
and slow down the control signal and increase the system robustness. However, the same we can get with an
appropriately slow pre-controller. Therefore, below will be used "fast" inversion (c = 0) described by (3).
( ) ( )
( )
( )
( ) [ ( )]
 ✁
✁
✂ ✁
✁
✯ ✂ ✯ ✏ ❅
☎ ✆✞✝
✟ ✠☛✡ ☎ ✆✞✝ ✠ ☞
☎ ✆✞✝
= =
= − −
ψ − −
ψ
✌
✌
(3)
If the inversion is accurately tuned, Fig. 2, then ε(k) ≅ 0, what implicates (4).
ψ(✯ ) ≅ ✂ ✌ [✯ − (✏ − ❅ )]❆ ❇ ❆ ψ(✯ + (✏ − ❅ )) ≅ ✂ ✌ [✯ ] (4)
Replacing in (1), according to (4), the variables ψ(k+i) by e1[k-(n-m)+i], i=1÷(n-m) and then inverting it we get
inversion(5).
❈ ❍ (❉ ) = ❊ ✟ ✠☛✡ ❏ [❈ ❍ (❉ − ■ )❑ ▲ ❑ ❈ ❍ (❉ − ❋ )❑ ● ✌ (❉ )❑ ▲ ❑ ● ✌ (❉ − ❋ )] (5)
Condition of its high accuracy is a good chosen structure (rule base, membership functions, etc.) and good tuning,
what can be made eg. in the system with reference model, Fig. 2, where the plant F is replaced by its model F* (error
backpropagation). Also other possibilities for the tuning given in [3] can be used. When the inversion is accurate, we
can replace on Fig. 1b the connection F*inv - F by the inversion reference model ▼ ◆◗ ❘☛❙ ❖❄P , Fig. 3.
❚ ❯ ❱ ❲
❳❩❨✩❬☛❭❀❪✖❫✣❴❩❵ ❨❛❫❝❜ ❜ ❬❞❨
ψ❡ ❬ ❬ ❢ ψ ε→❣
❤
❚ ❯ ❱ ❲
✐ ❥✰❦
❚ ❧
❤♠
Figure 3. System with reference model enabling design of the pre-controller Gc.
This way the pre-controller design can be made basing on linear control systems methods. For many plants of the
type n-m=1 good control quality is achieved with integral pre-controller given in discrete version by (6).
( ) ( )
( )
♥ ♦ ♣ ♦
♣ ♦
q
♦
r
r
= =
−
ts
✉
(6)
To increase the control quality, application of anti-windup (Fig. 1b) is recommended. The gain kc is the only
parameter tuned in the system. Its value can be pre-calculated basing on the reference model Gref, (which should not be
mistaken for the inversion reference model ▼ ◆ ❖❄P
◗ ❘☛❙
), Fig. 3. It can be assumed as inertia of the order (n-m+1). Becauseboth the plant and the inversion have nonlinearities and the setting unit has saturations, the gain kc is finally tuned
experimentally in the system with the real plant F or its model F*.
3. EXPERIMENT
The InvMC was applied for UV Krab II. Scheme of the controlled plant F is shown on Fig. 4.
a)
✂✁☎✄✆✄✞✝✂✠✟ ✡ ✁☎☛ ☞✌✄✞✁☎☞✍✝✏✎ ✎ ✝✑✄✓✒ ✔✖✕
✗ ✘ ✔ ✗ ψ
✙
✚ ✛
✜✣✢✥✤ ✦ ✧ ★✪✩✬✫
✭
✮✰✯ ✱✳✲✵✴
✶✞✷✬✷✠✸ ✹✬✺ ✶✞✷✑✸ ✷✞✻
✷✑✸ ✷✞✻ ✷✬✷✠✸ ✹✂✺
✶✓✷✼✹☎✸ ✻
✽✾✯ ✿❀✴
✻❁✹✬✸ ❂❃✷
Figure 4. Controlled plant with partial correction of the propellers nonlinearities, stabilizing feedback ks (a) and
propellers static characteristic (b).
The nonlinear dynamics of the UV can be approximated by transfer function (6),
( ) ( )
( ) ( )
❄ ❅
❅
❆ ❅
❇
❅ ❅✑❈
❉
❉
❉
= =
⋅ +
ψ
❊
(6)
where: ψ - course angle [deg], M - propeller torque [Nm].
The transfer function coefficients depend on the UV velocity. Experiments realized by Pluciñski [7] have shown,
that the nominal function has coefficients k
p = 0.04 [deg.s/Nm] and Tp = 1[s]. Fuzzy inversion F*inv of the plant F from
Fig. 4 was determined by inverting of its fuzzy model F* (what can not be shown because of the paper limitation) and
next tuning it in the system with the reference model, Fig. 2, at ❋ ● ❍✼■
❏ ❑✬▲
(z) = z -1. Membership functions for inputs and
outputs are shown on Fig. 5.
▼ µ◆ ψ❖ ◆ P✂◗✓❘✓❙ ❙❯❚
µ◆ ψ❖ ◆ P✂◗ ❱✂❙ ❙
❘
◗✓❘✓❲ ❳ ❨ ❘✓❲ ❳ ❨ ψ❖ ◆ P✂◗✓❘✓❙ ❩ ψ❖ ◆ P✂◗ ❱✂❙
▼✳❬❭▼❫❪❴▼ ❱ ▼ ❘ µ◆ ψ❖ ◆ P✂❙ ❙ ❚❵❘❛❚❜❱❝❚ ❪ ❚ ❬
❞ ❡❣❢❤❣✐❥❞ ❤✓❢❡ ❦❧❞ ♠✆❢❡✪✐♥❞ ♦✞❢ ♣ ❦ ❤✓❢❡✪q❥❡❣❢❤❣❤r♣✪♣ ❢❤❣❤✵♣ ✐❣❢ ♠✪s
ψt ✉ ✈✂✇②①✪③✓④ ⑤ × ⑥✏⑦✠⑧✏⑨
⑩❷❶ ⑩✳❸ µ✉ ❹ ❺ ✉ ✈✂✇ ✇ ❻✌❼ ❻❁❽
⑩❫❾❿⑩➁➀❧⑩ ❽ ⑩❫➂❴⑩ ❼ ⑩✳➃➄⑩ ③ ❻❵③➅❻ ➃ ❻ ❸ ❻ ➂ ❻ ❶ ❻ ➀ ❻ ❾
❞ ✐ ❦✞❢❡ ♦➆❞ ❦✆✐❣❢♦✪♦❁❞ ♣ s✆❢❦❁❞ ♣ ✐❣❢♦✪♦❁❞ s✆❢❦✼♣ ♣ ✐❣❢ ♠ q➇❦✆✐❣❢➈✪❤❀❦❣s✆❢✐ ❦❁✐✪✐❣❢ ♠ ♦➉♦✆✐❣❢➈✆♠
❞ ❦❣s✆❢ ❢ ♣ ❡♥❞ ❦✪❦✞❢❡✆♠➇❞ ♣ ❦✞❢❡✪q➊❞ ✐❣❢♦❣♠ ♣ s✆❢✐ ❦❥❦✆✐❣❢ ♠ q➉✐✪✐❣❢➈✪q♥✐✆s✆❢♦
❹ ❺ ✉ ✈✂✇
Figure 5. Membership functions of the inputs e1(k), e1(k-1), e1(k-2) and of the output M0(k) of fuzzy inversion F*inv.
b)Membership functions in the inversion are based on extrapolation truth described by Piegat in [6]. Owing to this the
inversion output M0 doesn’t saturate and the UV can be rotated at arbitrarily big angle ψ. The inversion rule base is
given in Tab. 1 and 2.
Rule base: rules R1-R16:
IF [e1(k-2) = N] AND [e1(k-1) = ...] AND [e1(k) = ...] THEN [ M0(k) = ... ],
where AND: product operator.
Tab.1.Rules R1-R16 of the model inversion F*inv.
ψ(k)
ψ(k-1) N4 N3 N2 N1 P1 P2 P3 P4
N N6 N5 N3 N2 P4 P5 P7 P8
P N9 N8 N6 N5 P1 P2 P4 P5
Rules R17-R32:
IF [e1(k-2) = P] AND [e1(k-1) = ...] AND [e1(k) = ...] THEN [ M0(k) = ... ],
Tab.2.Rules R17-R32 of the model inversion F*inv.
ψ(k)
ψ(k-1) N4 N3 N2 N1 P1 P2 P3 P4
N N5 N4 N2 N1 P5 P6 P8 P9
P N8 N7 N5 N4 P2 P3 P5 P6
Analysis of the rule base shows its monotonicity. In the InvMC-system a pre-controller according to (6) was applied.
Its gain kc was determined experimentally to get possibly short setting time at small overshoot and quick disturbance
rejection. The "optimal" value is kc= 3. Bigger gains give not shorter control time with much stronger oscillations.
Fig. 6a and 6b present results of experiments realized with the described control system. The given course was set
into -20 [deg] and 20 [deg]. Fig. 7a and 7b present the reaction of the system on the disturbance acting on the input of
the UV.
ψ  ✁✄✂✆☎✞✝
✟✡✠ 50 100 150 200
-20
-10
0
10
20
Time (second)
☛  ☞✍✌✎✝
✏ ✠ 50 100 150 200
-20
-10
0
10
20
30
Time (second)
Figure 6. UV course angle (a) and the propeller torque (b) for the fuzzy InvMC-system.
It was found, that for the gain taken for experiments (kc = 3), the system work stable in a broad interval of UV
transfer function coefficients variation:
0.015 ≤ kP ≤ 0.063, 0.5 ≤ TP ≤ 11ψ  ✁✄✂✆☎✞✝
✟✡✠ 0 20 40 60 80 100
-0.1
-0.05
0
0.05
0.1
0.15
0.2
Time (second)
☛  ☞✍✌✎✝✑✏ ✒✔✓✄✕ ✖ ✁ ✕ ✖ ✗✘✂ ✠✚✙ ✁✛ ☞✍✌✎✝✑✏ ✁ ✟ ✒✢✜✘✂✣✁ ✕ ✖ ✗✘✂ ✠
✤ ✠ 0 20 40 60 80 100
-10
-5
5 0
10
Time (second)
Figure 7. UV course angle (a), the disturbance and the UV propeller torque (b) for the fuzzy InvMC-system - reaction on
the disturbance d = ± 10 [Nm].
4.CONCLUSIONS
The fuzzy InvMC-system proved its efficiency in practice. The most important advantages of the described system are
the easiness of its tuning and the possibility of stability proving.
During simulations which results are presented on Fig. 6 and 7, UV propellers worked for a long time with
maximum values and without big number of overswitchings This facts causes that the system works with short control
time. Fig. 7a and 7b show that the system has good possibility of compensating disturbances acting on the input of the
UV.
REFERENCES
[1] R. Babuska, J. Sousa, H.B. Verbruggen, Model-based design of fuzzy control systems. Proceedings of EUFIT’95,
August 28-31, Aachen, Germany, 1995.
[2] M. Brown, C. Harris, Neurofuzzy adaptive modelling and control, Prentice Hall, 1994.
[3] K.J. Hunt and others, Neural networks for control systems - a survey. Automatica, vol. 28, pp.1083-1112, 1992.
[4] M. Morari, E. Zafiriou, Robust process control, Prentice Hall, 1989.
[5] D. Neumerkel, F. Lohnert, Artificial neural networks: state of the art in automation, Automatisierungstechnische
Praxis, No. 11, pp.640-645, (in german), 1992.
[6] A. Piegat, Extrapolation truth, Proceedings of EUFIT’97, September 8-11, Aachen, Germany, pp.324-329, 1997
[7] M. Pluciñski, Adaptive course-angle control system of unmanned underwater vehicle with fuzzy knowledge base
about the plant, Dissertation, Technical University of Szczecin, Poland, (in polish), 1996.
[8] J. Sousa, R. Babuska, H.B. Verbruggen, Adaptive fuzzy model-based control, Proceedings of EUFIT’95, August 28-
31, Aachen, Germany, pp.865-869.
[9] D. Wloka and others, Neural networks in robotics, Automatisierungstechnische Praxis, No. 5, pp.296-305, 1993.

Performance Analysis of Hybrid Fuzzy-PID
Controller Action on Boiler Drum Level Control
Abstract— Boiler unit plays a vital role in power plant and
controlling the boiler drum level is one of the critical operations.
Nowadays, Fuzzy Logic controller or Conventional PID
controller is designed in MATLAB simulation and further
prototype is implemented using Embedded Technology, hybrid
platform is required in power plant. To overcome this and to get
better solution intelligent controller (fuzzy logic) is used to tune
the conventional PID controller automatically in online process.
Hybrid Fuzzy-PID controller result proves that, it offer better
performance (in terms of settling time, rising time, steady state
error) than conventional PID controller in boiler drum level
control. The simulation results are achieved by using LabVIEW
Fuzzy-PID Tool.
Keywords— Boiler Drum Level, Soft Computing, Hybrid FuzzyPID Controller.
I. INTRODUCTION
In power plants boiler drum is a closed vessel in which
steam is produced by burning coal in the furnace. The main
input variable of the boiler drum are fuel, feed water and air.
The outputs of the boiler system are electrical power, steam
pressure, steam temperature and flue gas as shown in (fig.1.)
According to these parameters the drum level may increase or
decrease. The boiler drum level should be monitored
continuously. It should be within a certain limited value. If
the measurement of boiler drum level is not within the limited
value, water carryover may be occurred. If the level is low in
drum overheating occur in boiler water tube this cause the
damages in boiler drum surface and creates the cracks in the
surface. If the value of level in boiler drums more than the
limited value it may transfer the moisture into the turbine,
which reduce the boiler efficiency.
Fig. 1. Basic Block Diagram of Drum Level Control
P.Meenu [1] is described the boiler drum level
control which is done by fuzzy logic controller. If the water
level is too low in boiler drum it leads to boiler explosion. If
the water level is very in boiler drum it cause damage to
turbine due to the affect of separator steam water. Serious
consequences occur in high or low level so we must take strict
control. Conventional Proportional integral derivative
controller is popular three element control. If there is any
process disturbance the 3-element control does perform well
due to the lack of proportional controller gain knowledge .The
collected data from the PID controller is used to gain the
knowledge on the intelligent control technique and developed
fuzzy logic control.
Wang Zhuo [2] is illustrated the water in the boiler drum
has a nonlinear characteristics, large time-varying, strongcoupling and multivariable. Using mathematical model of
boiler drum, fuzzy control theory is designed for controlling
the boiler drum level system. Both fuzzy logic and
conventional logic are used in controlling the drum level. In
MATLAB software verification and simulation are done and
get the result of drum level system. By observing that static
and dynamic characteristics of fuzzy controller is improved
and also best real-time control of boiler drum level is
achieved.
K.Ghousiya Begum [3] explained the boiler drum level
controlling using an intelligent model. In three element boiler
drum the parameters are determined using PID tuning methods
like Ziegler-Nicholas method, Tyreus-Luyben method and
Internal Model Control in which IMC used to tune the PID
controller .
Keyur Solkanki [4] describes the approach for controlling
a very crucial parameter of boiler level of the boiler drum
using PID controller. IMC based PID tuning method is used
with feedwater and feedback strategy is used to control two
element drum level. Besides also the modeling of the process
for level control and implemented it in simulink. Hardware
model has also been developed and proved open loop
validation for theoretically derived model and practical model
and simulation results.
K.Padma priya [5] described the boiler drum level control
using LabVIEW software. The boiler parameters are measured
and controlled using embedded system through LabVIEW.
Water droplet identifiers are used to check the steam dry or
wet. Dry steam is required for power production, if wet steam
passed to turbine it will damage and production is affect.
Water droplets identifiers detect the wet steam and converted
into dry steam.
Gowthaman E1 Prasanna Moorthy V2 Saravanan S Naveenbalu K Aravind S Naveen S3
1Assistant professor, EIE Dept., Hindustan College of Engg.& Tech, gowthameie@gmail.com.
2Assistant Professor (Sr.Grade), Electrical Engineering, Government College of Technology, prasanna@gct.ac.in.
3UG Scholar, EIE Dept., Hindustan College of Engg. & Tech, Coimbatore.
978-1-5090-4556-3/16/$31.00 ©2016 IEEE
2016 Online International Conference on Green Engineering and Technologies (IC-GET)Shaoyuan li [6] presented the paper about the new
development of the boiler-turbine coordinated control strategy
using fuzzy reasoning and auto tuning techniques. Boilerturbine system is a very complex process due to multivariable,
nonlinear, slowly time-varying plant with large settling time
and a lot of uncertainties. The main stream pressure control
loop and power output are strongly coupled in boiler turbine
unit. The automatic coordinated control of two loops is very
challenging problem. The Gaussian partition a special subclass
of fuzzy inference is used to self-tune the main steam pressure
PID controllers.
Yu Daren [7] analyzed the work about possibilities of
applying feedback linearization techniques to the non linear
control of super heat steam pressure and power output of
boiler-turbine generating unit. Computer simulation is used to
design and evaluated the nonlinear coordinated controller. The
simulation results are helps to compare the proposed strategy
with conventional strategy. Improvements of nonlinear control
system are observed by the results.
Zaiyi Liao[8] describes the optimize control of the boiler
in multi-zone heating system by an inferential model-based
predictive control scheme to save energy and to improve
thermal comfort. It has only three inputs they are outside air
temperature, total solar radiation falling on the exterior
building and temperature of water in the boiler drum. Inputoutput data are collected from the portable temperature
loggers to estimates the parameter for the modeling. The
simulation results show that overall performance of the
heating system is improved compared to the conventional
boiler control scheme.
Gowthaman E[9] designed self tuned PID controller for
controlling speed of PMDC motor with 49 fuzzy rules. Fuzzy
rules are framed based on two fuzzy inputs “Error & Change
in Error” and three fuzzy outputs such as “Proportional
gain(Kp), Integral gain (Ki) & Derivative
gain(Kd)”.Dynamically PID Controller parameters are updated
through fuzzy interference system in platform of LabVIEW.
Fig. 2. Closed loop control for Boiler Drum Level Control
The above block diagram (Fig.2.) presents the
overview of proposed boiler drum level control which consists
of the following one major stage. The Hybrid Fuzzy-PID with
self tuning technique i.e. IF and THEN fuzzy control or rule
base according to the error and change in error that used to
provide necessary actions to achieve optimal response through
the auto-selection of PID controller parameters.
II. MATHEMATICAL MODELING OF BOILER DRUM
The modeling of boiler drum is described based on steam
drum, valve [1].Transfer function for steam drum is described
in equation (1).
Fig. 3. Modeling Block Diagram of Boiler Drum
0.25 0.25
2
2
( )
s
s s
G s
p
− +
+
= (1)
Transfer function for valve:
0.25 0.25
3 2
0.25
S
G
pv
s s s
− +
=
+ +
(2)
The equation (3) shows the series of process transfer function
and valve transfer function from the (fig.3.)
0.25 0.25
3 2
0.25
s
G
pv
s s s
− +
=
+ +
(3)
The transfer function of the boiler drum is:
0.25 0.25
3 2
0.3 2.15
s
G
pv
s s s
− +
=
+ +
(4)
III. DESIGN AND IMPLEMENTATION OF HYBRID FUZZY- PID
CONTROLLER
In industrial control systems widely used controller is PID
controller which is a generic feedback control loop
mechanism. Error value calculated by the PID controller is the
difference between a measured process variable (PV) and a
desired set point (SP).
The steady state error of any process could be eliminated
by PID controller through the integral action on error and
expecting output changes through derivative action on the
error with respect to the set point.
In PID Controller algorithm the output y(t) and error input
e(t) are related by the following equation,
( ) ( ) ( ) 1 ( ( ))
P d
i
de t
y t K e t e t dt T
dt
T
= + + 
 
 
 
 
(5)
Where,
Kp
=Proportional gain, Ki =Integral time and Kd=Derivative
time.
For tuning the three model controller parameters (Kp, Ki
and Kd) several methods are available. Currently the parameter
of three mode(P+I+D) controller are adjust manually by
tuning methods like Ziegler-Nichols method for closed loop
system and Process reaction curve method for open loop
system.
2016 Online International Conference on Green Engineering and Technologies (IC-GET)Using the some mathematical analysis above mentioned
common methods are applicable for manual tuning process.
For some selecting precise value of controller parameter these
manual tuning is not appreciable for eliminating the steady
state error of any process. To rectify such problems natural
system are combined with intelligent agent to get better
solution.
In this developed system fuzzy based rule is given to the
PID controller for precise and faster optimal response. It helps
to achieve desired set point with minimum rise time and
settling time than conventional PID controller. The Basic
structure of hybrid PID controller is shown in (fig.4.)
+
+
+
+
Fig.4.Basic Block of Hybrid Fuzzy-PID Controller
The relation between fuzzy input parameter(error and change
in error ) and fuzzy output parameter(Kp, Ki and Kd) are given
by the Hybrid Fuzzy- PID controller which is shown in figure
5.
By the principle of fuzzy self tuning, the three parameters
Kp
, Ki and Kd are modified in order to achieve control actions
if there is any update in error and change in error for various
level as set point.
Fig.5.Basic structure of Hybrid Fuzzy-PID
The error and change in error values are determined from
previous set point values and present set point values to
perform good dynamic, steady state and static performance
without any disturbances.
Five fuzzy labels (NL, NS, ZE, PS and PL) are framed for
fuzzy input parameters such as error and change in error
values which are mentioned in Table I. Similarly, six fuzzy
labels (PVS, PS, PMS, PM, PL, PVL) are framed for fuzzy
output parameters as represented in Table II.
The triangular membership function is used for both input
variables (“e”, “ec”) and output variables (Kp, Ki & Kd) as
shown in the figure 6 (a),(b),(c),(d),(e).
TABLE I FUZZY SET VALUES FOR INPUTS
Fuzzy Label Description
ZE Negative Large
PS Negative Small
PL Zero
ZE Positive Small
PS Positive Large
PL Negative Large
In the developed fuzzy system, the PID control parameters has
been modified as follows,
Kp
=K
p’+ΔKp (6)
Ki=Ki’+ΔKi (7)
Kd=Kd’+ΔKd (8)
TABLE II FUZZY SET VALUES FOR OUTPUTS
Fuzzy Label Description
PVS Positive Very Small
PS Positive Small
PMS Positive Medium small
PM Positive Medium
PL Positive Large
PVL Positive Very Large
(a)
(b)
2016 Online International Conference on Green Engineering and Technologies (IC-GET)(c)
(d)
(e)
Fig.6.Fuzzy Control of MF’s (a) Error “e” (b) Change in Error “ec” (c)
Proportional Gain “Kp” (d) Intregral Gain “Ki” (e) Derivative Gain “Kd”
The Twenty five rules are formulated for the developed
fuzzy logic control system. Inputs and outputs are related as
shown in (fig.7.) The working flowchart proposed drum level
control is shown in (fig.8.) Few of the rules are listed below.
Rule1. IF Error “e” is Negative Large (NL) and Change in
Error “ec” also Negative Large (NL) THEN Change in
proportional gain Kp’ is Positive Very Large (PVL) and
Change in Integral gain Ki’ is Positive Medium (PM) and
Change in Derivative gain Kd’ is Positive Very Small (PVS).
R2. IF “e’ is NS and “ec” is NL THEN Kp’is PVL and Ki’ is
PM and Kd’is PMS.
R3. IF “e’ is ZE and “ec” is NL THEN Kp’is PVL and Ki’ is
PM and Kd’is PM.
R4. IF “e’ is PS and “ec” is NL THEN Kp’is PVL and Ki’ is
PM and Kd’is PL.
R5. IF “e’ is PL and “ec” is NL THEN Kp’is PVL and Ki’ is PM and Kd’is
PVL
Fig.7.Output variable (Kd) versus the input variable Error “e” and
Change in Error “ec”
Fig.8.Flowchart of Closed Loop Drum Level Control.
IV. EXPERIMENTAL SETUP
This section is described simulation of conventional PID
Controller and hybrid fuzzy-PID controller action on drum
level control. Conventional PID controller (existing approach)
parameters are tuned by using zeiglar-nicholas tuning
methodology. The proposed approach (fuzzy tuning) is
achieved by trial and error method.
A. Conventional PID Controller Action on Drum Level
Control
Fig.8.Conventional PID controller simulation-Block diagram window
The above simulation block diagram (Fig.9.) describes the
conventional PID controller based on the boiler drum level
control. It consist of the transfer function of the boiler,
conventional PID controller and their parameter like
proportional gain Kp, integral time τ i and derivative time
τ d .
2016 Online International Conference on Green Engineering and Technologies (IC-GET)(a)
(b)
Fig. 4. Simulation result of PID controller Front Panel window
(a)before settling point (b)after settling point
In conventional PID controller the set point”20cm” is
attained after 39 seconds. It takes more time to settled due to
overshoot present in the simulation results Fig.10 (a) & (b).
B. Hybrid Fuzzy-PID Controller Action on Drum Level
Fig.11.Hybrid Fuzzy- PID controller Block Diagram Window
The above block diagram window (Fig.11) describes
the simulation block of hybrid fuzzy- PID controller based on
the boiler drum level control. The simulation block is designed
using boiler drum model parameters. Membership functions
and rule viewer for input and output variables are available in
the Fuzzy tuned controller.
Fig.12.Simulation results for Hybrid Fuzzy-PID controller
In hybrid Fuzzy-PID controller simulation results there is
no overshoot. Using logic rules Fuzzy controller is designed to
rectify the overshoot which is occurred in the conventional
PID controller. Also settling time also decreased in Hybrid
Fuzzy-PID controller. Here set point “20cm” is settled at 27.5
seconds as shown in (Fig.12.)
TABLE III TENTATIVE RESULTS OF PID CONTROLLER BASED DRUM
LEVEL CONTROL
Set point Level Settling time ts
(s)
Rise time tr
(s)
Steady
State
Error
in cm
25 % of Level 39.8 10.7 1.17
50 % of Level 47.3 10.7 1.57
75 % of Level 50 10.7 1.61
100 % of Level 54 10.7 1.67
TABLE IV HYBRID FUZZY-PID BASED DRUM LEVEL CONTROL
Set point Level Settling time ts
(s)
Rise time tr
(s)
Steady
State
Error
in cm
25 % of Level 27.5 8.4 0
50 % of Level 27.4 8.6 0
75 % of Level 27.8 8.2 0
100 % of Level 27.4 8.7 0
C. Comparison of Conventional PID and Hybrid Fuzzy-PID
Controller Performance on Drum Level Control
The designed Hybrid Fuzzy-PID controller performance is
investigated on various parameters (Settling time, Rise time
and Steady State Error) under various set point drum level
ranges along with conventional PID controller performance.
Tentative results are consolidated in Table III & Table IV
respectively.
2016 Online International Conference on Green Engineering and Technologies (IC-GET)Fig.13.Hybrid Fuzzy-PID and Conventional PID controller performance on
settling time.
The comparison of Hybrid Fuzzy-PID and conventional
PID controller performance on settling time is shown in
(fig.13.) The performance of both digital controllers are
investigated for the setpoint ranges 10cm, 20cm, 30cm and
40cm of drum level with their settling time. From the graph, it
is clear that, Hybrid fuzzy-PID controller to boiler drum level,
the response settled to 40cm of drum level quickly at the time
instance of 27.5 seconds whereas conventional PID controller
obtained 12.3 seconds (i.e.39.8 s.) more than that to settle due
to improper selection of PID controller parameters.
Fig.14.Hybrid Fuzzy-PID and Conventional PID controller performance
on rising time.
Similarly, Hybrid Fuzzy-PID controller and Conventional
PID controller responses are compared with different drum
level ranges for rise time. From the above (fig.14.) it is well
known that, the rise time of Hybrid Fuzzy PID response is less
than conventional PID controller.
Fig.15.Hybrid Fuzzy-PID and Conventional PID controller performance
on Steady State Error.
The (fig.15.) is illustrated the Steady State Error (SSE)
comparison of both conventional and hybrid PID controllers
with various drum level ranges such as 10cm, 20cm, 30cm and
40cm. The steady state error of Hybrid Fuzzy-PID controller
is constant zero (0 cm) for various operating level ranges
whereas conventional PID controller is obtained
1.17s,1.57s,1.61s,1.67s respectively.
V. CONCLUSION
The Conventional PID and Hybrid Fuzzy-PID
Controller techniques are successfully implemented for closed
loop control of Boiler drum level control system. The
performances of two different controllers are analyzed by the
investigation of settling time, rise time, dead time and steady
state error with simulation results. From the LabVIEW
Platform based simulation results, it is concluded that, the
Hybrid Fuzzy-PID parameters are tuned automatically to meet
the desired response and also results are short listed at various
set point of the level. The Hybrid Fuzzy-PID controller offers
the better performance control over the conventional PID
controller. While comparing with conventional PID controller
performance, Hybrid Fuzzy PID controller offers better
dynamic response, shorter settling time, rise time and zero
steady state error.
REFERENCES
[1] P. Meenu and G.Priya “Fuzzy logic Based Boiler Drum Level Control
and Production System” in International Journal for Research and
Development in Engineering, 2014. pp- 255-260.
[2] Wang Zhuo,Jilin, Wang Shichaoa and Jiang Yanyan “Simulation of
control of water level in boiler drum, in World Automation Congress ,
2012,pp.1-4.
[3] Zaiyi Liao and Arthur L Dexter, “An Inferential Model – Based
Predictive Control Scheme for Optimizing the Operation of Boilers in
Building Space – Heating Systems” IEEE Transaction on Control
System, Vol.18 , no.5,pp. 1092 – 1102, september 2010.
[4] Yu Daren and Xu Zhiqiang “Nonlinear Coordinated Control of Drum
Boiler Power Unit Based on Feedback Linearization” IEEE Transaction
on Energy Conversion,Vol.20, no.1,pp. 204 – 210,March 2005.
[5] Shaoyuan Li,Hangbo Liu, Wen- jian Cai , Yeng-Chai Soh and Li-Hua
Xie “A New Coordinated Control Strategy for Boiler-Turbine System of
Coal-Fired Power Plant” IEEE Tranaction on Control System,Vol.13,
no.6,pp. 943 – 954,2005.
[6] Keyur Solanki, Jalpa Shah, Nishith Bhattt “Modeling and Simulation of
prototype of boiler drum level control” International Journal on
Mechanical Engineering and Robotics,Vol.2,no.2,pp.1-6,2014.
[7] K. Ghousiya Begum, D.Mercy, H. Kiren Vedi and M. Ramathilagam
“An Intelligent Model Based Level Control of Boiler Drum”
International Journal of Emerging Technology and Advanced
Engineering,Vol.3,no.1,pp.516-521,January 2013.
[8] K.Padma priya, P.Naveen Kumar “Monitoring and Controlling of Boiler
Drum Parameters Using Lab View” International Journal of Innovative
Research in Science Engineering and Technology,vol.4,no.6,pp.1480-
1486,May 2015.
[9] E Gowthaman and CD Balaji “ Self Tuned PID Based Speed Control of
PMDC Drive. In: IEEE International Multi Conference on Automation,
Computing, Control, Communication &Compressed Sensing, Kerala,
India, pp.686-692, March 2013.

Machine Learning for Credit Card Fraud Detection
Luís Fernando Torres
Luís Fernando Torres

·
Follow

12 min read
·
Mar 30
40


1





Introduction
From food delivery apps to online clothing stores, the internet made it easier to purchase whatever we want whenever we want with the convenience of using our credit cards to do so. Credit cards are useful for a bunch of things, it saves us from the inconvenience of having to carry large amounts of cash with us to each place we go and it also allows us to advance a purchase that can be paid over time.

According to Statista, the number of worldwide transactions rose from 195 billion to 468 billion per year from 2014 to 2020.


Considering that we have billions of transactions happening daily all over the world and that people are using their credit cards more than ever through cellphone apps, online stores and using digital wallets, it isn’t hard to imagine that there are fraudsters looking for ways to make purchases using somebody else’s name and money.

In a scenario such as this one, it is extremely important that credit card companies are able to identify fraudulent transactions in order to avoid charging customers for items they didn’t purchase and that’s why I developed a machine learning classification model to teach a computer how to identify patterns that can tell when a transaction is in fact genuine or a fraud.

For this project, I used Python’s scikit-learn library and tested four different classification algorithms to identify which one of them would achieve the best results with our dataset.

Machine Learning

Briefly, machine learning is a branch of artificial intelligence and it focuses on the use of data and algorithms to teach a computer to imitate the human way of learning, improving its accuracy gradually through experience and performance of tasks.

Machine learning is an important component of data science and it uses statical methods and trains algorithms to perform tasks such as predictions and classifications based on some input data that allows the algorithm to produce an estimate about a pattern in the data.

If you wish to know more about machine learning and its use, click here to read an IBM article on the matter.

Scikit-Learn

Scikit-learn is an open-source machine learning library for Python that offers simple and efficient tools for classification, regression, clustering, dimensionality reduction, data preprocessing, comparisons, validations and parameters choosing for different algorithms.

Classification Algorithms

For this project, I used four different classification algorithms to perform the task of identifying patterns that make up fraudulent transactions. I used the Decision Tree classifier, which is a simple algorithm used to predict the value of a certain target variable through the learning of simple decision rules inferred from the data features.

I also used the Random Forest Classifier, Ada Boost Classifier and Gradient Boosting Classifier, algorithms based on ensemble methods which combines predictions of several base estimators to improve robustness over a single estimator. The Random Forest Classifier, for instance, is based on the construction of hundreds of random decision trees in order to get into a final result.

I’ll leave a link below for the Scikit-Learn online documentation where you’ll be able to read and acquire in-depth explanations on classifiers, how they work and the math behind them.

Credit Card Fraud Detection Project Development
This project was developed in Python language in Jupyter Notebook and published on GitHub and Kaggle.

The first step, of course, is importing all the necessary libraries for development

# Libraries for exploring, handling and visualizing dat
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, plotly.express as px
# Sklearn's preprocessing library
from sklearn.preprocessing import StandardScaler
# Importing train and test data split
from sklearn.model_selection import train_test_split
# Sklearn's metrics to evaluate our models
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score, f1_score
# Classifiers
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
# Setting theme style and color palette to seaborn
sns.set_theme(context = 'notebook', style='darkgrid',palette='muted')a
Understanding the Dataset
I then used pandas.read_csv( ) method to obtain our data and used df.head( ) to see the dataframe.

The dataset used for this project was the Credit Card Fraud Detection dataset, available on Kaggle, and it contains credit card transactions that were made during the month of September, 2013 by European clients during two days. It has 284,807 transactions and 30 variables.

The variable time contains the seconds elapsed between each transaction and the first transaction in the dataset. Amount contains the value of each transaction and lastly, we have class, which is a binary feature that tells us if that certain transaction was genuine or a fraud.

We also have other features V1, V2, … V28 that are numerical inputs result of a PCA transformation whose content couldn’t be displayed due to their confidential nature.

During exploration analysis, it was possible to see that the dataset contains only numerical inputs and no null values were present. I proceeded to an analysis on the amount feature with the describe( ) method.

# Statistics on the amounts
df.Amount.describe().round(2)

It is possible to see that 75% of transactions were below €77.16 and the highest transaction amount was €25,691.16, much higher than the average amount of €88.35. Let’s plot amounts on a scatter plot to see how these values are distributed in the dataset.

# Distribution of Amount
fig = px.scatter(df, x = 'Amount', y =df.index, color = df.Class,
                title = 'Distribution of Amount Values')
fig.update_layout(xaxis_title='Transaction Amount (in €)',
                    yaxis_title='Transactions')
fig.show('png')

It looks like most transactions are genuine, represented by the blue dots. We can also see that all high value transactions were in fact genuine, with no apparent fraudulent transaction being made above €5,000.00.

It seems, however, way too difficult to see any fraudulent transaction on the scatter plot, those were supposed to be represented by yellow dots, and all I can see are some small yellow colors behind all that blue on the left. This leaves us with a question: How many transactions were in fact fraud?

I used plotly to plot a pie chart to see how our classes are distributed in the dataset and this below is the result I’ve gotten.

# Visualizing Class distribution
fig = px.pie(df.Class,values = df.Class.value_counts(),
             names=['Genuine', 'Fraud'], title='Fraudulent x Genuine Transactions in the Dataset')
fig.show('png')

df.Class.value_counts()

Well, no wonder it wasn’t easy to see fraudulent transactions on the scatter plot. Only 0.17% of transactions were fraudulent! That’s 492 out of 284,807 transactions. We have a huge class imbalance to work on here.

Let’s try to get some more information on fraudulent transactions.

df.query("Class ==1").Amount.describe()

The highest fraud amount was €2,125.87, on average, fraudulent amounts costed around €122.21. Let’s plot these values on a scatter plot once again.

# Distribution of fraudulent transactions amount
fig = px.scatter(df.query("Class==1"), x = 'Amount', y =df.query("Class==1").index,
                title = 'Distribution of Fraudulent Amounts')
fig.update_layout(xaxis_title='Transaction Amount (in €)',
                    yaxis_title='Transactions')
fig.show('png')

Preparing Data
The time variable won’t be any useful for this project, so I dropped it from the dataframe.

df = df.drop(columns = ['Time'], axis = 1)
After doing that, I divided the dataset into the independent variables (X) and the target variable (y).

X = df.drop(columns=['Class'], axis=1)
y = df.Class
# Visualizing target variable class
y

# Visualizing independent variables
X

After doing that, it is time to split our data into training set and testing set. I’ve split them into 70% for training and 30% for testing.

train_x, test_x, train_y,test_y = train_test_split(X, y, test_size= .3, random_state = 123
print('X Train size: ', train_x.shape)
print('X Test size: ', test_x.shape)
print('X Test proportion ', "%s%%"%round((len(test_x) / (len(train_x) + len(test_x))) * 100))
print('Y Train size: ', train_y.shape)
print('Y Test size: ', test_y.shape)
print('Y Test proportion ', "%s%%"%round((len(test_y) / (len(train_y) + len(test_y))) * 100)))


We now have 199,364 transactions for training and 85,443 transactions for testing.

After that, I used StandardScaler to normalize amount values, since these values were way too discrepant when compared to the other features in the dataset. It is important to have all variables in a similar scale because otherwise the algorithm will attribute a heavier importance to the variable that has the biggest scale which will end up in a biased algorithm and negatively impact our results. We don’t want that!

StandardScaler is actually pretty simple and it standardizes a feature by subtracting the mean and then dividing values by the standard deviation.


In order to avoid the data leakage effect, it is important to apply StandardScaler on each training and testing set separately!

# Scaling data on the training set
scaler = StandardScaler()
train_x['Amount'] = scaler.fit_transform(train_x.Amount.values.reshape(-1,1))
train_x

# Scaling data on the testing set
scaler = StandardScaler()
test_x['Amount'] = scaler.fit_transform(test_x.Amount.values.reshape(-1,1))
test_x

After that, we proceed to deal with the imbalance of our data. I used imblearn library to apply SMOTE in order to oversample the fraudulent data, which means that I increased the total number of fraudulent transactions by synthetically generating more of them based on the frauds that we already have in the dataset.

Just to remember, this is how fraudulent and genuine transactions are distributed in the dataset.

y.value_counts() # 0 = Genuine Transactions | 1 = Fraud

I then applied SMOTE on the training set

from imblearn.over_sampling import SMOTE
train_x, train_y = SMOTE().fit_resample(train_x,train_y) # Reshaping data
And counted values on the train_y set

train_y.value_counts()

Now we have a 50 | 50 balance between genuine and fraudulent transactions in the training set. Note that I’ve only applied oversampling in the training set, while maintaining the test set untouched, with its original proportions, and I did that because the test set must be a full representation of reality.

Applying Classifiers
Well, after all that work done, let’s see how well the algorithms can predict the fraudulent transactions on the training set.

# Applying Random Forest Classifier 
random_forest = RandomForestClassifier(n_estimators = 100, random_state = 123)
random_forest.fit(train_x,train_y)
 
y_predictions_rf = random_forest.predict(test_x)
 
# Applying Decision Tree Classifier 
decision_tree = DecisionTreeClassifier(random_state = 123)
decision_tree.fit(train_x,train_y)
 
y_predictions_dt = decision_tree.predict(test_x)
 
# Applying Ada Boost Classifier 
ada_boost = AdaBoostClassifier(n_estimators = 100, random_state = 123)
ada_boost.fit(train_x,train_y)
 
y_predictions_ab = ada_boost.predict(test_x)
 
# Applying Gradient Boosting Classifier 
gradient_boosting = GradientBoostingClassifier(n_estimators = 100, random_state = 123)
gradient_boosting.fit(train_x,train_y)
 
y_prediction_gb = gradient_boosting.predict(test_x)
Before evaluating how well our models performed, I’ll briefly talk about the evaluation metrics for classification algorithms

Evaluation Metrics for Classification Models
When dealing with classification models, there are some evaluation metrics that we can use in order to see the efficiency of our models.

One of those evaluation metrics is the confusion matrix which is a summary of predicted results compared to the actual values of our dataset. This is what a confusion matrix looks like for a binary classification problem:


TP is for True Positive and it shows the correct predictions of a model for a positive class.

FP is for False Positive and it shows the incorrect predictions of a model for a positive class.

FN is for False Negative and it shows the incorrect predictions of a model for a negative class.

TN is for True Negative and it shows the correct predictions of a model for a negative class.

Beyond the confusion matrix, we also have some other relevant metrics. They are:

Accuracy
Accuracy simply tells us the proportion of correct predictions. This is how we calculate it:


Precision
Precision tells us how frequently our model correctly predicts positives. This is how we calculate it:


Recall
Recall, which can also be referred to as sensitivity, can tell us how well our model predicts the class that we want to predict. This is how we calculate it:


F1 Score
Lastly, F1 Score is the harmonic mean of precision and recall. This is how we calculate it:


Random Forest Scores
Let’s the metrics for Random Forest and its confusion matrix

# Printing Evaluation Metrics for Random Forest
metrics = [['Accuracy',(accuracy_score(test_y, y_predictions_rf))],
                         ['Precision',precision_score(test_y, y_predictions_rf)],
                         ['Recall', recall_score(test_y, y_predictions_rf)],
                         ['F1_score',f1_score(test_y, y_predictions_rf)]]
metrics_df = pd.DataFrame(metrics, columns = ['Metrics', 'Results'])
metrics_df

Confusion Matrix for Random Forest
confusion_matrix_rf = confusion_matrix(test_y, y_predictions_rf)
# Visualization
plt.figure(figsize=(10,7))
ax = plt.subplot()
sns.heatmap(confusion_matrix_rf, annot=True, fmt='g', ax = ax)
ax.set_xlabel('Predicted Values')
ax.set_ylabel('Actual Values')
ax.set_title('Confusion Matrix - Random Forest')
ax.xaxis.set_ticklabels(['Genuine','Fraud'])
ax.yaxis.set_ticklabels(['Genuine','Fraud'])
plt.show()

Decision Tree Scores


Ada Boost Scores


Gradient Boosting Scores


Let’s see how many fraudulent and genuine transactions we have in the testing set to compare these values with what we have on the confusion matrixes above.

# Counting how many fraudulent and how many genuine transactions we have on the testing set
test_y.value_counts()

The model that predicted the most numbers of fraudulent transactions correctly was the Ada Boost Classifier, who correctly identified 147 frauds out of 160, with a 91.87% recall, the highest of all the other algorithms. Remember, recall tells us how well our model predicts the class we want to predict.

Conclusion
When we work with a machine learning model, we must always know for a fact what it is that we’re trying to get from that model.

In this project, our goal is to detect fraudulent transactions when they occur, and the model that best performed such task was the Ada Boost Classifier with a recall of 91.87%, correctly detecting 147 fraudulent transactions out of 160. However, it is also important to note that the Ada Boost classifier had the biggest number of false positives, that is, 1,321 genuine transactions were mistakenly labeled as fraud, that’s 1.54% of all genuine transactions.

A genuine purchase being incorrectly identified as a fraud could be a problem.

In this scenario it is necessary to understand the business and make a few questions such as:

how cheap would a false positive be?
Would we keep the Ada Boost Classifier with the best performance in detecting frauds, while also detecting a lot of false positives or should we use the Random Forest Classifier, who also performed pretty well identifying frauds (82.50% recall) and reduced the number of false positives (0.02% of genuine transactions flagged as fraud). But that would also imply in a larger number of fraudsters getting away with it and customers being mistakenly charged…
These questions and a deeper understanding of how the business works and how we want to approach solving a problem using machine learning are fundamental for a decision-making process to choose whether or not if we’re willing to deal with a larger number of false positives to detect the largest amounts of frauds as possible.

GitHub and Kaggle
If you wish to see the full project and how I developed it step by step, you can see it on GitHub and also Kaggle, where plotly plots are interactive!

References
Credit Card Fraud Detection Dataset

Decision Trees

Ensemble Methods

Machine Learning

Number of Purchase Transactions on Global General Purpose Card Brands American Express, Diners/Discover, JCB, Mastercard, UnionPay and Visa from 2014 to 2020 (in billions)

Principal Component Analysis

Thank you!

Luís Fernando Torres

Follow me on LinkedIn.

Follow me on Kaggle.

What are Ensemble methods in Machine Learning?
Aqeel Anwar
Towards Data Science
Aqeel Anwar

·
Follow

Published in
Towards Data Science

·
6 min read
·
Jan 28, 2021
219


1





A visual walkthrough of the ensemble methods in machine learning with a cheatsheet

Background
Let's say you moved to a new place and want to dine out. How do you find a good place?

Solution 1: Find a food critic who is really good at his/her work and see if he/she has any recommendations for the restaurants in your area

Solution 2: Use Google and randomly look at one user's review for a couple of restaurants.

Solution 3: Use Google and look at multiple users' reviews for a couple of restaurants and average their ratings.

Let us analyze each of the above-mentioned solutions.

Solution 1:
Food critics are in general much accurate.
It is difficult to find a food critic
Maybe the food critic you found was a strict vegetarian, and you are not. In that case, the recommendations from the food critic will be biased.
Solution 2:
On the other hand, picking up a random person’s star rating for a restaurant on the internet is

Much less accurate
Easier to find
Solution 3:
Collectively it can be just the right amount of accuracy you need
Easier to find over the internet
Much less biased, since the users who have rated the restaurants come from various backgrounds.
Hence without the need to ask from a food critic, you can get a reasonably good recommendation on restaurants just by looking at a collective opinion of a group of random (but large) people. This is known as the wisdom of the crowd and is the backbone to various informative websites like Quora, Stack-exchange, and Wikipedia, etc.

What are Ensemble methods?
Ensemble methods in Machine Learning use more than one weak learner collectively to predict the output. Instead of training one large/complex model for your dataset, you train multiple small/simpler models (weak-learners) and aggregate their output (in various ways) to form your prediction as shown in the figure below


Inference: Ensemble method (Image by Author)
Types of Ensemble Methods
Generally speaking, there are three different types of Ensemble Methods commonly used in ML these days

Bagging
Boosting
Stacking
These methods have the same wisdom-of-the-crowd concept but differ in the details of what it focuses on, the type of weak learners used, and the type of aggregation used to form the final output.

1. Bagging
In Bagging (Bootstrap Aggerating), multiple weak-learners are trained in parallel. For each weak-learner, the input data is randomly sampled from the original dataset with replacement and is trained. A random sampling of the subset with replacement creates nearly iid samples. During inference, the test input is fed to all the weak-learners and the output is collected. The final prediction is carried out by voting on the outputs of each weak-learner.

The complete steps are shown in the block diagram below.


Ensemble Method — Bagging (Image by Author)
In bagging methods, the weak-learners, usually are of the same type. Since the random sampling with replacement creates iid samples, and aggregating iid variables doesn’t change the bias but reduces variance, the bagging method doesn't change the bias in the prediction but reduces its variance.

2. Boosting
In boosting, multiple weak-learners are learned sequentially. Each subsequent model is trained by giving more importance to the data points that were misclassified by the previous weak-learner. In this way, the weak-learners can focus on specific data points and can collectively reduce the bias of the prediction. The complete steps are shown in the block diagram below.


Ensemble Method — Boosting (Image by Author)
The first weak-learner is trained by giving equal weights to all the data points in the dataset. Once the first weak-learner is trained, the prediction error for each point is evaluated. Based on the error for each data point, the corresponding weight of the data point for the next learner is updated. If the data point was correctly classified by the trained weak-learner, its weight is reduced, otherwise, its weight is increased. Apart from updating the weights, each weak-learner also maintains a scalar alpha that quantifies how good was the weak-learner in classifying the entire training dataset.

The subsequent models are trained on these weighted sets of points. One way of carrying out training on a weighted set of points is to represent the weight term in the error. Instead of using mean-squared error, a weighted mean-squared error is used ensuring that data points with higher assigned weight, are given more importance in being correctly classified. The other way could be weighted sampling i.e. sample points based on their weights when training.

In the inference phase, the test input is fed to all the weak-learners and their output is recorded. The final prediction is achieved by scaling each weak-learner’s output with the corresponding weak-learner’s weight alpha before using them for voting as shown in the diagram above.

3. Stacking
In stacking, multiple weak-learners are trained in parallel, which is similar to what happens in bagging. But unlike bagging, stacking does not carry out simple voting to aggregate the output of each weak-learner to calculate the final prediction. Rather, another meta-learner is trained on the outputs of weak-learners to learn a mapping from the weak-learners output to the final prediction. The complete block diagram can be seen below.


Ensemble Method — Stacking (Image by Author)
Stacking usually has weak-learners of different types. Hence a simple voting method that gives equal weights to all the weak-learners prediction doesn't seem like a good idea (it would have been if the weak-learners were identical in structure). That is where the meta-learner comes in. It tries to learn which weak-learner is more important.

The weak-learners are trained in parallel, but the meta learner is trained sequentially. Once the weak-learners are trained, their weights are kept static to train the meta-learner. Usually, the meta-learner is trained on a different subset than what was used to train the weak-learners.

Cheat Sheet
The following cheat sheet covers the topic of Ensemble methods that might come in handy.


Cheat Sheet Ensemble methods (Source: http://cheatsheets.aqeel-anwar.com/)
Summary
Instead of training one network, Ensemble methods use multiple weak-learners and aggregate their individual output to create final predictions. A comparison of different Ensemble methods can be seen in the table below.


Comparison of Ensemble Methods (Image by Author)
Bonus:
Compact cheat sheets for this topic and many other important topics in Machine Learning can be found in the link below

Cheat Sheets for Machine Learning Interview Topics
A visual cheatsheet for ML interviews (www.cheatsheets.aqeel-anwar.com)
medium.com

If this article was helpful to you, feel free to clap, share and respond to it. If you want to learn more about Machine Learning and Data Science, follow me @
Aqeel Anwar
 or connect with me on LinkedIn.


Shailey Dash
Towards Data Science
Shailey Dash

·
Follow

Published in
Towards Data Science

·
22 min read
·
Nov 2, 2022
262


13





Though Decision Trees look simple and intuitive, there is nothing very simple about how the algorithm goes about the process deciding on splits and how tree pruning occurs. In this post I take you through a simple example to understand the inner workings of Decision Trees.


Iris Decision Tree from Scikit Learn ( Image source: sklearn)
Decision Trees are a popular and surprisingly effective technique, particularly for classification problems. But, the seemingly intuitive interface hides complexities. The criterion for selecting variables and hierarchy can be tricky to get, not to mention Gini index, Entropy ( wait, isn’t that physics?) and information gain (isn’t that information theory?). As you can see there are lots of tricky problems on which you can get stuck on. The best way to understand Decision Trees is to work through a small example which has sufficient complexity to be able to demonstrate some of the common points one suddenly goes, ‘ not sure what happens here…?’.

This post is therefore more like a tutorial or a demo where I will work through a toy dataset that I have created to understand the following:

1. What is a decision tree: root node, sub nodes, terminal/leaf nodes

2. Splitting criteria: Entropy, Information Gain vs Gini Index

3. How do sub nodes split

4. Why do trees overfit and how to stop this

5. How to predict using a decision tree

So, let’s get demonstrating…

1. What does a Decision Tree do?

Let’s begin at the real beginning with core problem. For example, we are trying to classify whether a patient is diabetic or not based on various predictor variables such as fasting blood sugar, BMI, BP, etc. This is obviously a prediction problem for a new patient. We also have 1000 patient records to help us develop an understanding of which features are most useful in predicting. Unlike other classification algorithms such as Logistic Regression, Decision Trees have a somewhat different way of functioning and identifying which variables are important.

The first thing to understand in Decision Trees is that they split the predictor space, i.e., the target variable into different sub groups which are relatively more homogenous from the perspective of the target variable. For example, if the target variable is binary, with categories 1 and 0 ( shown by green and red dots in the image below, then the decision tree works to split the target variable space into sub groups that are more homogenous in terms of having either 1’s or 0’s.


Target Variable Splitting process (Image source: author)
That is the overall concept. Let us begin with understanding the various elements of a decision tree.

Understanding components of a Decision Tree

A decision tree is a branching flow diagram or tree chart. It comprises of the following components:

. A target variable such as diabetic or not and its initial distribution.

A root node: this is the node that begins the splitting process by finding the variable that best splits the target variable
Node purity: Decision nodes are typically impure, or a mixture of both classes of the target variable (0,1 or green and red dots in the image). Pure nodes are those that have one class — hence the term pure. They either have green or red dots only in the image.
Decision nodes: these are subsequent or intermediate nodes, where the target variable is again split further by other variables
Leaf nodes or terminal nodes are pure nodes, hence are used for making a prediction of a numerical or class is made.
Let’s see this visually..


Structure of a Decision Tree (image source: my collection)
In general a decision tree takes a statement or hypothesis or condition and then makes a decision on whether the condition holds or does not. The conditions are shown along the branches and the outcome of the condition, as applied to the target variable, is shown on the node.

Arrows leading away from a node indicate a condition which is being applied to the node. Arrows pointing to a node indicate a condition that is being satisfied.

This is the first level of the Decision Tree — understanding the flow of splitting the decision space into smaller spaces which ultimately become more and more homogenous in the target variable. This ultimately leads to a prediction.

Decision Trees offer tremendous flexibility in that we can use both numeric and categorical variables for splitting the target data. Categoric data is split along the different classes in the variable. Numeric is a little more tricky as we have to split into thresholds for the condition being tested such as <18 and ≥18, for example. A numeric variable can appear multiple times in the data with different cut offs or thresholds. Also final classifications can be repeated.

The important things from data science perspective are:

1. Flow of information through the Decision Tree

2. How does Decision Trees select which variable to split on at decision nodes?

3. How does it decide that the tree has enough branches and that it should stop splitting?

Now let us look at a simplified toy example to understand the above process more concretely.

First the problem:

We have data for 15 data points of student data on pass or fail an online ML exam. To understand the basic process we begin with a dataset which comprises a target variable that is binary ( Pass/Fail) and various binary or categorical predictor variables such as:

whether enrolled in other online courses
whether student is from a maths, computer science or other background
Whether working or not working
The dataset is given below:


Toy Dataset for Online ML Exam( source: author)
Notice that only one variable, ‘Student Background’ has more than 2 levels or categories — Maths, CS, Others. It is one for the advantages of Decision Trees compared to other classification models such as Logistic Regression or SVM, that we do not need to carry out one hot encoding to make these into dummy variables.

Let us first look at the flow of how a decision tree works and then we will dive into the complexities of how the decisions are actually made…

Flow of a Decision Tree

A decision tree begins with the target variable. This is usually called the parent node. The Decision Tree then makes a sequence of splits based in hierarchical order of impact on this target variable. From the analysis perspective the first node is the root node, which is the first variable that splits the target variable.

To identify the root node we would evaluate the impact of all the variables that we have currently on the target variable to identify the variable that splits the exam Pass/Fail classes into the most homogenous groups. Our candidates for splitting this are: Background, Working Status and Other Online Courses.

What do we hope to achieve with this split? Suppose we begin with Working Status as the root node. This splits into 2 sub nodes, one each for Working and Not working. Thus the Pass/Fail status is updated in each sub node respectively.


Sample Decision Tree Flow( Image source: author created
So, this is the basic flow of the Decision Tree. As long as there is a a mixture of Pass and Fail in a sub node, there is scope to split further to try and get it to be only one category. This is termed the purity of the node. For example, Not Working has 5 Pass and 1 Fail, hence it is purer than the Working node which has 5P and 4F. A leaf node would be one which contains either Pass or Fail class only.

A node which is impure can be branched further for improving purity. However, most of the time we do not necessarily go down to the point where each leaf is ‘pure’. It is also important to understand that each node is standalone and hence the attribute that best splits the ‘Working’ node may not be the one that best splits the ‘Not Working’ node.

Now, let us move on to learning the core part of Decision Trees - key questions:

How does the Tree decide which variable to branch out at each level?

Greedy top down approach

Decision trees follow a a top-down, greedy approach that is known as recursive binary splitting. The recursive binary splitting approach is top-down because it begins at the top of the tree and then it successively splits the predictor space. At each split the predictor space gets divided into 2 and is shown via two new branches pointing downwards. The algorithm is termed is greedy because at each step of the process, the best split is made for that step. It does not project forwards and try and pick a split that might be more optimal for the overall tree.

The algorithm therefore evaluates all variables on some statistical criteria and then chooses the variable that performs best on the criteria.

Variable selection criterion

Here is where the true complexity and sophistication of decision lies. Variables are selected on a complex statistical criterion which is applied at each decision node. Now, variable selection criterion in Decision Trees can be done via two approaches:

1. Entropy and Information Gain

2. Gini Index

Both criteria are broadly similar and seek to determine which variable would split the data to lead to the underlying child nodes being most homogenous or pure. Both are used in different Decision Tree algorithms. To add to the confusion, it is not clear which one is the preferred approach. So, one has to have an understanding of both.

Let us begin with Entropy and Information Gain criterion

What is Entropy?

Entropy is a term that comes from physics and means a measure of disorder. More specifically, we can define it as:

Entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory.

https://en.wikipedia.org/wiki/Entropy

In information theory, the entropy of a random variable is the average level of “information”, “surprise”, or “uncertainty” inherent to the variable’s possible outcomes.

https://en.wikipedia.org/wiki/Entropy_(information_theory)

In the context of Decision Trees, entropy is a measure of disorder or impurity in a node. Thus, a node with more variable composition, such as 2Pass and 2 Fail would be considered to have higher Entropy than a node which has only pass or only fail. The maximum level of entropy or disorder is given by 1 and minimum entropy is given by a value 0.

Leaf nodes which have all instances belonging to 1 class would have an entropy of 0. Whereas, the entropy for a node where the classes are divided equally would be 1.

Entropy is measured by the formula:


Where the pi is the probability of randomly selecting an example in class i. Let us understand this a bit better in the context of our example. So, the initial entropy of at the parent node is given by the probability of getting a pass vs fail. In our dataset, the target variable has 9 passes and 6 fails. Hence the probabilities for the entropy formula are:



Now essentially what a Decision Tree does to determine the root node is to calculate the entropy for each variable and its potential splits. For this we have to calculate a potential split from each variable, calculate the average entropy across both or all the nodes and then the change in entropy vis a vis the parent node. This change in entropy is termed Information Gain and represents how much information a feature provides for the target variable.


Entropy_parent is the entropy of the parent node and Entropy_children represents the average entropy of the child nodes that follow this variable. In the current case since we have 3 variables for which this calculation must be done from the perspective of the split.

1. Work Status

2. Online Course Status

3. Student Background

To calculate entropy, first let us put our formulas for Entropy and Information Gain in terms of variables in our dataset:

Probability of pass and fail at each node, i.e, the Pi:

2. Entropy:


3. Average Entropy at child nodes:


Note, average Entropy is the weighted average of all the sub nodes that a parent node splits into. Thus, in our example this would be 2 sub nodes for Working Status and 3 sub nodes for Student Background.

4. Information Gain:


Parent Node Calculations

First we will calculate parent node entropy using the formula above. Use any log2 calculator online to calculate the log values. In our case they work out to:



(Mathematical note: log to base 2 of anything less than 1 is a negative number, hence we multiply by a minus sign to get a positive number)

So far, this is just the entropy of the parent node. Now we have to decide which attribute or variable to use to split this to get the root node.

Calculating the Root Node

For this we have to calculate a potential split from each variable, calculate the average entropy across both the nodes and then the change in entropy via a vis the parent node.

Let us begin with Work Status variable and calculate the entropy of the split.



We then calculate the average entropy for the Working status split as a weighted average with weights of the share of observations from the total number of observations that fall in each sub node.


Information Gain = Entropy_Parent — Entropy_child =

0.9183–0.8119 = .1858

(Calculations are shown in the spreadsheet below)

In a similar fashion we can evaluate the entropy and information gain for Student Background and Online Courses variables. The results are provided in the table below:

We will drop this variable for the time being and move on to evaluate the other variables. The spreadsheet below shows the Entropy calculations for all the variables:


Root Node Entropy Calculations (source: author)
To find the root node attribute we look at the Information gain from Student Background vis a vis initial parent entropy. This shows the maximum reduction of .5370. Hence, this is the attribute that would be selected as the root node. The other’s variables — ‘Working Status’ and ‘Online Courses’ show a much lower decrease in entropy vis a vis the parental node.

So, on the basis of the above calculations, we have determined what the root node would be. The tree would now look as follows:


Root Node of Decision Tree
Student Background splits the target variable into 3 groups. Everyone from CS background clearly passes and hence this is a terminal or leaf node. Everyone from Other backgrounds fails and this is also a terminal node. Maths background is split into 3 Pass and 4 Fail and hence it is impure and there is some scope for further splitting to attain greater purity.

Now to split the Maths background sub node, we need to calculate Entropy and Information Gain for the remaining variables, i.e., Working Status and Online Courses. We would then select the variable that shows the highest Information Gain.

The Entropy and Information Gain Calculations for the Maths Background node can be seen in the table below. Notice, we now have the Maths Background as the node that is being split, hence average Entropy for the splits is calculated using it as a base.

Note: putting in log(0) throws an error. However mathematically we can use the limit. Normally we just don’t include the pj=0 case in the calculation. However, I have included just to show the complete calculation.

By convention 0 log 0 = 0, since y log y → 0 as y → 0.

https://www.icg.isy.liu.se/courses/infotheory/lect1.pdf

The Entropy for each potential split is:


Splitting the Maths Subnode (Image source: author)
As we can see Information Gain is higher for the Working Status variable. Hence this is the variable used to continue branching.


Maths Node Branching
We now see that the Maths node has split into 1 terminal node on the right and one node which is still impure. Notice, now almost all our nodes are terminal nodes. There is only one node which is not terminal. We can try splitting it further using Other Online Courses. Anyway, you get the picture. In any case most Decision Trees do not necessarily split to the point where every node is a terminal node. Most algorithms have built in stops which we will discuss a little further down. Further, if the Decision Tree continues to split we have another problem which is that of overfitting. Again we shall discuss that below after we have briefly reviewed an alternative approach to developing a Decision Tree using the Gini Index.

Gini Index

The other way of splitting a decision tree is via the Gini Index. The Entropy and Information Gain method focuses on purity and impurity in a node. The Gini Index or Impurity measures the probability for a random instance being misclassified when chosen randomly. The lower the Gini Index, the better the lower the likelihood of misclassification.

The formula for Gini Index


Where j represents the no. of classes in the target variable — Pass and Fail in our example

P(i) represents the ratio of Pass/Total no. of observations in node.

So, Let’s take an example from the decision tree above. Let’s begin with the root node and calculate the Gini Index for each of the splits. The Gini Index has a minimum (highest level of purity) of 0. It has a maximum value of .5. If Gini Index is .5, it indicates a random assignment of classes.

Now let us calculate the Gini index for the root node for Student Background attribute. In this case we have 3 nodes. Gini formula requires us to calculate the Gini Index for each sub node. Then do a weighted average to calculate the overall Gini Index for the node.

Maths sub node: 4Pass, 3Fail


CS sub node: 4Pass, 0 Fail


Others sub node: 0Pass, 4 Fail


As we can see the probability for misclassification in CS node is zero, since everyone passes. Similarly no scope for misclassification on Others node as everyone fails. Only the maths node has possibility of misclassification, and this is quite high, given that the maximum Gini Index is .5.

The overall Gini Index for this split is calculated similarly to the entropy as weighted average of the distribution across the 3 nodes.


Similarly, we can also compute the Gini Index for Working Status and Online Courses. These are given below:

Working/Not working


Online Courses


The Gini Index is lowest for the Student Background variable. Hence, similar to the Entropy and Information Gain criteria, we pick this variable for the root node. In a similar fashion we would again proceed to move down the tree, carrying out splits where node purity is less

Gini Index vs Information Gain

Depending on which impurity measurement is used, tree classification results can vary. This can make small (or sometimes large) impact on your model. There seems to be no one preferred approach by different Decision Tree algorithms. For example, CART uses Gini; ID3 and C4.5 use Entropy.

The Gini index has a maximum impurity is 0.5 and maximum purity is 0, whereas Entropy has a maximum impurity of 1 and maximum purity is 0.

How does a prediction get made in Decision Trees

Now that we have understood, hopefully in detail, how Decision Trees carry out splitting and variable selection, we can move on to how they do prediction. Actually, once a tree is trained and tested, prediction is easy. The tree basically provides a flow chart based on various predictor variables. Suppose we have a new instance entering the flow along with its values of different predictor variables. Unlike training and test data , it will not have the class for the target attribute. We are trying to predict this class by moving down the tree, testing its values of different predictor variables at different branches. Ultimately, the new instance will move into a leaf node and will be classified according to the class prevailing in the leaf node.

Suppose it looks like the below configuration:


Based on our tree, we would first check the Math branch, then Working Yes branch. That as we have seen is a leaf node and the new observation would be classified on the basis of the majority vote in this node, i.e., since it is Pass, this new observation would also be predicted to be Pass.

In practice, when the algorithm is evaluating a new example and reaches a leaf node, the prediction is based on the modal value of categories in the leaf node. As seen in the above case, the Working node is not fully pure. However, we go with the prediction of the modal value, which is Pass. In general, most leaf nodes are not pure and, hence for categorical prediction, we use the modal value for prediction. If it is a numerical prediction (regression tree), we predict the mean value of the target values at each leaf node.

Overfitting and Decision Trees

Overfitting can be a big challenge with Decision Trees. Even in our toy example, we can see the algorithm continues to split till it reaches a leaf node. Often the leaf node may just have one or two instances. This will clearly lead to a complex tree structure which may not generalize well to a test scenario. This is because each leaf will represent a very specific set of attribute combinations that are seen in the training data, and the tree will not be able to classify attribute combinations not seen in the training data. There are several ways we can prevent the decision tree from becoming too unwieldy: 3 broad approaches to avoiding overfitting are distinguished:

Pre pruning or Early stopping: Preventing the tree from growing too big or deep
Post Pruning: Allowing a tree to grow to its full depth and then getting rid of various branches based on various criteria
3. Ensembling or using averages of multiple models such as Random Forest

We will only briefly overview at pre and post pruning techniques here. Ensemble techniques such as Random Forest require more explanation and hence will be tackled in a separate article.

Pruning

Pre pruning

The pre-pruning technique refers to the early stopping of the growth of the decision tree. The pre-pruning technique involves tuning the hyperparameters of the decision tree model prior to the training pipeline. The hyperparameters of the DecisionTreeClassifier in SkLearn include max_depth, min_samples_leaf, min_samples_split which can be tuned to early stop the growth of the tree and prevent the model from overfitting. The best way is to use the sklearn implementation of the GridSearchCV technique to find the best set of hyperparameters for a Decision Tree model.

A challenge with the early stopping approach is that it faces a ‘horizon’ problem, where an early stopping may prevent some more fruitful splits down the line.

Post Pruning

In this technique, we allow the tree to grow to its maximum depth. Then we remove parts of the tree to prevent overfitting. We effectively consider subtrees of the full tree which are evaluated on a criteria and then removed. Hence we are effectively going ‘up’ the tree and converting leaves to nodes and subtrees. The criteria whether a particular consolidation goes through or not is usually MSE for regression trees and classification error for classification trees.

A challenge with post pruning is that a decision tree can grow very deep and large and hence evaluating every branch can be computationally expensive. An important post pruning technique is Cost complexity pruning (ccp) which provides a more efficient solution in this regard. CCP is a complex and advanced technique which is parametrized by the parameter α in Scikit Learn DecisionTreesclassifier module.

So, how does CCP work and what does it do?

The basic problem that CCP addresses is: How to determine the best way to prune a tree? Intuitively, we would select a sub tree to prune such that its removal leads to a lower test error rate. This can be done using cross validation or, if we have sufficient sample, the validation set approach. However, given the number of sub trees in a fully grown tree for even a small sample, this is likely to be a very computationally and time intensive process.

Cost complexity pruning — also known as weakest link pruning — gives us a way to do just this. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter α.…….

The tuning parameter α controls a trade-off between the subtree’s complexity and its fit to the training data. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree

An Introduction to Statistical Learning, P 333

Essentially the parameter α is very similar to the penalty term in Lasso regression. The basic equation for CCP is given below


CCP equation, Hastie, p332 (image source: James et al)
This is one complex equation. But let’s try and understand a bit further. Some definitions:

Rm: Rm is the rectangle (i.e. the subset of predictor space) corresponding to the mth terminal node

yˆRm is the predicted response associated with mth terminal leaf

(yi -yˆRm) — MSE for the sub tree referenced by terminal node m ( we are using regression tree approach for this equation. For simplicity I am following the equation and approach in James et al(2014).

|T|: is the no. of terminal nodes in the tree T

Now let’s see what the equation is doing. We are essentially minimizing cost or loss given by (yi -yˆRm) across all terminal nodes. Now α is a term that multiplies the total number terminal nodes in the tree. If α=0, then we are minimizing the training loss. The tree will be the same as the original tree. However, with α>0, we add a penalty term which increases with the number of terminal nodes |T|. This means the overall cost gets minimized for a smaller subtree.

Minimal cost complexity pruning recursively finds the node with the “weakest link”. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first.

https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html

What does this mean? It means that the algorithm is hunting out nodes where the training loss is already high and hence can only be minimized with a small α. On the other hand, nodes where the training loss is smaller, can accommodate a larger penalty term as part of minimization.

To get an idea of what values of ccp_alpha could be appropriate, scikit-learn provides DecisionTreeClassifier.cost_complexity_pruning_path that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process.

As α increases, more of the tree is pruned. We then have a tradeoff between bias and variance. With the ccp_alpha

Effectively we increase the bias of the model, i.e. , we simplify it. However, on the con side this means we have to tolerate increasing levels of impurity in the terminal nodes. We see that as α increases both no. of nodes and tree depth reduces.

How to determine optimal α

Plotting ccp_alpha vs train and test accuracy we see that when α =0 and keeping the other default parameters of DecisionTreeClassifier, the tree overfits, leading to a 100% training accuracy and 88% testing accuracy. As alpha increases, more of the tree is pruned, thus creating a decision tree that generalizes better. at some point, however, further increases in α actually lead to a decrease in test accuracy as the model becomes too simplified. In this example, setting ccp_alpha=0.015 maximizes the testing accuracy. (refer to https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html for details).


Accuracy vs Alpha(Image source SkLearn)
Advantages and Disadvantages of Trees Decision trees

1. Trees give a visual schema of the relationship of variables used for classification and hence are more explainable. The hierarchy of the tree provides insight into variable importance.

2. At times they can actually mirror decision making processes.

3. White box model which is explainable and we can track back to each result of the model. This is in contrast to black box models such as neural networks.

4. In general there is less need to prepare and clean data such as normalization and one hot encoding of categorical variables and missing values.

Note the Sklearn implementation currently does not support categorical variables, so we do need to create dummy variables. Similarly it does not support missing values. But both can be handled in theory.

5. Model can be validated statistically

Disadvantages

1. Prone to overfitting and hence lower predictive accuracy

2. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem for example can be mitigated by using decision trees within an ensemble

3. Can be non-robust, i.e., a small change in the data can cause a large change in the final estimated tree

4. Predictions are approximate, based on relevant terminal nodes. Hence it it may not be the best method to extrapolate the results of the model to unseen cases.

5. Decision tree learners create biased trees if some classes dominate. It is required to balance the dataset prior to fitting with the decision tree.

So, that’s it for Decision Trees form start to at least two thirds of the way. There are a lot of complexities, hence I cannot say end. I hope you liked this blog on the inner workings of Decision Trees. One thing is clear, this is far from a simple technique. I have so far reviewed only the complexities of how variables hierarchy is chosen and a tree structure is built up and how pruning is done. There are many types of Decision Tree algorithms even in Scikit Learn. These include: ID3, C4.5, C5.0 and CART. Exploration of these models is for another blog.

References

· G. James, D. Witten, T. Hastie, and R. Tibshirani (2013), An Introduction to Statistical Learning: with Applications in R, Springer, (2013)

Advanced exploratory data analysis (EDA) with Python
Michael Notter
EPFL Extension School
Michael Notter

·
Follow

Published in
EPFL Extension School

·
14 min read
·
Feb 18, 2022
1.8K


18





How to quickly get a handle on almost any tabular dataset
[Find the code to this article here.]

Getting a good feeling for a new dataset is not always easy, and takes time. However, a good and broad exploratory data analysis (EDA) can help a lot to understand your dataset, get a feeling for how things are connected and what needs to be done to properly process your dataset.

In this article, we will touch upon multiple useful EDA routines. However, to keep things short and compact we might not always dig deeper or explain all of the implications. But in reality, spending enough time on a proper EDA to fully understand your dataset is a key part of any good data science project. As a rule of thumb, you probably will spend 80% of your time in data preparation and exploration and only 20% in actual machine learning modeling.

Investigation of structure, quality and content
Overall, the EDA approach is very iterative. At the end of your investigation you might discover something that will require you to redo everything once more. That is normal! But to impose at least a little bit of structure, I propose the following structure for your investigations:

Structure investigation: Exploring the general shape of the dataset, as well as the data types of your features.
Quality investigation: Get a feeling for the general quality of the dataset, with regards to duplicates, missing values and unwanted entries.
Content investigation: Once the structure and quality of the dataset is understood, we can go ahead and perform a more in-depth exploration on the features values and look at how different features relate to each other.
But first we need to find an interesting dataset. Let’s go ahead and load the road safety dataset from OpenML.


1. Structure Investigation
Before looking at the content of our feature matrix X, let’s first look at the general structure of the dataset. For example, how many columns and rows does the dataset have? And how many different data types do those features include?


1.1. Structure of non-numerical features
Data types can be numerical and non-numerical. First, let’s take a closer look at the non-numerical entries.


Even though Sex_of_Driver is a numerical feature, it somehow was stored as a non-numerical one. This is sometimes due to some typo in data recording. These kind of things need to be taken care of during data preparation.

Once this is taken care of, we can use the .describe() function to investigate how many unique values each non-numerical feature has and with which frequency the most prominent value is present - using the code df_X.describe(exclude="number") :


1.2. Structure of numerical features
Next, let’s take a closer look at the numerical features. More precisely, let’s investigate how many unique values each of these feature has. This process will give us some insights about the number of This process will give us some insights about the number of binary (2 unique values), ordinal (3 to ~10 unique values) and continuous (more than 10 unique values) features in the dataset.



1.3. Conclusion of structure investigation
At the end of this first investigation, we should have a better understanding of the general structure of our dataset. Number of samples and features, what kind of data type each feature has, and how many of them are binary, ordinal, categorical or continuous. For an alternative way to get such kind of information you could also use df_X.info() or df_X.describe().

2. Quality Investigation
Before focusing on the actual content stored in these features, let’s first take a look at the general quality of the dataset. The goal is to have a global view on the dataset with regards to things like duplicates, missing values and unwanted entries or recording errors.

2.1. Duplicates
Duplicates are entries that represent the same sample point multiple times. For example, if a measurement was registered twice by two different people. Detecting such duplicates is not always easy, as each dataset might have a unique identifier features (e.g. an index number or recording time that is unique to each new sample). So you might want to ignore them first. And once you are aware about the number of duplicates in your dataset, you can simply drop them with .drop_duplicates().


2.2. Missing values
Another quality issue worth to investigate are missing values. Having some missing values is normal. What we want to identify at this stage are big holes in the dataset, i.e. samples or features with a lot of missing values.

2.2.1. Per sample
To look at number of missing values per sample we have multiple options. The most straight forward one is to simply visualize the output of df_X.isna(), with something like this:



This figure shows on the y-axis each of the 360'000 individual samples, and on the x-axis if any of the 67 features contains a missing value. While this is already a useful plot, an even better approach is to use the missingno library, to get a plot like this one:



From both of these plots we can see that the dataset has a huge hole, caused by some samples where more than 50% of the feature values are missing. For those samples, filling the missing values with some replacement values is probably not a good idea.

Therefore, let’s go ahead and drop samples that have more than 20% of missing values. The threshold is inspired by the information from the ‘Data Completeness’ column on the right of this figure.


2.2.2. Per Feature
As a next step, let’s now look at the number of missing values per feature. For this we can use some pandas trickery to quickly identify the ratio of missing values per feature.



From this figure we can see that most features don’t contain any missing values. Nonetheless, features like 2nd_Road_Class, Junction_Control, Age_of_Vehicle still contain quite a lot of missing values. So let's go ahead and remove any feature with more than 15% of missing values.


2.2.3. Small side note
Missing values: There is no strict order in removing missing values. For some datasets, tackling first the features and than the samples might be better. Furthermore, the threshold at which you decide to drop missing values per feature or sample changes from dataset to dataset, and depends on what you intend to do with the dataset later on.

Also, until now we only addressed the big holes in the dataset, not yet how we would fill the smaller gaps. This is content for another post.

2.3. Unwanted entries and recording errors
Another source of quality issues in a dataset can be due to unwanted entries or recording errors. It’s important to distinguish such samples from simple outliers. While outliers are data points that are unusual for a given feature distribution, unwanted entries or recording errors are samples that shouldn’t be there in the first place.

For example, a temperature recording of 45°C in Switzerland might be an outlier (as in ‘very unusual’), while a recording at 90°C would be an error. Similarly, a temperature recording from the top of Mont Blanc might be physical possible, but most likely shouldn’t be included in a dataset about Swiss cities.

Of course, detecting such errors and unwanted entries and distinguishing them from outliers is not always straight forward and depends highly on the dataset. One approach to this is to take a global view on the dataset and see if you can identify some very unusual patterns.

2.3.1. Numerical features
To plot this global view of the dataset, at least for the numerical features, you can use pandas’ .plot() function and combine it with the following parameters:

lw=0: lw stands for line width. 0 means that we don't want to show any lines
marker=".": Instead of lines, we tell the plot to use . as markers for each data point
subplots=True: subplots tells pandas to plot each feature in a separate subplot
layout=(-1, 4): This parameter tells pandas how many rows and columns to use for the subplots. The -1 means "as many as needed", while the 2 means to use 2 columns per row.
figsize=(15, 30), markersize=1: To make sure that the figure is big enough we recommend to have a figure height of roughly the number of features, and to adjust the markersize accordingly.
So what does this plot look like?



Each point in this figure is a sample (i.e. a row) in our dataset and each subplot represents a different feature. The y-axis shows the feature value, while the x-axis is the sample index. These kind of plots can give you a lot of ideas for data cleaning and EDA. Usually it makes sense to invest as much time as needed until you’re happy with the output of this visualization.

2.3.2. Non-numerical features
Identifying unwanted entries or recording errors on non-numerical features is a bit more tricky. Given that at this point, we only want to investigate the general quality of the dataset. So what we can do is take a general look at how many unique values each of these non-numerical features contain, and how often their most frequent category is represented. To do so, you can use: df_X.describe(exclude=["number", "datetime])


There are multiple ways for how you could potentially streamline the quality investigation for each individual non-numerical features. None of them is perfect, and all of them will require some follow up investigation. But for the purpose of showcasing one such a solution, what we could do is loop through all non-numerical features and plot for each of them the number of occurrences per unique value.



We can see that the most frequent accident (i.e. Accident_Index), had more than 100 people involved. Digging a bit deeper (i.e. looking at the individual features of this accident), we could identify that this accident happened on February 24th, 2015 at 11:55 in Cardiff UK. A quick internet search reveals that this entry corresponds to a luckily non-lethal accident including a minibus full of pensioners.

The decision for what should be done with such rather unique entries is once more left in the the subjective hands of the person analyzing the dataset. Without any good justification for WHY, and only with the intention to show you the HOW - let’s go ahead and remove the 10 most frequent accidents from this dataset.


2.4. Conclusion of quality investigation
At the end of this second investigation, we should have a better understanding of the general quality of our dataset. We looked at duplicates, missing values and unwanted entries or recording errors. It is important to point out that we didn’t discuss yet how to address the remaining missing values or outliers in the dataset. This is a task for the next investigation, but won’t be covered in this article.

3. Content Investigation
Up until now we only looked at the general structure and quality of the dataset. Let’s now go a step further and take a look at the actual content. In an ideal setting, such an investigation would be done feature by feature. But this becomes very cumbersome once you have more than 20–30 features.

For this reason (and to keep this article as short as needed) we will explore three different approaches that can give you a very quick overview of the content stored in each feature and how they relate.

3.1. Feature distribution
Looking at the value distribution of each feature is a great way to better understand the content of your data. Furthermore, it can help to guide your EDA, and provides a lot of useful information with regards to data cleaning and feature transformation. The quickest way to do this for numerical features is using histogram plots. Luckily, pandas comes with a builtin histogram function that allows the plotting of multiple features at once.



There are a lot of very interesting things visible in this plot. For example…

Most frequent entry: Some features, such as Towing_and_Articulation or Was_Vehicle_Left_Hand_Drive? mostly contain entries of just one category. Using the .mode() function, we could for example extract the ratio of the most frequent entry for each feature and visualize that information.



Skewed value distributions: Certain kind of numerical features can also show strongly non-gaussian distributions. In that case you might want to think about how you can transform these values to make them more normal distributed. For example, for right skewed data you could use a log-transformation.

3.2. Feature patterns
Next step on the list is the investigation of feature specific patterns. The goal of this part is two fold:

Can we identify particular patterns within a feature that will help us to decide if some entries need to be dropped or modified?
Can we identify particular relationships between features that will help us to better understand our dataset?
Before we dive into these two questions, let’s take a closer look at a few ‘randomly selected’ features.



In the top row, we can see features with continuous values (e.g. seemingly any number from the number line), while in the bottom row we have features with discrete values (e.g. 1, 2, 3 but not 2.34).

While there are many ways we could explore our features for particular patterns, let’s simplify our option by deciding that we treat features with less than 25 unique features as discrete or ordinal features, and the other features as continuous features.


3.2.1. Continuous features
Now that we have a way to select the continuous features, let’s go ahead and use seaborn’s pairplot to visualize the relationships between these features. Important to note, seaborn's pairplot routine can take a long time to create all subplots. Therefore we recommend to not use it for more than ~10 features at a time.

Given that in our case we only have 11 features, we can go ahead with the pairplot. Otherwise, using something like df_continuous.iloc[:, :5] could help to reduce the number of features to plot.



There seems to be a strange relationship between a few features in the top left corner. Location_Easting_OSGR and Longitude, as well as Location_Easting_OSGR and Latitude seem to have a very strong linear relationship.



Knowing that these features contain geographic information, a more in-depth EDA with regards to geolocation could be fruitful. However, for now we will leave the further investigation of this pairplot to the curious reader and continue with the exploration of the discrete and ordinal features.

3.2.2. Discrete and ordinal features
Finding patterns in the discrete or ordinal features is a bit more tricky. But also here, some quick pandas and seaborn trickery can help us to get a general overview of our dataset. First, let’s select the columns we want to investigate.


As always, there are multiple way for how we could investigate all of these features. Let’s try one example, using seaborn’s stripplot() together with a handy zip() for-loop for subplots.

Note, to spread the values out in the direction of the y-axis we need to chose one particular (hopefully informative) feature. While the ‘right’ feature can help to identify some interesting patterns, usually any continuous feature should do the trick. The main interest in this kind of plot is to see how many samples each discrete value contains.



There are too many things to comment here, so let’s just focus on a few. In particular, let’s focus on 6 features where the values appear in some particular pattern or where some categories seem to be much less frequent than others. And to shake things up a bit, let’s now use the Longitude feature to stretch the values over the y-axis.



These kind of plots are already very informative, but they obscure regions where there are a lot of data points at once. For example, there seems to be a high density of points in some of the plots at the 52nd latitude. So let’s take a closer look with an appropriate plot, such as violineplot ( or boxenplot or boxplot for that matter). And to go a step further, let's also separate each visualization by Urban_or_Rural_Area.



Interesting! We can see that some values on features are more frequent in urban, than in rural areas (and vice versa). Furthermore, as suspected, there seems to be a high density peak at latitude 51.5. This is very likely due to the more densely populated region around London (at 51.5074°).

3.3. Feature relationships
Last, but not least, let’s take a look at relationships between features. More precisely how they correlate. The quickest way to do so is via pandas’ .corr() function. So let's go ahead and compute the feature to feature correlation matrix for all numerical features.

Note: Depending on the dataset and the kind of features (e.g. ordinal or continuous features) you might want to use the spearman method instead of the pearson method to compute the correlation. Whereas the Pearson correlation evaluates the linear relationship between two continuous variables, the Spearman correlation evaluates the monotonic relationship based on the ranked values for each feature. And to help with the interpretation of this correlation matrix, let's use seaborn's .heatmap() to visualize it.



This looks already very interesting. We can see a few very strong correlations between some of the features. Now, if you’re interested actually ordering all of these different correlations, you could do something like this:


As you can see, the investigation of feature correlations can be very informative. But looking at everything at once can sometimes be more confusing than helpful. So focusing only on one feature with something like df_X.corrwith(df_X["Speed_limit"]) might be a better approach.

Furthermore, correlations can be deceptive if a feature still contains a lot of missing values or extreme outliers. Therefore, it is always important to first make sure that your feature matrix is properly prepared before investigating these correlations.

3.4. Conclusion of content investigation
At the end of this third investigation, we should have a better understanding of the content in our dataset. We looked at value distribution, feature patterns and feature correlations. However, these are certainly not all possible content investigation and data cleaning steps you could do. Additional steps would for example be outlier detection and removal, feature engineering and transformation, and more.

A proper and detailed EDA takes time! It is a very iterative process that often makes you go back to the start, after you addressed another flaw in the dataset. This is normal! It’s the reason why we often say that 80% of any data science project is data preparation and EDA.

Take home message
Be mindful that an in-depth EDA can consume a lot of time. And just because something seems interesting doesn’t mean that you need to follow up on it. Always remind yourself what the dataset will be used for and tailor your investigations to support that goal. Sometimes it is also ok, to just do a quick-and-dirty data preparation and exploration. This will allow you to move on to the data modeling part rather quickly, and to establish a few preliminary baseline models and perform some informative results investigation.


Viola Jones Algorithm and Haar Cascade Classifier
Complete explanation and mathematics for beginners
Mrinal Tyagi
Towards Data Science
Mrinal Tyagi

·
Follow

Published in
Towards Data Science

·
6 min read
·
Jul 13, 2021
126


1





Viola Jones is a novel approach to rapid detection of objects with running capabilities of 15 frames per second. It was the first to achieve real time object detection.


(Image by author)
In this article, I talk about “Viola Jones Algorithm” and it includes the following subtopics :

Viola Jones Detector
What are Haar like features?
What is an integral image?
Calculation of Haar like features with Integral Image
Boosting and AdaBoost algorithm
Deep Dive into AdaBoost Algorithm mathematics
Cascade Filter
Implementation using OpenCV Library
Viola Jones Detector
A Viola Jones detector consists of following steps :

Calculating Integral Image
Calculating Haar like features
AdaBoost Learning Algorithm
Cascade Filter
What are Haar like Features?
Haar features are the relevant features for face detection. It was proposed by Alfred Haar in 1909. They are like convolutional kernels. There are various types of haar like features but the most dominant features used are :

2 Rectangular Haar features
3 Rectangular Haar features
4 Rectangular Haar features

(Image by author) Inspired (https://docs.opencv.org/3.4/d2/d99/tutorial_js_face_detection.html)
The values of a 2 rectangular feature is the difference between the sum of the pixels within 2 rectangular regions. The regions have same shape and size and are horizontally and vertically adjacent. A three rectangular feature computes the sum in a centre rectangle. Finally a four rectangular feature computes the difference between diagonal pairs of rectangles. Various variations of these regions of different sizes are convolved through the image in order to get multiple filters that will be inputs to the AdaBoost training algorithm. Calculation of these features using the standard technique would require a high computation time. In order to reduce this time, a new approach called the integral image was suggested by the authors of the paper.

What is an integral Image?
Because we have to use haar-like features in all possible sizes and locations which eventually result in around 200k features to calculate which is a really big number. The problem with novel calculation of haar features is that we have to calculate the average of a given region multiple times and the time complexity of these operations are O(n*n). We can use an integral image approach to achieve O(1) running time. A given pixel in the integral image is the sum of all the pixels on the left and all the pixels above it.


(Image by author) Inspired (https://www.mathworks.com/help/images/integral-image.html)

(Image by author) Inspired (https://m.blog.naver.com/natalliea/222198638897)
The sum of all purple boxes in the original image is equal to the sum of green boxes in the integral image subtracted by the purple boxes in the integral image.

Calculation of Haar like features with Integral Image
Using integral images we can achieve constant time evaluation of Haar features.

Edge Features or 2 Rectangular Features requires only 6 memory lookups
Line Features or 3 Rectangular Features requires only 8 memory lookups.
Diagonal Features or 4 Rectangular Features requires only 9 memory lookups.
2 Rectangle = A-2B+C-D+2E-F

3 Rectangle = A-B-2C+2D+2E-2F-G+H

4 Rectangle = A-2B+C-2D+4E-2F+H-2I+J


Technique for calculation of sum of regions for calculation of haar like features in a constant amount of time. (Image by author)
Boosting and AdaBoost Algorithm
Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. AdaBoost also known as Adaptive Boosting is one of the most popular Boosting techniques used.

AdaBoost : One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor under-fitted. This results in new predictors focusing more and more on the hard cases. This is known as Adaptive Boosting. For example, to build an Adaptive Boosting classifier, a first base classifier (such a Decision Tree or SVM classifier) is trained and used to make predictions on the training set. The relative weights of the misclassified predictions are altered and increased in order to lay more emphasis on these predictions while making the next predictor. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated and so on. Once all the predictions are trained, the ensemble method makes predictions very much like boosting except the predictors have different weights depending on their overall accuracy on the weighted training set. The drawback of this type of algorithm is that it cannot be parallelized thereby increasing time required. Thus after successfully running AdaBoost on all the features we are left with the most relevant features required for detection. Therefore, this reduces computational time as we don’t have to go through all the features and is much more efficient.


(Image by author) Inspired (Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems)
Deep Dive into AdaBoost Algorithm
Let’s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially set to 1/m. A first predictor is trained and its weighted error rate r1 is computed on the training set


Here we take only the misclassified instances and sum up the weights of those instances to get the weighted error rate (Image by author)
The predictor’s weight j is then computed using the formulae given below. The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, most often, it is wrong and its weight will be negative.


(Image by author)
Next the instance weights are updated using the formula provided below in order to boost the misclassified instances


(Image by author)
Then all the predictors are normalized using the formulae provided below.


(Image by author)
Finally, a new predictor is trained using the updated weights, and the whole process is repeated until the desired number of predictors is reached which is specified by the user.

During inference, AdaBoost simply computed the predictions of all the predictors and weights then using the predictor weight αj. The predicted class is the one that receives the majority of weighted votes.

Cascade Filter
Strong features are formed into a binary classifier. : Positive matches are sent along to the next feature. Negative matches are rejected and exit computation.
Reduces the amount of computation time spent on false windows.
Threshold values might be adjusted to tune accuracy. Lower threshold yield higher detection rated and more false positives.

(Image by author) Inspired (https://www.researchgate.net/figure/Cascade-classifier-illustration-2_fig2_323057610)
In simple terms, each feature acts as a binary classifier in a cascade filter. If an extracted feature from the image is passed through the classifier and it predicts that the image consists of that feature then it is passed on to the next classifier for next feature existence check otherwise it is discarded and next image is checked. This thereby decreases computation time as we have to check only some features in windows where the object is not present rather than checking all features. This is the main part of the algorithm that allows it to process videos at a rate of approximately 15 frames per second and enables real time implementation.

Implementation using OpenCV Library

References
Rapid Object Detection using a Boosted Cascade of Simple Features : https://web.iitd.ac.in/~sumeet/viola-cvpr-01.pdf
Detecting Faces (Viola Jones Algorithm) — Computerphile : https://www.youtube.com/watch?v=uEJ71VlUmMQ
Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems by Aurelien Geron
Stay tuned for new research papers’ explanation like this!

Feel free to connect and give ur suggestions :

Prev Tutorial: Cascade Classifier

Introduction
Working with a boosted cascade of weak classifiers includes two major stages: the training and the detection stage. The detection stage using either HAAR or LBP based models, is described in the object detection tutorial. This documentation gives an overview of the functionality needed to train your own boosted cascade of weak classifiers. The current guide will walk through all the different stages: collecting training data, preparation of the training data and executing the actual model training.

To support this tutorial, several official OpenCV applications will be used: opencv_createsamples, opencv_annotation, opencv_traincascade and opencv_visualisation.

Note
Createsamples and traincascade are disabled since OpenCV 4.0. Consider using these apps for training from 3.4 branch for Cascade Classifier. Model format is the same between 3.4 and 4.x.
Important notes
If you come across any tutorial mentioning the old opencv_haartraining tool (which is deprecated and still using the OpenCV1.x interface), then please ignore that tutorial and stick to the opencv_traincascade tool. This tool is a newer version, written in C++ in accordance to the OpenCV 2.x and OpenCV 3.x API. The opencv_traincascade supports both HAAR like wavelet features [226] and LBP (Local Binary Patterns) [133] features. LBP features yield integer precision in contrast to HAAR features, yielding floating point precision, so both training and detection with LBP are several times faster then with HAAR features. Regarding the LBP and HAAR detection quality, it mainly depends on the training data used and the training parameters selected. It's possible to train a LBP-based classifier that will provide almost the same quality as HAAR-based one, within a percentage of the training time.
The newer cascade classifier detection interface from OpenCV 2.x and OpenCV 3.x (cv::CascadeClassifier) supports working with both old and new model formats. opencv_traincascade can even save (export) a trained cascade in the older format if for some reason you are stuck using the old interface. At least training the model could then be done in the most stable interface.
The opencv_traincascade application can use TBB for multi-threading. To use it in multicore mode OpenCV must be built with TBB support enabled.
Preparation of the training data
For training a boosted cascade of weak classifiers we need a set of positive samples (containing actual objects you want to detect) and a set of negative images (containing everything you do not want to detect). The set of negative samples must be prepared manually, whereas set of positive samples is created using the opencv_createsamples application.

Negative Samples
Negative samples are taken from arbitrary images, not containing objects you want to detect. These negative images, from which the samples are generated, should be listed in a special negative image file containing one image path per line (can be absolute or relative). Note that negative samples and sample images are also called background samples or background images, and are used interchangeably in this document.

Described images may be of different sizes. However, each image should be equal or larger than the desired training window size (which corresponds to the model dimensions, most of the times being the average size of your object), because these images are used to subsample a given negative image into several image samples having this training window size.

An example of such a negative description file:

Directory structure:

/img
 img1.jpg
 img2.jpg
bg.txt
File bg.txt:

img/img1.jpg
img/img2.jpg
Your set of negative window samples will be used to tell the machine learning step, boosting in this case, what not to look for, when trying to find your objects of interest.

Positive Samples
Positive samples are created by the opencv_createsamples application. They are used by the boosting process to define what the model should actually look for when trying to find your objects of interest. The application supports two ways of generating a positive sample dataset.

You can generate a bunch of positives from a single positive object image.
You can supply all the positives yourself and only use the tool to cut them out, resize them and put them in the opencv needed binary format.
While the first approach works decently for fixed objects, like very rigid logo's, it tends to fail rather soon for less rigid objects. In that case we do suggest to use the second approach. Many tutorials on the web even state that 100 real object images, can lead to a better model than 1000 artificially generated positives, by using the opencv_createsamples application. If you however do decide to take the first approach, keep some things in mind:

Please note that you need more than a single positive samples before you give it to the mentioned application, because it only applies perspective transformation.
If you want a robust model, take samples that cover the wide range of varieties that can occur within your object class. For example in the case of faces you should consider different races and age groups, emotions and perhaps beard styles. This also applies when using the second approach.
The first approach takes a single object image with for example a company logo and creates a large set of positive samples from the given object image by randomly rotating the object, changing the image intensity as well as placing the image on arbitrary backgrounds. The amount and range of randomness can be controlled by command line arguments of the opencv_createsamples application.

Command line arguments:

-vec <vec_file_name> : Name of the output file containing the positive samples for training.
-img <image_file_name> : Source object image (e.g., a company logo).
-bg <background_file_name> : Background description file; contains a list of images which are used as a background for randomly distorted versions of the object.
-num <number_of_samples> : Number of positive samples to generate.
-bgcolor <background_color> : Background color (currently grayscale images are assumed); the background color denotes the transparent color. Since there might be compression artifacts, the amount of color tolerance can be specified by -bgthresh. All pixels within bgcolor-bgthresh and bgcolor+bgthresh range are interpreted as transparent.
-bgthresh <background_color_threshold>
-inv : If specified, colors will be inverted.
-randinv : If specified, colors will be inverted randomly.
-maxidev <max_intensity_deviation> : Maximal intensity deviation of pixels in foreground samples.
-maxxangle <max_x_rotation_angle> : Maximal rotation angle towards x-axis, must be given in radians.
-maxyangle <max_y_rotation_angle> : Maximal rotation angle towards y-axis, must be given in radians.
-maxzangle <max_z_rotation_angle> : Maximal rotation angle towards z-axis, must be given in radians.
-show : Useful debugging option. If specified, each sample will be shown. Pressing Esc will continue the samples creation process without showing each sample.
-w <sample_width> : Width (in pixels) of the output samples.
-h <sample_height> : Height (in pixels) of the output samples.
When running opencv_createsamples in this way, the following procedure is used to create a sample object instance: The given source image is rotated randomly around all three axes. The chosen angle is limited by -maxxangle, -maxyangle and -maxzangle. Then pixels having the intensity from the [bg_color-bg_color_threshold; bg_color+bg_color_threshold] range are interpreted as transparent. White noise is added to the intensities of the foreground. If the -inv key is specified then foreground pixel intensities are inverted. If -randinv key is specified then algorithm randomly selects whether inversion should be applied to this sample. Finally, the obtained image is placed onto an arbitrary background from the background description file, resized to the desired size specified by -w and -h and stored to the vec-file, specified by the -vec command line option.

Positive samples also may be obtained from a collection of previously marked up images, which is the desired way when building robust object models. This collection is described by a text file similar to the background description file. Each line of this file corresponds to an image. The first element of the line is the filename, followed by the number of object annotations, followed by numbers describing the coordinates of the objects bounding rectangles (x, y, width, height).

An example of description file:

Directory structure:

/img
 img1.jpg
 img2.jpg
info.dat
File info.dat:

img/img1.jpg 1 140 100 45 45
img/img2.jpg 2 100 200 50 50 50 30 25 25
Image img1.jpg contains single object instance with the following coordinates of bounding rectangle: (140, 100, 45, 45). Image img2.jpg contains two object instances.

In order to create positive samples from such collection, -info argument should be specified instead of -img:

-info <collection_file_name> : Description file of marked up images collection.
Note that in this case, parameters like -bg, -bgcolor, -bgthreshold, -inv, -randinv, -maxxangle, -maxyangle, -maxzangle are simply ignored and not used anymore. The scheme of samples creation in this case is as follows. The object instances are taken from the given images, by cutting out the supplied bounding boxes from the original images. Then they are resized to target samples size (defined by -w and -h) and stored in output vec-file, defined by the -vec parameter. No distortion is applied, so the only affecting arguments are -w, -h, -show and -num.

The manual process of creating the -info file can also been done by using the opencv_annotation tool. This is an open source tool for visually selecting the regions of interest of your object instances in any given images. The following subsection will discuss in more detail on how to use this application.

Extra remarks
opencv_createsamples utility may be used for examining samples stored in any given positive samples file. In order to do this only -vec, -w and -h parameters should be specified.
Example of vec-file is available here opencv/data/vec_files/trainingfaces_24-24.vec. It can be used to train a face detector with the following window size: -w 24 -h 24.
Using OpenCV's integrated annotation tool
Since OpenCV 3.x the community has been supplying and maintaining a open source annotation tool, used for generating the -info file. The tool can be accessed by the command opencv_annotation if the OpenCV applications where build.

Using the tool is quite straightforward. The tool accepts several required and some optional parameters:

--annotations (required) : path to annotations txt file, where you want to store your annotations, which is then passed to the -info parameter [example - /data/annotations.txt]
--images (required) : path to folder containing the images with your objects [example - /data/testimages/]
--maxWindowHeight (optional) : if the input image is larger in height then the given resolution here, resize the image for easier annotation, using --resizeFactor.
--resizeFactor (optional) : factor used to resize the input image when using the --maxWindowHeight parameter.
Note that the optional parameters can only be used together. An example of a command that could be used can be seen below

opencv_annotation --annotations=/path/to/annotations/file.txt --images=/path/to/image/folder/
This command will fire up a window containing the first image and your mouse cursor which will be used for annotation. A video on how to use the annotation tool can be found here. Basically there are several keystrokes that trigger an action. The left mouse button is used to select the first corner of your object, then keeps drawing until you are fine, and stops when a second left mouse button click is registered. After each selection you have the following choices:

Pressing c : confirm the annotation, turning the annotation green and confirming it is stored
Pressing d : delete the last annotation from the list of annotations (easy for removing wrong annotations)
Pressing n : continue to the next image
Pressing ESC : this will exit the annotation software
Finally you will end up with a usable annotation file that can be passed to the -info argument of opencv_createsamples.

Cascade Training
The next step is the actual training of the boosted cascade of weak classifiers, based on the positive and negative dataset that was prepared beforehand.

Command line arguments of opencv_traincascade application grouped by purposes:

Common arguments:
-data <cascade_dir_name> : Where the trained classifier should be stored. This folder should be created manually beforehand.
-vec <vec_file_name> : vec-file with positive samples (created by opencv_createsamples utility).
-bg <background_file_name> : Background description file. This is the file containing the negative sample images.
-numPos <number_of_positive_samples> : Number of positive samples used in training for every classifier stage.
-numNeg <number_of_negative_samples> : Number of negative samples used in training for every classifier stage.
-numStages <number_of_stages> : Number of cascade stages to be trained.
-precalcValBufSize <precalculated_vals_buffer_size_in_Mb> : Size of buffer for precalculated feature values (in Mb). The more memory you assign the faster the training process, however keep in mind that -precalcValBufSize and -precalcIdxBufSize combined should not exceed you available system memory.
-precalcIdxBufSize <precalculated_idxs_buffer_size_in_Mb> : Size of buffer for precalculated feature indices (in Mb). The more memory you assign the faster the training process, however keep in mind that -precalcValBufSize and -precalcIdxBufSize combined should not exceed you available system memory.
-baseFormatSave : This argument is actual in case of Haar-like features. If it is specified, the cascade will be saved in the old format. This is only available for backwards compatibility reasons and to allow users stuck to the old deprecated interface, to at least train models using the newer interface.
-numThreads <max_number_of_threads> : Maximum number of threads to use during training. Notice that the actual number of used threads may be lower, depending on your machine and compilation options. By default, the maximum available threads are selected if you built OpenCV with TBB support, which is needed for this optimization.
-acceptanceRatioBreakValue <break_value> : This argument is used to determine how precise your model should keep learning and when to stop. A good guideline is to train not further than 10e-5, to ensure the model does not overtrain on your training data. By default this value is set to -1 to disable this feature.
Cascade parameters:
-stageType <BOOST(default)> : Type of stages. Only boosted classifiers are supported as a stage type at the moment.
-featureType<{HAAR(default), LBP}> : Type of features: HAAR - Haar-like features, LBP - local binary patterns.
-w <sampleWidth> : Width of training samples (in pixels). Must have exactly the same value as used during training samples creation (opencv_createsamples utility).
-h <sampleHeight> : Height of training samples (in pixels). Must have exactly the same value as used during training samples creation (opencv_createsamples utility).
Boosted classifier parameters:
-bt <{DAB, RAB, LB, GAB(default)}> : Type of boosted classifiers: DAB - Discrete AdaBoost, RAB - Real AdaBoost, LB - LogitBoost, GAB - Gentle AdaBoost.
-minHitRate <min_hit_rate> : Minimal desired hit rate for each stage of the classifier. Overall hit rate may be estimated as (min_hit_rate ^ number_of_stages), [227] §4.1.
-maxFalseAlarmRate <max_false_alarm_rate> : Maximal desired false alarm rate for each stage of the classifier. Overall false alarm rate may be estimated as (max_false_alarm_rate ^ number_of_stages), [227] §4.1.
-weightTrimRate <weight_trim_rate> : Specifies whether trimming should be used and its weight. A decent choice is 0.95.
-maxDepth <max_depth_of_weak_tree> : Maximal depth of a weak tree. A decent choice is 1, that is case of stumps.
-maxWeakCount <max_weak_tree_count> : Maximal count of weak trees for every cascade stage. The boosted classifier (stage) will have so many weak trees (<=maxWeakCount), as needed to achieve the given -maxFalseAlarmRate.
Haar-like feature parameters:
-mode <BASIC (default) | CORE | ALL> : Selects the type of Haar features set used in training. BASIC use only upright features, while ALL uses the full set of upright and 45 degree rotated feature set. See [135] for more details.
Local Binary Patterns parameters: Local Binary Patterns don't have parameters.
After the opencv_traincascade application has finished its work, the trained cascade will be saved in cascade.xml file in the -data folder. Other files in this folder are created for the case of interrupted training, so you may delete them after completion of training.

Training is finished and you can test your cascade classifier!

Visualising Cascade Classifiers
From time to time it can be useful to visualise the trained cascade, to see which features it selected and how complex its stages are. For this OpenCV supplies a opencv_visualisation application. This application has the following commands:

--image (required) : path to a reference image for your object model. This should be an annotation with dimensions [-w,-h] as passed to both opencv_createsamples and opencv_traincascade application.
--model (required) : path to the trained model, which should be in the folder supplied to the -data parameter of the opencv_traincascade application.
--data (optional) : if a data folder is supplied, which has to be manually created beforehand, stage output and a video of the features will be stored.
An example command can be seen below

opencv_visualisation --image=/data/object.png --model=/data/model.xml --data=/data/result/
Some limitations of the current visualisation tool

Only handles cascade classifier models, trained with the opencv_traincascade tool, containing stumps as decision trees [default settings].
The image provided needs to be a sample window with the original model dimensions, passed to the --image parameter.
Example of the HAAR/LBP face model ran on a given window of Angelina Jolie, which had the same preprocessing as cascade classifier files –>24x24 pixel image, grayscale conversion and histogram equalisation:

A video is made with for each stage each feature visualised:


Skip to content
MathWorks
Products
Solutions
Academia
Support
Community
Events

Stateflow
Search MathWorks.com
Search MathWorks.com
 

Stateflow
Model and simulate decision logic using state machines and flow charts
Have questions? Contact Sales.

Stateflow provides a graphical language that includes state transition diagrams, flow charts, state transition tables, and truth tables. You can use Stateflow to describe how MATLAB algorithms and Simulink models react to input signals, events, and time-based conditions. 

Stateflow enables you to design and develop supervisory control, task scheduling, fault management, communication protocols, user interfaces, and hybrid systems. 

With Stateflow, you model combinatorial and sequential decision logic that can be simulated as a block within a Simulink model or executed as an object in MATLAB. Graphical animation enables you to analyze and debug your logic while it is executing. Edit-time and run-time checks ensure design consistency and completeness before implementation.

Get Started:
Design Control Logic
Execute and Debug Charts
Develop Reusable Logic for MATLAB Applications
Schedule Simulink Algorithms
Validate Designs and Generate Code
What Is Stateflow? 2:06Video length is 2:06
What Is Stateflow?

FREE INTERACTIVE COURSE

Stateflow Onramp
Get started
Design Control Logic
Model system logic using state machines, flow charts, and truth tables.

Designing State Machines Graphically
Build state machines graphically by drawing states and junctions connected by transitions. You can also create functions using flow chart notation, Simulink subsystems, MATLAB code, and truth tables.

Represent Operating Modes by Using States
Simulink Subsystems as States
Reuse MATLAB Code by Defining MATLAB Functions
Getting Started with Stateflow (12:48)
Stateflow diagram defining the logic for a boiler temperature control system. The diagram uses graphical functions (right side) to implement utility algorithms called by the heater system (left side).
Stateflow diagram defining the logic for a boiler temperature control system. The diagram uses graphical functions (right side) to implement utility algorithms called by the heater system (left side).
Designing Flow Charts
Create flow charts by drawing transitions that are connected at junctions. The Pattern Wizard lets you create commonly used logic flow patterns. You can use flow charts to design logic for transitioning between states.

Flow Charts in Stateflow
Create Flow Charts by Using Pattern Wizard
Stateflow flow chart
Represent combinatorial logic such as decision trees and iterative loops graphically with flow charts.

Designing Logic with Tables
Truth tables in Stateflow let you model logic in Simulink when the output depends purely on the current input. State transition tables provide a structured environment for modeling state machines in Simulink.

Reuse Combinatorial Logic by Defining Truth Tables
State Transition Tables in Stateflow
State Transition Tables (4:53)
Truth table implementing the logic for selecting a valid sensor reading in a fault-detection algorithm.
Truth table implementing the logic for selecting a valid sensor reading in a fault-detection algorithm.
Execute and Debug Charts
Visualize the behavior of your system for analysis and debugging.

Executing Stateflow Charts
Visualize system behavior using state diagram animations to highlight active states and transitions in your charts.

Execution of a Stateflow Chart
Types of Chart Execution
Understanding Complex Logic with Stateflow

 2:59Video length is 2:59
Understanding Complex Logic with Stateflow
Debugging Stateflow Charts
Stateflow debugging capabilities let you step through chart execution in detail. You can set breakpoints, monitor data values, and step through different functions in your state diagrams.

Set Breakpoints to Debug Charts
Detect Modeling Errors During Edit Time
Analyzing and Debugging Logic with Stateflow (4:00)
Simulation data visualization options in Stateflow. Top left: Simulink Data Inspector for comparing specific signals; bottom left: custom MATLAB interface for analyzing data; right: Simulink Signal Selector for comparing specific states.
Simulation data visualization options in Stateflow. Top left: Simulink Data Inspector for comparing specific signals; bottom left: custom MATLAB interface for analyzing data; right: Simulink Signal Selector for comparing specific states.

Develop Reusable Logic for MATLAB Applications
Use Stateflow chart objects to develop reusable logic for MATLAB applications. Design state machine and timing logic for a wide range of applications, including test and measurement, autonomous systems, signal processing, and communications.

Reusable Chart Objects
Create standalone Stateflow charts that use the full capabilities of the MATLAB language in state and transition actions. Use these charts as MATLAB objects in your applications that require state machine and timing logic.

Create Stateflow Charts for Execution as MATLAB Objects
Execute and Unit Test Stateflow Chart Objects
Debug a Standalone Stateflow Chart
State Machine and Timing Logic
Accelerate the development of MATLAB applications by using Stateflow to graphically design state machine and timing logic that would be difficult to implement textually.

Design Human-Machine Interface Logic by Using Stateflow Charts
Model a Communications Protocol by Using Chart Objects
Analog Trigger App by Using Stateflow Charts
Using Stateflow to Provide the Logic for a MATLAB App

 2:07Video length is 2:07
Using Stateflow to Provide the Logic for a MATLAB App
Deploying Stateflow Applications
Create MATLAB applications that include Stateflow chart objects and share them without requiring Stateflow.

Share MATLAB applications that include Stateflow charts without needing a Stateflow license.
Share MATLAB applications that include Stateflow chart objects with users who do not have Stateflow. 

Schedule Simulink Algorithms
Schedule algorithms modeled in Simulink.

Periodic and Continuous Scheduling
You can model conditional, event-based, and time-based logic in Stateflow to invoke Simulink algorithms in a periodic or continuous manner. Orchestrate the execution of components to simulate the scheduling of your real-time environment.

Transition Between Operating Modes
Continuous-Time Modeling in Stateflow
Synchronize Model Components by Broadcasting Events
You can model logic in Stateflow to call Simulink and MATLAB algorithms in a periodic or continuous manner.
You can model logic in Stateflow to call Simulink and MATLAB algorithms in a periodic or continuous manner.

Temporal Operators
Use event-based and time-based operators (such as after and duration) to specify state-transition logic based on event counts, elapsed time, and denoised signals without having to create and maintain your own timers and counters.

Control Chart Execution by Using Temporal Logic
Control Oscillations Using the Duration Operator
Temporal Logic Operators

 0:53Video length is 0:53
Temporal Logic Operators
Validate Designs and Generate Code
Validate your design against requirements and generate code for implementation on your embedded system.

Validating Designs
Use Stateflow with other Simulink products to validate your design against requirements.

Link requirements directly to Stateflow objects using drag and drop with Requirements Toolbox.
Check that your state diagrams comply with standards using Simulink Check.
Collect model and generated code coverage metrics with Simulink Coverage.
Detect design errors and generate test vectors using Simulink Design Verifier.
Develop, manage, and execute simulation-based tests with Simulink Test.
Stateflow and Model Slicer
Highlight Active Logic using Model Slicer.
Generating Code
Generate code for implementation of your Stateflow logic on embedded systems.

Generate C and C++ code from Simulink and Stateflow models using Simulink Coder.
Generate VHDL and Verilog code for FPGA and ASIC designs with HDL Coder.
Generate IEC 61131-3 Structured Text for PLCs and PACs using Simulink PLC Coder.
Generate code to implement Stateflow logic. 
Generate code to implement Stateflow logic. 

Product Resources:
      
Get a Free Trial
30 days of exploration at your fingertips.

Ready to Buy?
Get pricing information and explore related products.

Are You a Student?
Your school may already provide access to MATLAB, Simulink, and add-on products through a campus-wide license.

What's Next?
Panel Navigation

Stateflow Onramp
Learn the basics of simulating physical systems in Simscape


FREE TUTORIALS

Self-Paced Online Courses


RELEASE HIGHLIGHTS

What’s New in the Latest Release of MATLAB and Simulink

MathWorks

Accelerating the pace of engineering and science

MathWorks is the leading developer of mathematical computing software for engineers and scientists.

Discover…

Explore Products

MATLAB
Simulink
Student Software
Hardware Support
File Exchange
Try or Buy

Downloads
Trial Software
Contact Sales
Pricing and Licensing
How to Buy
Learn to Use

Documentation
Tutorials
Examples
Videos and Webinars
Training
Get Support

Installation Help
MATLAB Answers
Consulting
License Center
Contact Support
About MathWorks

Careers
Newsroom
Social Mission
Customer Stories
About MathWorks
Select a Web SiteUnited States
Trust Center
Trademarks
Privacy Policy
Preventing Piracy
Application Status
© 1994-2023 The MathWorks, Inc.

FacebookTwitterInstagramYouTubeLinkedInRSS
Join the conversation
Control of Semiconductor Switches
via PWM Technologies
Control of the semiconductor switches is the most efficient and convenient means to achieve the control
of power converters and machine drives. They act as the actuators in the implementation of the control
systems where the manipulated control inputs in the form of three phase voltage signals are realized by
turning on and off the semiconductor switches.
Depending on the application, a large variety of power electronic devices, different in types of semiconductor switches, construction topologies and concepts, have been developed. The common functionality
of these devices is to conduct power flow by varying the on–off duration of each switch. Among them,
the 2-Level Voltage Source Inverter (2L-VSI), as shown in Figure 2.1, is the most widely adopted mechanism to control three-phase AC machines. A similar topology is illustrated in the previous chapter for
the power converter (see Figure 1.9). Based on a DC power supply for controlling AC motors, the primary concern of using the semiconductor switches for the applications presented in this book is to create
sinusoidal phase voltage signals via the fundamental components of rectangular signals that are produced by varying magnitudes and frequencies through turning on–off each power switch for a duration
of time. A similar operational principle applies to the voltage source power converter (see Figure 1.9).
The command signal for turning on–off each power switch is called a gate signal.
There are two approaches used in this book to generate the gate signal for the semiconductor switches.
The first approach uses Pulse Width Modulation (PWM) based on which the PID controllers (see
Chapters 3 to 5) and the traditional model predictive controllers (see Chapters 8 to 9) are implemented.
In the control applications, the control signals calculated are the three phase voltage signals that are
obtained from one of the controller designs using the model either in the d − q reference frame or in
𝛼 − 𝛽 reference frame. The role of the voltage source inverter with the power electronics devices is
to realize the three phase voltage control signals as closely as possible. Namely, the sinusoidal phase
voltage signals contained as fundamental components of rectangular wave signals created by turning
on–off each power switch with the PWM technologies are aimed to be closely matched with the
three phase voltage control signals. The second methodology features a much simpler approach in the
implementation of the control systems that generates such a gate signal by direct optimization of an
error function between the desired control signals and those achieved by the semiconductor switches
(see Chapters 6 to 7). In this second approach, there is no need to use the PWM technology, therefore it
significantly reduces the complexity of controlling the semiconductor switches.
PWM technology, originally developed in the telecommunication engineering community, has gained
wide popularity and has been the subject of intensive research investigations in the control of power
PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®, First Edition.
Liuping Wang, Shan Chai, Dae Yoo, Lu Gan and Ki Ng.
© 2015 John Wiley & Sons Singapore Pte Ltd. Published 2015 by John Wiley & Sons Singapore Pte Ltd.
Companion Website: www.wiley.com/go/wang/pid28 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
electronics over the past several decades (for example, Holtz, 1992, Holmes and Lipo, 2003). With PWM
technology and a voltage source inverter, the efficient control of the AC motor position, speed and torque
by Variable Speed Drive (VSD) becomes possible. Power converters are also controlled by the PWM
technology. A wide variety of PWM generation techniques has been developed, which could be categorized into two broad classes: continuous PWM (CPWM) and discontinuous PWM (DPWM) (Hava
et al., 1999, Zhou and Wang, 2002). Compared with DPWM, CPWM attains superior performance in
the low modulation index range which is within the operating conditions of servo drives and the power
converters for most of the time. Therefore, it has gained more popularity in servo drive and converter
applications. These methods could be implemented by two methodologies: carrier based PWM and direct
digital implementation. For a long time, the carrier based PWM implementation, such as Sine-Triangle
intersection techniques, has dominated the industrial applications. With the development of fast Digital
Signal Processors (DSPs), direct digital implementation has also gained popularity in more recent years,
of which the Space Vector Modulator (SVM) has been well recognized.
In Section 2.1, the topology of a two-level, voltage source inverter is introduced, where the relationship
between the semiconductor power switches and the three phase sinusoidal control signal is established.
The remainder of this chapter discusses how to manage the semiconductor power switches so that the
three phase sinusoidal control signal can be reconstructed. In Section 2.2, the six-step mode is introduced. In Section 2.3, several carrier based PWM techniques are discussed, among which zero-sequence
techniques are used to improve the modulation index. Section 2.4 discusses the space vector modulation,
which has a direct digital implementation. In Section 2.5, a simulation study of the effect of PWM is
conducted to reveal the current ripples.
2.1 Topology of IGBT Inverter
One of the most widely adopted semiconductor power switches for the medium-range power converter/inverter is the Insulated Gate Bipolar Transistor (IGBT), which offers the benefits of both
MOSFET and bipolar switches. However, IGBT can only allow the current to flow in one direction, and
hence a freewheeling diode in parallel is required to conduct the current flow in the opposite direction.
In the three-phase 2L-VSI for controlling AC machine, as shown in Figure 2.1, each leg of the inverter
has two pairs of such a combination consisting of an IGBT switch and a freewheeling diode, where their
middle point is linked to the loads, either a passive load or an AC motor. Here, the front-end rectifier is
replaced by two DC sources connected in series and each supply offers half of the total DC bus voltage
(Vdc∕2). For the simplification of analysis, it is sufficient to assume that the middle point (denoted
by O in Figure 2.1) between two DC sources is referred to the ground. Thus, all the voltages can be
represented with respect to the ground. For example, the neutral point voltage of AC motor is referred
to as 𝑣
n with respect to the ground.
Vdc
Vdc
2 2
C
S
a
S
a
Sb
Sb
S
c
S
c
L R
Vn
Va
Vb
Vc
o
R R
L L
Figure 2.1 Topology of three-phase-leg IGBT.Control of Semiconductor Switches via PWM Technologies 29
As seen in Figure 2.1, there are two IGBT switches for each of the three legs. Within each leg of
an inverter, only one switch is allowed to turn on (denoted by “1”) while the other is off (denoted by
“0”) at any given time to prevent short circuit. Thus, the switching states of the inverter can be identified by only considering the states of the three upper switches. With the states of three upper switches
denoted as S
i (i = a, b, c), the states of their corresponding lower switches can be represented by their
negation Si (i = a, b, c). As a result, there are only eight possible switching states by turning on and off
all the switches in the inverter. Since the states of upper and lower switches within the same leg are
complementary to each other, all eight switching states can be independently identified by the states of
the three upper switches, as listed in Table 2.1. Among these, two switching states (− V→0 and − V→7), which
represent the cases where either all the upper or all the lower switches are turned on, leading to an
open circuit, are called zero vector. In contrast, the other six states that form a closed circuit, are called
active vector.
When the ith upper switch is on, that is Si = 1 and Si = 0, the output of the corresponding phase leg
is connected to the top rail of the supplies and thus 𝑣i = V2dc . Conversely, when the lower switch is on,
the output is connected to the bottom rail of the supplies and hence 𝑣i = − V2dc . Corresponding to the
switching states in Table 2.1, the resulting output voltages 𝑣a, 𝑣b, 𝑣c, are summarized in Table 2.2.
Equivalently, the output voltages could be represented in terms of their switching states,
𝑣
i = VdcSi −
V
dc
2
(i = a, b, c). (2.1)
Note that the output voltages are expressed with respect to the ground defined before. It follows that the
three-phase voltages with respect to the neutral point of the load are obtained by
𝑣
an = 𝑣a − 𝑣n
𝑣
bn = 𝑣b − 𝑣n (2.2)
𝑣
cn = 𝑣c − 𝑣n.
From (2.1), it is seen that the three voltage signals 𝑣a, 𝑣b and 𝑣c generated from the voltage source
inverter via the semiconductor switches are in rectangular-wave forms with amplitude changes between
Table 2.1 Switching states of inverter
−→V0
−→V1
−→V2
−→V3
−→V4
−→V5
−→V6
−→V7
S
a 0 1 1 0 0 0 1 1
S
b 0 0 1 1 1 0 0 1
S
c 0 0 0 0 1 1 1 1
Table 2.2 Output voltage of inverter
−→V0
−→V1
−→V2
−→V3
−→V4
−→V5
−→V6
−→V7
𝑣
a −
V
dc
2
V
dc
2
V
dc
2
−
V
dc
2
−
V
dc
2
−
V
dc
2
V
dc
2
V
dc
2
𝑣
b −
V
dc
2
−
V
dc
2
V
dc
2
V
dc
2
V
dc
2
−
V
dc
2
−
V
dc
2
V
dc
2
𝑣
c −
V
dc
2
−
V
dc
2
−
V
dc
2
−
V
dc
2
V
dc
2
V
dc
2
V
dc
2
V
dc
230 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
±
Vdc
2
and, therefore, their fundamental components are of primary interest in the realization of the three
phase control signals.
2.2 Six-step Operating Mode
One of early technologies to control the power electronic switches is called six-step mode (see Holtz
(1992)). In this approach, the maximum magnitude of the fundamental with a 2L-VSI is achieved by
sequentially switching the six active vectors, that is starting from − V→1, to − V→2 and the on to − V→6 within one
electrical cycle. For one cycle of the six-step operation, each output voltage 𝑣i (i = a, b, c) will have half
of cycle connect to top rail of inverter and the other half bottom rail. As an example, Figure 2.2 illustrates
the rectangular wave of 𝑣c over one cycle and its associated fundamental where the period of the wave is
2𝜋
𝜔
e
. Since this rectangular signal 𝑣c is an odd function, it can be approximated using a Fourier sine series
as (see Kreyszig (2010)) where
𝑣
c(t) = b1 sin 𝜔et + b3 sin 3𝜔et + · · · + bn sin n𝜔et + · · · ,
where n = 1, 3, 5, … , is an odd number and the coefficient for the nth index is
b
n =
2𝜔
e
𝜋 ∫
𝜋 𝜔e
0
V
dc
2
sin n𝜔
etdt
=
2
n𝜋
V
dc. (2.3)
The fundamental component of the Fourier sine series b1 sin 𝜔et is the approximated sinusoidal output
of the voltage source inverter for the third phase of the voltage and is the function to be closely matched
with the control signal at this phase. The rest of the components in the Fourier sine series are the harmonics. The Fourier coefficient b
1 =
2 𝜋
V
dc represents the amplitude of the fundamental component of
the rectangular wave 𝑣c(t) generated by one cycle of the six step operation. This amplitude of the fundamental voltage of six-step mode limits the maximum amplitude of the achievable modulation signal.
In other words, the maximum amplitude of the sinusoidal voltage signals that is achievable when using
the six-step mode is 2
𝜋
V
dc. From a control system design point of view, this means that if the calculated
control signal had an amplitude that exceeded this value, then the output of the inverter would not be
able to fully realize it.
One of the drawbacks for the six-step mode technique is that the amplitudes of the harmonics decay
in a linear way proportional to the index 1
n
. In particular, the third harmonic still has a relatively large
amplitude (see (2.3)) that only becomes one third of the fundamental component. From a control system
perspective, the harmonics become part of the noise and disturbances of the system. Therefore, their
amplitudes should be minimized. This means that the six-step mode technique will create large noise
effects and disturbances in the control system.
–Vdc/2
0
–π/ω
e
+π/ω
e
Vdc/2
Figure 2.2 Output 𝑣c of six-step mode.Control of Semiconductor Switches via PWM Technologies 31
2.3 Carrier Based PWM
Use of the six-step mode to control the power electronic switches is relatively simple in implementation;
however, this approach has large harmonic components relative to the amplitude of the fundamental
component of the output voltage signal, which is 2
𝜋
V
dc. To reduce the effects of the harmonics, carrier
based Pulse Width Modulation (PWM) techniques with zero sequence injection are introduced.
In general, the carrier based PWM technique is to compare the amplitude of an input voltage signal
with that of a carrier signal, usually a periodic triangular signal with frequency fc. If the input voltage
signal (say phase A voltage, 𝑣∗ a) is larger than the carrier in amplitude, the switch function Sa outputs high
level (logic “1”) and otherwise low level (logic “0”), leading to the actual voltage output calculated as
𝑣
a = VdcSa −
V
dc
2
.
The carrier frequency (denoted by fc) is usually chosen to be much higher than the fundamental frequency (denoted by f1) of the voltage signal and their ratio is the multiple of 3 for better reduction of
Total Harmonic Distortion (THD) (see Holmes and Lipo (2003)). For example, a ratio
fs f1
= 21
could be used for this purpose.
2.3.1 Sinusoidal PWM
As one of the earliest PWM generation techniques (see Bowes, 1975), the Sinusoidal PWM (SPWM)
generates the digital pulses to control the IGBT switches by directly comparing the three-phase voltages
with the carrier, commonly a periodic triangular waveform. It was popular due to its simplicity and
feasibility to be implemented by analog circuits. As an example to demonstrate the working principle of
sinusoidal PWM technique, a carrier signal and three phase sinusoidal signals are shown in Figure 2.3
where the fundamental frequency of the sinusoidal signals is f1 and the carrier frequency is fc, and fc
f1
= 21.
The carrier signal has amplitude of ± Vdc
2
, where Vdc is the DC voltage of the power supply for the control
application of AC drives or Vdc as the DC voltage output for the control application of power converter. In
this illustration, there are 21 cycles of the carrier signal and one cycle of the three phase sinusoidal signals
(see Figure 2.3(a)). When comparing the three phase sinusoidal signals with the carrier signal, the values
of S
a, Sb and Sc are taken either 1 or 0, depending on whether the corresponding sinusoidal function is
greater or smaller than the carrier signal. The actual output voltages are in rectangular waveforms (see
Figure 2.3(b)) with their magnitudes being ± Vdc
2
and their switches dependent on the values of Sa, Sb
and S
c:
𝑣
i = VdcSi −
V
dc
2
(i = a, b, c).
The fundamental components of the three rectangular waves in Figure 2.3(b) are found through Fourier
series analysis and are shown in the corresponding plots. It is seen that they have similar characteristics
as the input sinusoidal signals shown in Figure 2.3(a).
However, for the modulation to work within the linear range where its fundamental resembles the
desired signal, it requires that the maximum amplitude of the sinusoidal signal be less than Vdc
2
. Otherwise,
the exceeding parts will cause the switch states stay at either “1” or “0” and thus the fundamentals lose
their linear relationship to the desired reference voltage signals.
As a result, the controller output voltage should limit its value within ± Vdc
2
to avoid the nonlinear
modulation region if the sinusoidal PWM technique is used. The maximum modulation range could be
improved by zero-sequence injection, which has led to several different modulation schemes as introduced below.32 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
Vdc/2
–Vdc/2
–Vdc/2
–Vdc/2
Vdc/2
Vdc/2
–Vdc/2
Vdc/2
Sine-triangle intersection
(a)
(b)
Figure 2.3 Illustration of sinusoidal PWM. Three phase voltage signals in (a) are reconstructed with Pulse-Width
Modulation (b). (a) Sine-triangle intersection. (b) Pulses and fundamental sinusoidal signals from Fourier analysis.
Top figure corresponds to solid-line wave in (a); middle figure corresponds to dashed-line wave in (a); bottom figure
corresponds to dotted-line wave in (a).
2.3.2 Carrier Based PWM with Zero-sequence Injection
To understand the carrier based PWM with zero-sequence injection, the existence of a zero sequence is
examined. For a three-phase system, let
𝑣∗
a = 𝑣m sin(𝜔et)
𝑣∗
b = 𝑣m sin (𝜔et − 23𝜋 ) (2.4)
𝑣∗
c = 𝑣m sin (𝜔et + 23𝜋 )Control of Semiconductor Switches via PWM Technologies 33
+ +
+ +
+ +
Zero-sequence
S
a
Sb
S
c
≥ ≥ ≥
v
*a
vb*
v
*c
Figure 2.4 Carrier based PWM with zero sequence injection (Hava et al., 1999).
denote the desired reference voltages that will be closely approximated by the outputs of the voltage
source inverter. The neutral point voltage (𝑣n) with respect to ground, shown in Figure 2.1, can be represented by
𝑣
n =
𝑣∗
a + 𝑣∗ b + 𝑣∗ c
3
. (2.5)
If the desired three-phase voltage, 𝑣∗ a, 𝑣∗ b and 𝑣∗ c, are balanced, it means that the neutral point voltage 𝑣n
equals 0. Because most balanced three-phase motors are three-wired systems with the isolated neutral
point, there is the freedom of adding a nonzero value voltage to the neutral point voltage (𝑣n), leading to
the modified three desired voltage signals,
𝑣∗∗
a = 𝑣∗ a + 𝑣n
𝑣∗∗
b = 𝑣∗ b + 𝑣n (2.6)
𝑣∗∗
c = 𝑣∗ c + 𝑣n.
Then, based on the modified three desired voltage signals, the sinusoidal PWM technique explained in the
previous section can be applied. Figure 2.4 illustrates the operation of the carrier based PWM technique
with zero-sequence injection, which shows that the same “zero sequence” is added to all three reference
voltage signals and these signals are compared with the carrier signal to produce the switching signals
S
a, Sb and Sc.
There are many approaches in choosing the zero-sequence signal, leading to a variety of carrier based PWM schemes in the literatures (see Hava et al., 1999). Amongst them, a commonly
encountered selection of the zero-sequence signal for injection is the third harmonic injection PWM
technique.
Third Harmonics Injection PWM (THIPWM) exploits the third harmonic component of the desired
reference signal as the injection signal. There are two types of injections with different amplitudes of the
third harmonic. With the desired reference voltage defined in (2.4), the THIPWM 1∕6 with one sixth of
the reference amplitude is given by
𝑣
n =
1 6
𝑣
m sin(3𝜔et).
Similarly, the THIPWM 1∕4 is given by
𝑣
n =
1 4
𝑣
m sin(3𝜔et).34 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
Figure 2.5 Waveform of THIPWM 1/6. Key: dashed-line original sinusoidal wave; dotted-line 1
6 𝑣m sin(3𝜔et);
solid-line sinusoidal wave with third harmonic injection.
With the third harmonics injection, the peak value of modified desired sinusoidal is reduced by the
injection, as the waveform in Figure 2.5 demonstrates.
It can shown that the maximum amplitude of the three phase voltage signals is reduced to
2√
3
V
dc
2
=
1√
3
V
dc
if the THIPWM1∕6 is used and to
3√3
√7
V
dc
2
if the THIPWM1∕4 technique is used.
The ratio between the maximum amplitude of the three phase voltage signals and the value Vdc
2
is called
the modulation index, which is
m∗
max−THIPWM1∕6 =
2∕√3Vdc∕2
V
dc∕2 = 1.1547 (2.7)
for the THIPWM 1∕6 and
m∗
max−THIPWM1∕4 =
3√3∕√7Vdc∕2
V
dc∕2 = 1.1223 (2.8)
for the THIPWM 1∕4. Both modulation indices indicate that the linear modulation range is larger than
the one generated by the original sinusoidal PWM.
Note that THIPWM1/4 is derived for the purpose of the minimization of the Total Harmonic Distortion
(THD), whereas THIPWM1/6 is designed based on maximizing the linear modulation range (see Bowes
and Lai (1997)).
Now, with the Third Harmonic Injection technique, for the modulation to work within the linear range
where its fundamental resembles the desired signal, it requires that the maximum amplitude of the
sinusoidal signal be less than √13 Vdc or 3√√73 V2dc depending on whether THIPWM1∕6 or THIPWM1∕4
is being used. Basically, the modulation index will quantify the linear modulation range of the
three phase voltage signals with respect to the results from the original sinusoidal PWM. The linear
modulation range is translated into operational constraints from the perspective of controller design in
later chapters.Control of Semiconductor Switches via PWM Technologies 35
2.4 Space Vector PWM
Space Vector PWM (SVPWM), as its name conveys, utilizes the concept of space vector and its geometrical features to derive the on-off time duration for each switch. Similar to the definition of MMF space
vector in (1.12), the space vector of three-phase reference voltage is defined as
−→∗Vs
=
2 3
(𝑣∗ an + 𝑣∗ bnej 23𝜋 + 𝑣∗ cnej 43𝜋 ) . (2.9)
If a balanced three-phase voltage is employed, then − V→s∗ is a rotating vector with electrical speed 𝜔e, which
is the frequency of sinusoidal signal.
The modulation of the desired space vector − V→s∗ is obtained by the time average of its two nearest active
vectors and a zero vector, either − V→0 or − V→7. Taking the first sector for example, as illustrated in Figure 2.6,
−→∗Vs
could be modulated with the time average of the active vector − V→1 and − V→2 within one sampling period
Ts
by
Ts
−→∗Vs
= T
1
−→V1
+ T
2
−→V2
(2.10)
where T
1 and T2 are the duration of on-time for the active vectors − V→1 and − V→2 respectively. The relationship
between the modulated vector − V→∗
s and two nearest active vectors is obtained by applying the geometric
properties of the triangle,
Ts
⋅ |− V→s∗|
sin ( 23𝜋 ) =
T1
⋅ |− V→1|
sin ( 𝜋3 − 𝜃) =
T2
⋅ |− V→2|
sin(𝜃)
(2.11)
that implies the duty cycle ratio of each active vector is
T
1
Ts
=
|− V→s∗|
2 3
V
dc
⋅
sin ( 𝜋3 − 𝜃)
sin ( 23𝜋 ) =
|− V→s∗|
V
dc∕√3
⋅ sin (𝜋3 − 𝜃) (2.12)
T2 Ts
=
|− V→s∗|
2 3
V
dc
⋅
sin(𝜃)
sin ( 23𝜋 ) =
|− V→s∗|
V
dc∕√3
⋅ sin(𝜃), (2.13)
V5(100) V6(101)
V4(110)
V3(010) V2(011)
V1(001)
|Vi| = 2 3Vdc
V0(000)
V7(111)
|V
max| = 1
3 Vdc
T1
θ
T2 Vs
Ts
Vs
= T1V0 + T2V2
T
0 + T7 = Ts − T1 − T2
*
Figure 2.6 Principle of SVM.36 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
where the length of each active vector |→− Vi| = 2 3Vdc is used. It follows that the duration of the zero vector
applied is the remaining time of the sampling period,
T0
+ T
7 = Ts − T1 − T2. (2.14)
Since the hexagon has sixfold symmetry, the geometrical method discussed above can be used for the
other five sectors as well by rotating the modulation vector by (m − 1) 𝜋
3
rads with m = 1 · · · 6 denoting
the sector in which it locates. The conventional SVPWM symmetrically distributes the four switching
vectors, two active vectors and two zero vectors, within one sampling time, as shown in Figure 2.7.
Such an arrangement offers the benefits of fixed switching frequency and better harmonics reduction
performance.
Figure 2.7 shows a digital implementation of SVPWM within one sampling period for the example
illustrated in Figure 2.6. There are four switching vectors denoted by − V→0, − V→1, − V→2, − V→7 corresponding to
the four on-time periods, T0, T1, T2 and T7 calculated using (2.12)–(2.13). The arrangement of the four
switching vectors is shown in the top part of Figure 2.7, together with the three switching states Sa, Sb
and S
c. It is seen that the arrangement begins with the zero vector − V→0 and ends with the zero vector − V→7 for
the first half, and symmetrical with the second half of the graph. This forms a symmetrical pattern from
the center of the graph. The individual on-time period is also illustrated in the top part of Figure 2.7.
The bottom part of Figure 2.7 illustrates how SPVPWM is implemented. In the direct digital implementation, a parameter MAX is selected and a internal counter is set to count up and down within one sampling
period Ts to form two straight lines as illustrated, which can be described by the linear equations:
PWM
time =
2MAX
Ts
t 0 ≤ t <
Ts 2
(2.15)
PWM
time = 2MAX − 2MAX
Ts
t
Ts 2
≤ t < T
s. (2.16)
In the bottom part of Figure 2.7, on the vertical axis marked are the parameter MAX, and the three
IGBT switching counts PWMS
a
, PWMSb, PWMSc. With the on-time periods T0, T1, T2 and T7 calculated,
MAX
Ts
0
P W MSc
P W MSa
P W MSb
S
a
Sb
S
c
V0 V1 V2 V7 V7 V2 V1 V0
T02
T
1
2
T22
T72
T72
T22
T
1
2
T02
Figure 2.7 Digital implementation of SVPWM.Control of Semiconductor Switches via PWM Technologies 37
the corresponding count at which the IGBT switch turn-on period is calculated using the following
ratios:
PWM
S
a
MAX
=
T0 Ts
PWM
Sb
MAX
=
T0
+ T
1
Ts
(2.17)
PWM
S
c
MAX
=
T0
+ T
1 + T2
Ts
.
In this example, at the beginning of the sample period, Sa = Sb = Sc = 0. When PWMtime > PWMS
a
and
PWM
time is increasing, the switching state Sa = 1. Similarly, when PWMtime > PWMSb and PWMtime is
increasing, the switching state Sb = 1. The same conditions apply to the switching state Sc. After the
time t reaches Ts
2
, the operations continue in reverse. Namely, when PWMtime < PWMS
c
and PWM
time
is decreasing, the switching state Sc = 0. The same conditions apply to the switching states Sb and Sa
to complete one cycle of space vector implementation of PWM. The maximum amplitude of the three
phase voltage signals to be realized by the space vector modulation technique is seen from Figure 2.6,
which is √13 Vdc. Thus, the modulation index is calculated as
m∗
SVPWM =
2√
3
= 1.155. (2.18)
To ensure that the modulation is within the linear modulation range, it requires that voltages in the 𝛼 − 𝛽
reference frame satisfy
√𝑣𝛼(t)2 + 𝑣𝛽(t)2 ≤ √13 Vdc. (2.19)
This is based on the definition of the voltage space vector in relation to the voltage variables in 𝛼 − 𝛽
reference frame (see (1.22)). Similarly, the voltages in the d − q reference frame are also required to
satisfy
√𝑣d(t)2 + 𝑣q(t)2 ≤ √13 Vdc, (2.20)
based on the definition of the voltage space vector in relation to those in the d − q reference frame (see
(1.28)). The inequalities (2.19) and (2.20) will be used as constraints in the control system design and
implementation (see Chapter 4).
Note that the maximum amplitude of the three phase voltage signals when using the PWM with the
third harmonic injection technique ( 1
6
) is identical to the case when using the space vector modulation
technique, which is √13 Vdc. Thus, in the applications, the limit on the amplitude of the control signal is
often taken as √13 Vdc.
2.5 Simulation Study of the Effect of PWM
Since the time varying input voltages for AC drives and power converter are to be realized by the
PWM generation and inverter, taking the PMSM as an example, the complete plant model, as shown
in Figure 2.8, consists of machine model of PMSM, PWM and IGBT inverters. Note that Figure 2.8 is an
equivalent representation in d − q frame which does not illustrate the real hardware implementation in
three-phase representation. When the modulation signal (𝑣∗ i ) works within the linear modulation region
of chosen PWM, the fundamental of output voltage (𝑣i) from the IGBT inverter approximates the desired
modulation signal, that is 𝑣∗ i ≈ 𝑣i. With the combination of Park-Clarke and its inverse transformation of
which their multiplication is an identity matrix, it is assumed that 𝑣∗ d ≈ 𝑣d and 𝑣∗ q ≈ 𝑣q when using IGBT
inverters as actuators for controller implementation.38 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
dq/abc PWM IGBT abc/dq PMSM
model
id
i
q
vd
v
q
v
a
v
b
v
c
v
a vd
v
b
v
c
v
q
S
a
S
b
S
c
*
* * *
*
Figure 2.8 Model of PMSM combined with PWM generator.
To study the impact of carrier frequency in PWM, simulation studies are performed using the physical parameters of the PMSM given in Table 1.2. Two sets of simulation results are obtained by using
the PMSM MATLAB/Simulink model illustrated in Figure 2.8. The first set of simulation results uses
a smaller PWM carrier frequency where fc = 1.05 kHz in contrast with the second set of simulation
results where a larger carrier frequency fc = 10.5 kHz is utilized. The voltage input signals to the PMSM
model are set as 𝑣∗
d = 3 V and 𝑣∗ q = 12.1244 V, and the DC bus voltage is fixed at 100 V so that the
modulation signal is within its linear modulation range. Figure 2.9(a) shows the three phase currents
0 0.02 0.04 0.06 0.08 0.1
–2
–1
3 2 1 0
Time (s)
Phase current (A)
i
a
ib
i
c
(a)
0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
Frequency (kHz)
Normalized ampitude
(b)
Figure 2.9 Spectrum of phase currents (fc = 1.05 kHz, 𝑣d = 3 V and 𝑣q = 12.1244 V).Control of Semiconductor Switches via PWM Technologies 39
and Figure 2.9(b)shows the corresponding spectrum of A phase current for the lower carrier frequency
case. It is seen that with this lower carrier frequency, the ia, ib and ic currents contain a large amount
of high frequency harmonic noise (see Figure 2.9(a)) and the harmonics mainly occur around the carrier frequency and its multiples (see Figure 2.9(b)), that is 1.05n kHz with n denoting an integer in this
case. When the carrier frequency is increased to fc = 10.5 kHz, the high frequency harmonic noise in the
three phase currents is reduced as seen in Figure 2.10(a) and the current ripple still occurs at the carrier
frequency (see Figure 2.10(b)), but the multiples move to the high frequency region. At a certain high
frequency region, the current ripple is then attenuated by the limited bandwidth of the PMSM, which is
a first order type of system depending on the values of resistance (Rs) and inductance (Ld, Lq).
In summary, the high frequency harmonic ripple with a low carrier frequency is much more severe
than the one with a high carrier frequency, and the harmonics occur around the carrier frequency and its
multiples. Thus, a higher carrier frequency in PWM offers improved performance in terms harmonics
attenuation. However, in practice, the resulting high switching loss, inherent characteristics of switching
devices and limited computational power prevent the use of a very high carrier frequency.
0 0.02 0.04 0.06 0.08 0.1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Time (s)
phase current (A)
i
a
ib
i
c
(a)
00 20 40 60 80 100
0.05
0.1
0.15
0.2
Frequency (kHz)
Normalized Ampitude
(b)
Figure 2.10 Spectrum of phase currents (fc = 10.5 kHz, 𝑣d = 3 V and 𝑣q = 12.1244 V). (a) three-phase current and
(b) Spectrum of ia at steady-state.40 PID and Predictive Control of Electrical Drives and Power Converters using MATLAB®/Simulink®
2.6 Summary
Two popular categories of PWM techniques, carrier based PWM and space vector based PWM, have also
been briefly revisited. It has been shown in Zhou and Wang (2002) that both implementation approaches
can lead to the same type of PWM, such as SVPWM, THIPWM and several variations of Discontinuous
PWM (DPWM) schemes (see Hava et al. (1999), Zhou and Wang (2002)). The implementation of these
PWM schemes is achieved by appropriate zero-sequence injection in the carrier based PWM approach,
while its corresponding equivalence is the varieties of the placement of zero-vector in the space vector
based PWM approach.
The maximum modulation index of the linear modulation ranges for different PWM is discussed.
Simulation results show that the PWM techniques have a reduced effect on the system when the carrier
frequency is increased.
2.7 Further Reading
A book was written on PWM by Holmes and Lipo (2003). An early work on sinusoidal PWM generator
was presented in Bowes (1975). Survey papers published on PWM technologies included Holtz (1992)
and Holtz (1994). PWM schemes based on voltage space vectors were realized and analyzed in van
der Broeck et al. (1988). Relationships between space-vector modulation and three-phase carrier-based
PWM were analyzed in Zhou and Wang (2002), Blasko (1997), Bowes and Lai (1997) and Chai and Wang
(2013). Carrier-based PWM-VSI over-modulation strategies were designed, analyzed and compared in
Hava et al. (1998). Graphic methods were discussed for carrier-based PWM-VSI drives in Hava et al.
(1999). Space vector modulation implementation with the ADMCF32X was discussed in Analog Devices
Inc (2000).
Geyer (2011) made comparative studies of control and modulation schemes for medium-voltage drives
between predictive control concepts and PWM-Based Schemes.
References
Analog Devices Inc 2000 Implementing Space Vector Modulation with the ADMCF32X.
Blasko V 1997 Analysis of a hybrid PWM based on modified space-vector and triangle-comparison methods. IEEE
Transactions on Industry Applications 33(3), 756–764.
Bowes S 1975 New sinusoidal pulsewidth-modulated invertor. Electrical Engineers, Proceedings of the Institution of
122(11), 1279–1285.
Bowes S and Lai YS 1997 The relationship between space-vector modulation and regular-sampled PWM. IEEE Transactions on Industrial Electronics 44(5), 670–679.
Chai S and Wang L 2013 A unified pulse generation approach for 2L-VSI from SVPWM to direct switching Industrial
Electronics Society, IECON 2013–39th Annual Conference of the IEEE, pp. 346–351.
Geyer T 2011 A comparison of control and modulation schemes for medium-voltage drives: Emerging predictive
control concepts versus PWM-based schemes. IEEE Transactions on Industry Applications 47(3), 1380–1389.
Hava A, Kerkman R and Lipo T 1998 Carrier-based PWM-VSI overmodulation strategies: analysis, comparison, and
design. IEEE Transactions on Power Electronics 13(4), 674–689.
Hava A, Kerkman R and Lipo T 1999 Simple analytical and graphical methods for carrier-based PWM-VSI drives.
IEEE Transactions on Power Electronics 14(1), 49–61.
Holmes D and Lipo T 2003 Pulse Width Modulation for Power Converters: Principles and Practice vol. 18. Wiley.
Holtz J 1992 Pulsewidth modulation-a survey. IEEE Transactions on Industrial Electronics 39(5), 410–420.
Holtz J 1994 Pulsewidth modulation for electronic power conversion. Proceedings of the IEEE 82(8), 1194–1214.
Kreyszig E 2010 Advanced Engineering Mathematics 10th edn. John Wiley & Sons, Inc., New York.
van der Broeck H, Skudelny HC and Stanke G 1988 Analysis and realization of a pulsewidth modulator based on
voltage space vectors. IEEE Transactions on Industry Applications 24(1), 142–150.
Zhou K and Wang D 2002 Relationship between space-vector modulation and three-phase carrier-based PWM: a
comprehensive analysis. IEEE Transactions on Industrial Electronics 49(1), 186–196.

Robust Control Systems
12.1 Introduction 855
12.2 robust Control Systems and System Sensitivity 856
12.3 analysis of robustness 860
12.4 Systems with Uncertain parameters 862
12.5 the Design of robust Control Systems 864
12.6 the Design of robust pID-Controlled Systems 868
12.7 the robust Internal Model Control System 872
12.8 Design examples 875
12.9 the pseudo-Quantitative Feedback System 886
12.10 robust Control Systems Using Control Design Software 888
12.11 Sequential Design example: Disk Drive read System 891
12.12 Summary 893
Preview
Physical systems and the external environment in which they operate cannot be
modeled precisely, may change in an unpredictable manner, and may be subject
to significant disturbances. The design of control systems in the presence of significant uncertainty motivates the concept of robust control system design. Advances
in robust control design methodologies can address stability robustness and performance robustness in the presence of uncertainty. In this chapter, we describe five
methods for robust design, including root locus, frequency response, ITAE methods
for a robust PID systems, internal model control, and pseudo-quantitative feedback
methods. However, we should also realize that classical design techniques may also
produce robust control systems. Control engineers who are aware of these issues
can design robust PID controllers, robust lead-lag controllers, and so forth. The
chapter concludes with a PID controller design for the Sequential Design Example:
Disk Drive Read System.
DesireD OutcOmes
Upon completion of Chapter 12, students should:
❏❏ Appreciate the role of robustness in control system design.
❏❏ Be familiar with uncertainty models, including additive uncertainty, multiplicative uncertainty, and parameter uncertainty.
❏❏ Understand the various methods of tackling the robust control design problem using
root locus, frequency response, ITAE methods for PID control, internal model, and
pseudo-quantitative feedback methods.Section 12.1 Introduction 855
12.1 intrODuctiOn
A control system designed using the methods and concepts of the preceding chapters assumes knowledge of the model of the process and controller and constant
parameters. The process model will be an inaccurate representation of the actual
physical system due to
❏❏ parameter changes
❏❏ unmodeled dynamics
❏❏ unmodeled time delays
❏❏ changes in equilibrium point (operating point)
❏❏ sensor noise
❏❏ unpredicted disturbance inputs.
The goal of robust control system design is to maintain acceptable closed-loop system performance in the presence of model inaccuracies and changes.
(b)
Td(s)
(a)
Td(s)
Y(s)
G(s)
1
1
G
c(s)
N(s)
-1
1
1
R(s)
+
-
+
+
Controller
G
c(s)
+
+
N(s)
R(s) Y(s)
Process
G(s)
Ea
(s)
FIgUre 12.1
Closed-loop control
system. (a) Signal
flow graph.
(b) block diagram.
A robust control system maintains acceptable performance in the presence
of significant model uncertainty, disturbances, and noise.
A system structure that incorporates system uncertainties is shown in
Figure 12.1. This model includes the sensor noise N1s2, the disturbance input
Td1s2, and a process G1s2 with unmodeled dynamics or parameter changes. The856 Chapter 12 Robust Control Systems
unmodeled dynamics and parameter changes may be significant, and for these systems the challenge is to create a design that retains the desired performance.
12.2 rObust cOntrOl systems anD system sensitivity
Designing highly accurate systems in the presence of significant plant uncertainty
is a classical feedback design problem. The theoretical bases for the solution of this
problem date back to the works of H. S. Black and H. W. Bode in the early 1930s,
when this problem was referred to as the sensitivities design problem. A significant
amount of literature has been published since then regarding the design of systems
subject to large process uncertainty. The designer seeks to obtain a system that performs adequately over a large range of uncertain parameters. A system is said to be
robust when it is durable, hardy, and resilient.
A control system is robust when (1) it has low sensitivities, (2) it is stable over
the expected range of parameter variations, and (3) the performance continues to
meet the specifications in the presence of a set of changes in the system parameters
[3, 4]. Robustness is the low sensitivity to effects that are not considered in the analysis and design phase—for example, disturbances, measurement noise, and unmodeled dynamics. The system should be able to withstand these neglected effects when
performing the tasks for which it was designed.
For small-parameter perturbations, we may use, as a measure of robustness, the
differential sensitivities discussed in Sections 4.3 (system sensitivity) and Section 7.5
(root sensitivity) [6]. The system sensitivity is defined as
S
Ta
=
0T>T
0a>a, (12.1)
where a is the parameter and T the transfer function of the system. The root sensitivity is defined as
S
ria
=
0ri
0a>a. (12.2)
When the zeros of T1s2 are independent of the parameter a, we showed that
S
Ta
= - a
n
i=1
S
ria
#
1
s + ri
, (12.3)
for an nth-order system. For example, if we have a closed-loop system, as shown in
Figure 12.2, where the variable parameter is a, then T1s2 = 1>[s + 1a + 12], and
S
Ta
=
-a
s + a + 1. (12.4)
This follows because r1 = +1a + 12, and
-S
ria
= -a. (12.5)Section 12.2 Robust Control Systems and System Sensitivity 857
Therefore,
S
Ta
= -S
ria
1
s + a + 1 =
-a
s + a + 1. (12.6)
Let us examine the sensitivity of the second-order system shown in Figure 12.3.
The transfer function of the closed-loop system is
T1s2 = K
s2 + s + K. (12.7)
The system sensitivity for K is
S1s2 = SK T = s1s + 12
s2 + s + K. (12.8)
A Bode plot of the asymptotes of 20 log T1jv2 and 20 log S1jv2 is shown in
Figure 12.4 for K = 1>4 (critical damping). Note that the sensitivity is small for
lower frequencies, while the transfer function primarily passes low frequencies.
Of course, the sensitivity S1s2 only represents robustness for small changes in gain.
If K changes from K = 1>4 within the range K = 1>16 to K = 1, the resulting range
of step response is shown in Figure 12.5. This system, with an expected wide range of
-
+
R(s) 1 Y(s)
s + a
FIgUre 12.2
A first-order
system.
R(s) K Y(s)
s(s + 1)
+ -
FIgUre 12.3
A second-order
system.
40
20
0
-20
-40
0.01 0.1 0.5 1 10 100
Magnitude (dB)
20 log0T 0
20 log0S0
K = 1
4
0
Frequency (rad/s)
FIgUre 12.4 Sensitivity and 20 log T1jv2 for the
second-order system in Figure 12.3. The asymptotic
approximations are shown for K = 1 4.
1.4
1.0
y(t)
0 2 4
Time (s)
6 8 10 12
K = 1
K = 1/4 K = 1/16
FIgUre 12.5 The step response for selected gain K.858 Chapter 12 Robust Control Systems
K, may not be considered adequately robust. A robust system would be expected to
yield essentially the same (within an agreed-upon variation) response to a selected
input.
ExamplE 12.1 Sensitivity of a controlled system
Consider the system shown in Figure 12.6, where G1s2 = 1>s2 and a PD controller
G
c1s2 = Kp + KDs. Then the sensitivity with respect to changes in G1s2 is
SG T = 1
1 + G
c1s2G1s2 =
s2
s2 + KDs + Kp, (12.9)
and
T1s2 =
KDs + Kp
s2 + KDs + Kp. (12.10)
Consider the nominal condition z = 1 and vn = 2Kp. Then, KD = 2vn to achieve
z = 1. Therefore, we may plot 20 log S and 20 log T on a Bode plot, as shown
in Figure 12.7. Note that the frequency vn is an indicator on the boundary between
the frequency region in which the sensitivity is the important design criterion and
the region in which the stability margin is important. Thus, if we specify vn properly to take into consideration the extent of modeling error and the frequency of
external disturbance, we can expect the system to have an acceptable amount of
robustness. ■
ExamplE 12.2 System with a right-hand-plane zero
Consider the system shown in Figure 12.8, where the plant has a zero in the righthand plane. The closed-loop transfer function is
T1s2 =
K1s - 12
s2 + 12 + K2s + 11 - K2. (12.11)
The system is stable for a gain -2 6 K 6 1. The steady-state error due to a negative unit step input R1s2 = -1>s is
e
ss =
1 - 2K
1 - K , (12.12)
-
+
Controller Process
Kp
+ K
R(s) D s 1 Y(s)
s2
FIgUre 12.6
A system with a PD
controller.Section 12.2 Robust Control Systems and System Sensitivity 859
and e
ss = 0 when K = 1>2. The response is shown in Figure 12.9. Note the initial
undershoot at t = 1 s. This system is sensitive to changes in K, as recorded in Table
12.1. The performance of this system might not be acceptable for a change of gain
of only {10,. Thus, this system would not be considered robust. The steady-state
error of this system changes greatly as K changes. ■
20
0
-20
-40
0.01 0.1 1 10 100
Magnitude (dB)
-60
v/v
n
20 log0T 0
20 log0S0
FIgUre 12.7 Sensitivity and T1s2 for the secondorder system in Figure 12.6.
R(s) K Y(s)
-
+ s - 1
(s + 1)2
G(s)
FIgUre 12.8 A second-order system.
y(t)
0 5 10 15 20 25 30
1.0
0.8
0.6
0.4
0.2
0
-0.2
Time (s)
FIgUre 12.9
Step response
of the system in
Figure 12.8 with
K = 1
2.
table 12.1 results for example 12.2
K 0.25 0.45 0.50 0.55 0.75
 ess 0.67 0.18 0 0.22 1.0
Undershoot 5% 9% 10% 11% 15%
Settling time (seconds) 15 24 27 30 45860 Chapter 12 Robust Control Systems
12.3 analysis Of rObustness
System goals include maintaining a small tracking error for an input R1s2 and
keeping the output Y1s2 small for a disturbance Td1s2. The sensitivity function is
S1s2 = 1
1 + G
c1s2G1s2,
and the complementary sensitivity function is
C1s2 =
G
c1s2G1s2
1 + G
c1s2G1s2.
We also have the relationship
S1s2 + C1s2 = 1. (12.13)
For physically realizable systems, the loop gain L1s2 = Gc1s2G1s2 is small for high
frequencies. This means that S1jv2 approaches 1 at high frequencies.
Consider the closed-loop system shown in Figure 12.1. An additive perturbation characterizes the set of possible processes as follows:
G
a1s2 = G1s2 + A1s2,
where G1s2 is the nominal process, and A1s2 is the perturbation that is bounded in
magnitude. We assume that Ga1s2 and G1s2 have the same number of poles in the
right-hand s-plane (if any) [32]. Then the system stability will not change if
 A1jv2 6  1 + Gc1jv2G1jv2 for all v. (12.14)
This assures stability but not dynamic performance.
A multiplicative perturbation is modeled as
G
m1s2 = G1s2[1 + M1s2].
The perturbation is bounded in magnitude, and it is again assumed that Gm1s2 and
G1s2 have the same number of poles in the right-hand s-plane. Then the system
stability will not change if
 M1jv2 6 2 1 + Gc1jv21G1jv2 2 for all v. (12.15)
Equation (12.15) is called the robust stability criterion. This is a test for robustness
with respect to a multiplicative perturbation. This form of perturbation is often usedSection 12.3 Analysis of Robustness 861
because it satisfies the intuitive properties of (1) being small at low frequencies,
where the nominal process model is usually well known, and (2) being large at high
frequencies, where the nominal model is always inexact.
ExamplE 12.3 System with multiplicative perturbation
Consider the system of Figure 12.1 with Gc = K, and
G1s2 =
170000 1s + 0.12
s1s + 321s2 + 10s + 100002.
The system is unstable with K = 1, but a reduction in gain to K = 0.5 will stabilize
it. Now, consider the effect of an unmodeled pole at 50 rad/s. In this case, the multiplicative perturbation is determined from
1 + M1s2 = 50
s + 50,
or M1s2 = -s>1s + 502. The magnitude bound is then
 M1jv2 = 2 jv-+jv50 2 .
 M1jv2 and  1 + 1>1KG1jv22 are shown in Figure 12.10(a), where it is seen that
the criterion of Equation (12.15) is not satisfied. Thus, the system may not be stable.
If we use a lag compensator
G
c1s2 =
0.151s + 252
s + 2.5 ,
the loop transfer function is L1s2 = 1 + Gc1s2G1s2. We reshape the function
G
c1jv2G1jv2 in the frequency range 2 6 v 6 25 and check the condition
 M1jv2 6 2 1 + Gc1jv21G1jv2 2 ,
as shown in Figure 12.10(b). Here the robustness inequality is satisfied, and the system is robustly stable. ■
The control objective is to design a compensator Gc1s2 so that the transient,
steady-state, and frequency-domain specifications are achieved and the cost of feedback measured by the bandwidth of the compensator Gc1jv2 is sufficiently small.
This bandwidth constraint is needed mainly because of measurement noise. In
subsequent sections, we discuss including a pre-filter in a two-degree-of-freedom
configuration to help achieve the design goals.862 Chapter 12 Robust Control Systems
12.4 systems with uncertain Parameters
Many systems have several parameters that are constants but uncertain within a
range. For example, consider a system with a characteristic equation
sn + a
n - 1sn - 1 + an - 2sn - 2 + g+ a0 = 0 (12.16)
with known coefficients within bounds
ai … ai … bi and i = 0, c, n,
where a
n = 1.
To ascertain the stability of the system, we might have to investigate all possible combinations of parameters. Fortunately, it is possible to investigate a limited
number of worst-case polynomials [20]. The analysis of only four polynomials is
102
101
100
10-1
10-2
10-2 10-1 100 101 102 103
Frequency (rad/s)
10-2 10-1 100 101 102 103
Frequency (rad/s)
(a)
(b)
Magnitude
102
101
100
10-1
10-2
Magnitude
1
G
P1 + c( jv) G( jv)P
0M( jv)0
0M( jv)0
1
P1 + KG( jv) P
FIgUre 12.10
The robust
stability criterion
for Example 12.3.Section 12.4 Systems with Uncertain Parameters 863
sufficient, and they are readily defined for a third-order system with a characteristic
equation
s3 + a2s2 + a1s + a0 = 0. (12.17)
The four polynomials are
q11s2 = s3 + a2s2 + b1s + b0,
q21s2 = s3 + b2s2 + a1s + a0,
q31s2 = s3 + b2s2 + b1s + a0,
q41s2 = s3 + a2s2 + a1s + b0.
One of the four polynomials represents the worst case and may indicate either
unstable performance or at least the worst performance for the system in that case.
ExamplE 12.4 Third-order system with uncertain parameters
Consider a third-order system with uncertain coefficients such that
8 … a0 … 60 1 a0 = 8, b0 = 60;
12 … a1 … 100 1 a1 = 12, b1 = 100;
7 … a2 … 25 1 a2 = 7, b2 = 25.
The four polynomials are
q11s2 = s3 + 7s2 + 100s + 60,
q21s2 = s3 + 25s2 + 12s + 8,
q31s2 = s3 + 25s2 + 100s + 8,
q41s2 = s3 + 7s2 + 12s + 60.
We then proceed to check these four polynomials by means of the Routh–Hurwitz
criterion, and determine that the system is stable for all the range of uncertain
parameters. ■
ExamplE 12.5 Stability of uncertain system
Consider a unity feedback system with a process transfer function (under nominal
conditions)
G1s2 = 4.5
s1s + 121s + 22.
The nominal characteristic equation is then
q1s2 = s3 + 3s2 + 2s + 4.5 = 0,
where a0 = 4.5, a1 = 2, and a2 = 3. Using the Routh–Hurwitz criterion, we find
that this system is nominally stable. However, if the system has uncertain coefficients such that
4 … a0 … 5 1 a0 = 4, b0 = 5;
1 … a1 … 3 1 a1 = 1, b1 = 3; and
2 … a2 … 4 1 a2 = 2, b2 = 4,864 Chapter 12 Robust Control Systems
then we must examine the four polynomials:
q11s2 = s3 + 2s2 + 3s + 5,
q21s2 = s3 + 4s2 + 1s + 4,
q31s2 = s3 + 4s2 + 3s + 4,
q41s2 = s3 + 2s2 + 1s + 5.
Using the Routh–Hurwitz criterion, q11s2 and q31s2 are stable and q21s2 is marginally stable. For q41s2, we have
s3
s2
s1
s0 4 -31 1 2 5 5>2 .
Therefore, the system is unstable for the worst case, where a2 = minimum, a1 =
minimum, and b0 = maximum. This occurs when the process has changed to
G1s2 = 5
s1s + 121s + 12.
Note that the third pole has moved toward the jv@axis to its limit at s = -1 and that
the gain has increased to its limit at K = 5. ■
12.5 the Design Of rObust cOntrOl systems
The design of robust control systems involves determining the structure of the controller and adjusting the controller parameters to achieve acceptable performance
in the presence of uncertainty. The structure of the controller is chosen such that
the system response can meet certain performance criteria.
One possible objective in the design of a control system is that the controlled
system output should very accurately track the input. That is, we want to minimize
the tracking error. In an ideal setting, the Bode plot of the loop gain, L1s2, would
be 0-dB gain of infinite bandwidth and zero phase shift. In practice, this is not possible. One possible design objective is to maintain the magnitude response curve as
flat and as close to unity for as large a bandwidth as possible for a given plant and
controller combination [20].
Another important goal of a control system design is that the effect on the
output of the system due to disturbances is minimized. Consider the control system shown in Figure 12.11, where G1s2 is the plant and Td1s2 is the disturbance.
We then have
T1s2 =
Y1s2
R1s2 =
G
c1s2G1s2
1 + G
c1s2G1s2, (12.18)Section 12.5 The Design of Robust Control Systems 865
and
Y1s2
Td1s2 =
G1s2
1 + G
c1s2G1s2. (12.19)
Note that both the reference and disturbance transfer functions have the same
denominator; in other words, they have the same characteristic equation—namely,
1 + G
c1s2G1s2 = 1 + L1s2 = 0. (12.20)
Recall that the sensitivity of T1s2 with respect to G1s2 is
SG T = 1
1 + G
c1s2G1s2. (12.21)
Equation (12.21) shows that for low sensitivity, we desire a high value of loop gain
L1jv2. But it is known that a high gain can lead to instability and amplification of
the measurement noise. Thus, we seek the following:
1. T1s2 with wide bandwidth.
2. Large loop gain L1s2 at low frequencies.
3. Small loop gain L1s2 at high frequencies.
Setting the design of robust systems in frequency-domain terms, we scale a
compensator Gc1s2 such that the closed-loop sensitivity is less than some tolerance
value. But sensitivity minimization involves finding a compensator such that the
closed-loop sensitivity is minimized.
The gain and phase margin problem is to find a compensator to achieve prescribed
gain and phase margins. The disturbance rejection problem and measurement noise
attenuation problem seeks a solution with high loop gain at low frequencies and
low loop gain at high frequencies, respectively. For the frequency-domain specifications, we seek the following conditions for the Bode plot of Gc1jv2G1jv2, shown in
Figure 12.12:
1. For relative stability, the loop gain must have not more than a -20@dB>decade slope at
or near the crossover frequency vc.
2. Steady-state accuracy and measurement noise rejection achieved by the low gain at
high frequency.
3. Disturbance rejection by a high gain over low frequencies.
4. Accuracy over a bandwidth vB, by maintaining the loop gain above a prescribed
level.
-
+
R(s) Y(s)
Td(s)
+
+
Gc(s) G(s)
FIgUre 12.11
A system with a
disturbance.866 Chapter 12 Robust Control Systems
Using the root sensitivity concept, we can state that Sa r must be minimized
while attaining T1s2 with dominant roots that will provide the appropriate
response and minimize the effect of Td1s2. As an example, let Gc1s2 = K and
G1s2 = 1>1s1s + 122 for the system in Figure 12.11. This system has two roots, and
we select a gain K so that Y1s2>Td1s2 is minimized, SK r is minimized, and T1s2 has
desirable dominant roots. The sensitivity is
SK r = dr
dK
#
K r
=
ds
dK 2 s =r # Kr , (12.22)
and the characteristic equation is
s1s + 12 + K = 0. (12.23)
Therefore, dK>ds = -12s + 12, since K = -s1s + 12. We then obtain
SK r = -1
2s + 1
-s1s + 12
s 2 s =r. (12.24)
When z 6 1, the roots are complex and r = -0.5 + j 1
2
24K - 1. Then,
 SK r  = ¢ 4KK- 1 ≤1>2. (12.25)
The magnitude of the root sensitivity is shown in Figure 12.13 for K = 0.2 to
K = 5. The percent overshoot to a step is also shown. As illustrated in Figure 12.13,
select K L 1.25 yields a near minimum sensitivity while maintaining good performance for the step response. To reduce the root sensitivity while simultaneously
minimizing the effect of disturbances, we can use the design procedure as follows:
1. Sketch the root locus of the compensated system with Gc1s2 chosen to attain the
desired location for the dominant roots.
2. Maximize the gain of Gc1s2 to reduce the effect of the disturbance.
3. Determine SK r and attain the minimum value of the root sensitivity consistent with the
transient response required, as described in Step 1.
20 log0GcG0
Minimum performance
bounds
High gains for good performance
(command following)
Stable crossover
(gain and phase margins)
Robustness
bounds
Low gains to reduce
sensitivity to sensor
noise and model uncertainty
v
c
FIgUre 12.12
Bode plot for 20 log
 Gc1jv2G1jv2 .Section 12.5 The Design of Robust Control Systems 867
ExamplE 12.6 Sensitivity and compensation
Consider the system in Figure 12.11 when G1s2 = 1>s2 and Gc1s2 is to be selected
by frequency response methods. Therefore, the compensator is to be selected to
achieve an appropriate gain and phase margin while minimizing sensitivity and the
effect of the disturbance. Thus, we choose
G
c1s2 =
K1s>z + 12
s>p + 1 . (12.26)
Choose K = 10 to reduce the effect of the disturbance. To attain a phase margin
of 45°, select z = 2.0 and p = 12.0. The compensated diagram is shown in Figure
12.14. The closed-loop bandwidth is vB = 1.6vc. Thus, we will increase the bandwidth by using the compensator.
The sensitivity at vc is
 SG T 1jvc2 = 2 1 + Gc11jv2G1jv2 2 v = vc. (12.27)
80
K
60
40
20
0 1 2 3 4 5
0
0.5
1.0
1.5
2.0
Percent overshoot
Sensitivity
Percent overshoot
0SrK0
FIgUre 12.13
Sensitivity and
percent overshoot
for a second-order
system.
0.1 0.5 1 2 5 10 12 20 50
-1805
-1505
-1205
0
40
30
20
10
-10
-20
20 log0L0, dB
v
f(v)
Uncompensated
magnitude
-6 dB/dec
Compensated
magnitude
Phase margin
P.M. = 455
Compensated
phase angle
Uncompensated
phase angle Zero Pole
v
c = 5
FIgUre 12.14
Bode plot for
Example 12.6.868 Chapter 12 Robust Control Systems
To estimate  SG T  , we recall that the Nichols chart enables us to obtain
 T1jv2 = 2 1 +GcG1jcv12jvG21Gjv12jv2 2 . (12.28)
We can plot points of Gc1jv2G1jv2 on the Nichols chart and then read  T1jv2
from the chart. Then, we have
 SG T 1jv12 =  T1jv12
 Gc1jv12G1jv12 , (12.29)
where v1 is chosen at a frequency below vc. The Nichols chart for the compensated
system is shown in Figure 12.15. For v1 = vc>2.5 = 2, we have 20 log T1jv12 =
2.5 dB and 20 log Gc1jv12G1jv12 = 9 dB. Therefore,
 S1jv12 =  T1jv12
 Gc1jv12G1jv12 =
1.33
2.8
= 0.47. ■
12.6 the Design Of rObust PiD-cOntrOlleD systems
The PID controller has the transfer function
G
c1s2 = KP +
KI
s
+ KDs.
The popularity of PID controllers can be attributed partly to their robust performance over a wide range of operating conditions and partly to their functional simplicity, which allows engineers to operate them in a straightforward manner. To
implement the PID controller, three parameters must be determined for the given
process: proportional gain, integral gain, and derivative gain [31].
Consider the PID controller
G
c1s2 = KP + KI
s
+ KDs =
KDs2 + KPs + KI
s
=
KD1s2 + as + b2
s
=
KD1s + z121s + z22
s
, (12.30)
where a = KP>KD and b = KI>KD. Therefore, a PID controller introduces a transfer function with one pole at the origin and two zeros.
Recall that a root locus begins at the poles and ends at the zeros. If we have a
system as shown in Figure 12.16 with
G1s2 = 1
1s + 221s + 52,
and we use a PID controller with complex zeros, we can plot the root locus as shown
in Figure 12.17. As the gain KD of the controller is increased, the complex roots approach the zeros. The closed-loop transfer function isSection 12.6 The Design of Robust PID-Controlled Systems 869
T1s2 =
G1s2Gc1s2Gp1s2
1 + G1s2Gc1s2
=
KD1s + z121s + zn12
1s + r221s + r121s + nr12 Gp1s2 M
KDGp1s2
s + r2
, (12.31)
-210 -180 -150 -120 -90 -60 -30 0
-24
-18
-12
-6
6 0
12
18
24
30
36
G
cG
1 + G
Phase of cG = -25
Loop phase j(GcG), degrees
Loop gain GcG, decibels
G
cG
1 + G
cG
G
cG
1 + G
cG
Magnitude of = -18 dB
Loop gain-phase diagram
versus G
-0.10 dB
0 dB
0.10 dB
0.25 dB
0.5 dB
1.0 dB
2 dB
3 dB
4 dB
5 dB
6 dB
9 dB
12 dB
-0.25 dB
-0.5 dB
-1.0 dB
-2 dB
-3 dB
-4 dB
-5 dB
-6 dB
-12 dB
-24 dB
55
25
-1805 05
-1205
-305
-205
-105
-55
-25
-1505
-905
-605
-2105
GG
c( jv)
v1 = 1
v1 = 2
v1 = 5
v1 = 10
FIgUre 12.15
Nichols chart for
Example 12.7.870 Chapter 12 Robust Control Systems
because the zeros and the complex roots are approximately equal 1r1 L z12. Setting
Gp
1s2 = 1, we have
T1s2 = KD
s + r2
L
KD
s + KD
(12.32)
when KD W 1. The only limiting factor is the allowable magnitude of U1s2
(Figure 12.16) when KD is large. If KD is 100, the system has a fast response and
zero steady-state error. Furthermore, the effect of the disturbance is reduced
significantly.
In general, we note that PID controllers are particularly useful for reducing
steady-state error and improving the transient response when G1s2 has one or two
poles (or may be approximated by a second-order process).
The main problem in the selection of the three PID controller parameters is
that these coefficients do not readily translate into the desired performance and robustness characteristics that the control system designer has in mind. Several rules
and methods have been proposed to solve this problem. In this section, we consider
several design methods using root locus and performance indices.
The first design method uses the ITAE performance index. Hence, we select the
three PID coefficients to minimize the ITAE performance index, which produces
-
+
R(s) Y(s)
Ea
(s) U(s)
Td(s)
+
+
Gp
(s) Gc(s) G(s)
FIgUre 12.16
Feedback control
system with a
desired input R1s2
and an undesired
input Td1s2.
j2
j4
-j2
-j4
-r2
-10 -8 -6 -4 -2
-z1
-r1
KD
increasing
- z1
- r1
FIgUre 12.17
Root locus with
-z1 = -6 + j2.Section 12.6 The Design of Robust PID-Controlled Systems 871
an excellent transient response to a step or a ramp. The design procedure consists
of three steps:
1. Select the v
n of the closed-loop system by specifying the settling time.
2. Determine the three coefficients using the appropriate optimum equation (Table 5.6)
and the v
n of step 1 to obtain Gc1s2.
3. Determine a prefilter Gp1s2 so that the closed-loop system transfer function, T1s2,
does not have any zeros.
ExamplE 12.7 Robust control of temperature
Consider a temperature controller with a control system as shown in Figure 12.16
and a process
G1s2 = 1
1s + 122. (12.33)
If G
c1s2 = 1, the steady-state error is ess = 50%, and the settling time (with a 2%
criterion) is Ts = 3.2 s for a step input. We want to obtain an optimum ITAE performance for a step input and a settling time of Ts … 0.5 s. Using a PID controller,
we have
T11s2 =
Y1s2
R1s2 =
G
c1s2G1s2
1 + G
c1s2G1s2
=
KDs2 + KPs + KI
s3 + 12 + KD2s2 + 11 + KP2s + KI, (12.34)
where G
p1s2 = 1. The optimum coefficients of the characteristic equation for ITAE
are
s3 + 1.75v
ns2 + 2.15vn2s + vn3 = 0. (12.35)
We need to select v
n in order to meet the settling time requirement. Since
Ts
= 4>1zvn2 and z is unknown but near 0.8, we set vn = 10. Then, equating the
denominator of Equation (12.34) to Equation (12.35), we obtain the three coefficients as KP = 214, KD = 15.5, and KI = 1000.
Then Equation (12.34) becomes
T11s2 = 15.5s2 + 214s + 1000
s3 + 17.5s2 + 215s + 1000
=
15.51s + 6.9 + j4.121s + 6.9 - j4.12
s3 + 17.5s2 + 215s + 1000 . (12.36)
The response of this system to a step input has a percent overshoot of P.O. = 33.9%,
as recorded in Table 12.2.
We select a prefilter Gp1s2 so that we achieve the desired ITAE response with
T1s2 =
G
c1s2G1s2Gp1s2
1 + G
c1s2G1s2 =
1000
s3 + 17.5s2 + 215s + 1000. (12.37)872 Chapter 12 Robust Control Systems
table 12.2 results for example 12.7
controller
G
c1s2 = 1
PiD and
Gp
1s2 = 1
PiD with
Gp
1s2 Prefilter
Percent overshoot 4.2% 33.9% 1.9%
Settling time (seconds) 4.2 0.6 0.75
Steady-state error 50% 0.0% 0.0%
Disturbance error 52% 0.4% 0.4%
Therefore, we require that
Gp
1s2 = 64.5
s2 + 13.8s + 64.5 (12.38)
in order to eliminate the zeros in Equation (12.36) and bring the overall numerator
to 1000. The response of the system T1s2 to a step input is indicated in Table 12.2.
The system has a small percent overshoot, a settling time of Ts … 0.5 s, and zero
steady-state error. Furthermore, for a disturbance Td1s2 = 1>s, the maximum value
of y1t2 due to the disturbance is 0.4% of the magnitude of the disturbance. This is a
very favorable design.
Let us consider the system when the plant varies significantly, so that
G1s2 = K
1ts + 122, (12.39)
where 0.5 … t … 1 and 1 … K … 2. We want to achieve investigate the behavior
using the ITAE optimum system with the prefilter. The objective is to have an overshoot of P.O. … 4% and a settling time (with a 2% criterion) of Ts … 2 s while G1s2
can attain any value in the range indicated.
We then obtain the step response for the four conditions: t = 1, K = 1; t = 0.5,
K = 1; t = 1, K = 2; and t = 0.5, K = 2. The results are summarized in Figure
12.18. This is a very robust system. ■
The value of v
n that can be chosen will be limited by considering the maximum allowable u1t2, where u1t2 is the output of the controller, as shown in Figure
12.16. As an example, consider the system in Figure 12.16 with a PID controller,
G1s2 = 1>1s1s + 122, and the necessary prefilter Gp1s2 to achieve ITAE performance. If we select v
n = 10, 20, and 40, the maximum value of u1t2 is as recorded in
Table 12.3. If we wish to limit u1t2, we need to limit vn. Thus, we are limited in the
settling time we can achieve.
12.7 the rObust internal mODel cOntrOl system
The internal model control system is shown in Figure 12.19. We now consider the
use of the internal model design with special attention to robust system performance. The internal model principle states that if Gc1s2G1s2 contains R1s2 then
Y1s2 will track R1s2 asymptotically (in the steady state), and the tracking is robust.Section 12.7 The Robust Internal Model Control System 873
Consider a simple system with G1s2 = 1>s, for which we seek a ramp response
with a steady-state error of zero. A PI controller is sufficient, and we let K = 0 (no
state variable feedback). Then we have
G
c1s2G1s2 = ¢Kp + KsI ≤ 1s = Kpss+2 KI. (12.40)
Note that for a ramp, R1s2 = 1>s2, which is contained as a factor of Equation
(12.40), and the closed-loop transfer function is
T1s2 =
Kp
s + KI
s2 + K
ps + KI
. (12.41)
table 12.3 Maximum Value of plant Input
v
n 10 20 40
u1t2 maximum for R1s2 = 1>s 35 135 550
Settling time (seconds) 0.9 0.5 0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.2
0.4
0.6
0.8
1
1.2
Time (s)
y(t)
K = 1, t = 0.5
K = 1, t = 1
K = 2, t = 1
K = 2, t = 0.5
FIgUre 12.18
Response of the
closed loop system
in the presence of
uncertainty in K
and τ.
- -
+ +
R(s) Gc(s) Y(s)
Ea
(s)
G(s)
Process
K x
FIgUre 12.19
The internal model
control system.874 Chapter 12 Robust Control Systems
Using the ITAE specifications for a ramp response, we require that
T1s2 =
3.2v
ns + vn2
s2 + 3.2v
ns + vn2. (12.42)
We select v
n to satisfy a specification for the settling time. For a settling time (with a
2% criterion) of Ts = 1 s, we select vn = 5. Then we require Kp = 16 and KI = 25.
The response of this system settles in Ts = 1 s and then tracks the ramp with zero
steady-state error. If this system (designed for a ramp input) receives a step input, the
response has a percent overshoot of P.O. = 5% and a settling time of Ts = 1.5 s. This
system is very robust to changes in the plant. For example, if G1s2 = K>s changes
gain so that K varies by {50,, the change in the ramp response is insignificant.
ExamplE 12.8 Design of an internal model control system
Consider the system of Figure 12.20 with state variable feedback and a compensator
G
c1s2. We wish to track a step input with zero steady-state error. Here, we select a
PID controller for G
c1s2. We then have
G
c1s2 = KDs2 + KPs + KI
s
,
and G1s2Gc1s2 will contain R1s2 = 1>s, the input command. Note that we feed
back both state variables and add these additional signals after Gc1s2 in order to
retain the integrator in Gc1s2.
The goal is to achieve a settling time (to within 2% of the final value) of Ts … 1
second and a deadbeat response while retaining a robust response. Here, we assume
that the two poles of G1s2 can change by {50,. Then the worst-case condition is
Gn 1s2 = 1
1s + 0.521s + 12.
One design approach is to design the control for this worst-case condition. Another
approach, which we use here, is to design for the nominal G1s2 and one-half the desired settling time. Then we expect to meet the settling time requirement and attain
a very fast, highly robust system. Note that the prefilter Gp1s2 is used to attain the
desired form for T1s2.
- -
+ +
R(s) Y(s)
Process G(s)
X2(s) X1(s)
G
Gp(s) c(s) 1
s + 2
Ka
Kb
-
1
s + 1
FIgUre 12.20
An internal model
control with state
variable feedback
and G
c1s2.Section 12.8 Design Examples 875
The response desired is deadbeat, so we use a third-order transfer function as
T1s2 =
v
n
3
s3 + 1.9v
ns2 + 2.20vn2s + vn3, (12.43)
and the settling time (with a 2% criterion) is Ts = 4>vn. For a settling time of
Ts
= 0.5 s, we use vn = 8.
The closed-loop transfer function of the system of Figure 12.20 with the appropriate Gp1s2 is
T1s2 = KI
s3 + 13 + KD + Kb2s2 + 12 + KP + Ka + 2Kb2s + KI. (12.44)
We let K
a = 10, Kb = 2, KP = 127.6, KI = 527.5, and KD = 10.35. Note that T1s2
could be achieved with other gain combinations.
The step response of this system has a deadbeat response with a percent overshoot of P.O. = 1.65% and a settling time of Ts = 0.5 s. When the poles of G1s2
change by {50,, the percent overshoot changes to P.O. = 1.86%, and the settling
time is T
s = 0.95 s. This is an outstanding design of a robust deadbeat response
system. ■
12.8 Design examPles
In this section we present two illustrative examples. The first example illustrates
the design of two degree-of-freedom controllers (that is, two separate controllers)
for an ultra-precision diamond turning machine. In the second design example, we
consider the practical problem of designing a controller in the presence of an uncertain time delay. The specific problem under investigation is a PID controller for
a digital audio tape drive. The design process is highlighted with an emphasis on
robustness.
ExamplE 12.9 Ultra-precision diamond turning machine
The design of an ultra-precision diamond turning machine has been studied at
Lawrence Livermore National Laboratory. This machine shapes optical devices
such as mirrors with ultra-high precision using a diamond tool as the cutting
device. In this discussion, we will consider only the z-axis control. Using frequency
response identification with sinusoidal input to the actuator we determined that
G1s2 = 4500
s + 60. (12.45)
The system can accommodate high gains, since the input command is a series of
step commands of very small magnitude (a fraction of a micron). The system has
an outer loop for position feedback using a laser interferometer with an accuracy of
0.1 micron 110-7 m2. An inner feedback loop is also used for velocity feedback, as
shown in Figure 12.21.876 Chapter 12 Robust Control Systems
We want to select the controllers, G11s2 and G21s2, to obtain an overdamped,
highly robust, high-bandwidth system. The robust system must accommodate
changes in G1s2 due to varying loads, materials, and cutting requirements. Thus, we
seek a large phase margin and gain margin for the inner and outer loops, and low
root sensitivity. The specifications are summarized in Table 12.4.
Since we want zero steady-state error for the velocity loop, we propose a velocity loop controller G21s2 = G31s2G41s2, where G31s2 is a PI controller and G41s2 is
a phase-lead compensator. Thus, we have
G21s2 = G31s2G41s2 = ¢Kp + KsI ≤ # 1 + K4s
a¢1 + Ka4 s≤
and choose KP>KI = 0.00532, K4 = 0.00272, and a = 2.95. We now have
G21s2 = KP s + 188
s
#
s + 368
s + 1085.
The root locus for G21s2G1s2 is shown in Figure 12.22. When KP = 2, we have
the velocity closed-loop transfer function given by
T21s2 =
V1s2
U1s2 =
90001s + 18821s + 3682
1s + 20521s + 30521s + 1042 L
104
1s + 1042, (12.46)
- -
+ + U(s)
R(s)
Position
command
(microns)
V(s)
Velocity Y(s)
Position
Position
controller
G1(s)
Velocity
controller
G2(s) 1
s
Actuator
and cutter
G(s)
Laser
interferometer
1
Tachometer
1
FIgUre 12.21
Turning machine
control system.
table 12.4 Specifications for turning Machine Control System
transfer function
specification velocity, V1s2>U1s2 Position Y1s2>R1s2
Minimum bandwidth 950 rad/s 95 rad/s
Steady-state error to a step 0 0
Minimum damping ratio z 0.8 0.9
Maximum root sensitivity  SK r  1.0 1.5
Minimum phase margin 90° 75°
Minimum gain margin 40 dB 60 dBSection 12.8 Design Examples 877
600
400
200
0
-200
-400
-600
-1,200-1,000 -800 -600 -400 -200 0 200
Imaginary axis
Real axis
Kp
increasing
FIgUre 12.22
Root locus for
velocity loop as Kp
varies.
table 12.5 Design results for turning Machine Control System
achieved result velocity Position
transfer
Position transfer
function Y1s2>R1s2
Closed-loop bandwidth 4,000 rad/s 1,000 rad/s
Steady-state error 0 0
Damping ratio, z 1.0 1.0
Root sensitivity,  SK r  0.92 1.2
Phase margin 93° 85°
Gain margin Infinite 76 dB
which is a large-bandwidth system. The actual bandwidth and root sensitivity are
summarized in Table 12.5. Note that we have exceeded the specifications for the
velocity transfer function.
We propose a phase-lead compensator for the position loop of the form
G11s2 = K1
1 + K5s
a¢1 + Ka5 s≤,
and we choose a = 2.0 and K5 = 0.0185 so that
G11s2 =
K11s + 542
s + 108 .
We then plot the root locus for the loop transfer function
L1s2 = G11s2 # T21s2 # 1
s
.
If we use the approximate T21s2 of Equation (12.46), we have the root locus of
Figure 12.23(a). Using the actual T21s2, we get the close-up of the root locus shown
in Figure 12.23(b). We select KP = 1000 and achieve the actual results for the total
system transfer function as recorded in Table 12.5. The total system has a high phase
margin, has a low sensitivity, and is overdamped with a large bandwidth. This system
is very robust. ■878 Chapter 12 Robust Control Systems
ExamplE 12.10 Digital audio tape controller
Consider the feedback control system shown in Figure 12.24, where
Gd1s2 = e-Ts.
The exact value of the time delay is uncertain, but is known to lie in the interval
T1 … T … T2. Define
G
m1s2 = e-TsG1s2.
Then
G
m1s2 - G1s2 = e-TsG1s2 - G1s2 = 1e-Ts - 12G1s2,
300
400
200
100
0
-100
-200
-300
-400
-600 -500 -400 -300 -200 -100 0 100 200
Imaginary axis
Real axis
K1 increasing
-104
(a)
(b)
jv
s
FIgUre 12.23
The root locus
for K1 7 0 for (a)
overview and (b)
close-up near origin
of the s-plane.Section 12.8 Design Examples 879
or
G
m1s2
G1s2 - 1 = e-Ts - 1.
If we define
M1s2 = e-Ts - 1,
then we have
G
m1s2 = 11 + M1s22G1s2. (12.47)
In the development of a robust stability controller, we would like to represent
the time-delay uncertainty in the form shown in Figure 12.25 where we need to determine a function M1s2 that approximately models the time delay. This will lead
to the establishment of a straightforward method of testing the system for stability
robustness in the presence of the uncertain time-delay. The uncertainty model is
known as a multiplicative uncertainty representation.
Since we are concerned with stability, we can consider R1s2 = 0. Then we can manipulate the block diagram in Figure 12.25 to obtain the form shown in Figure 12.26.
Using the small gain theorem, we have the condition that the closed-loop system is
stable if
 M1jv2 6 2 1 + Gc1jv21G1jv2 2 for all v.
G
c(s) G(s)
Controller
e-Ts
Time delay
+ -
R(s) Y(s)
Process
FIgUre 12.24
A feedback system
with a time delay in
the loop.
G
c(s) G(s)
Controller
+
+ +
-
R(s) Y(s)
Process
M(s)
z e
FIgUre 12.25
Multiplicative
uncertainty
representation.
M(s)
z e
1 + G
c(s)G(s)
-G
c(s)G(s)
FIgUre 12.26
Equivalent block
diagram depiction
of the multiplicative
uncertainty.880 Chapter 12 Robust Control Systems
The challenge is that the time delay T is not known exactly. One approach to
solving the problem is to find a weighting function, denoted by W1s2, such that
 e-jvT - 1 6  W1jv2 for all v and T1 … T … T2. (12.48)
If W1s2 satisfies the inequality in Equation (12.48), it follows that
 M1jv2 6  W1jv2 .
Therefore, the robust stability condition can be satisfied by
 W1jv2 6 2 1 + Gc1jv21G1jv2 2 for all v. (12.49)
This is a conservative bound. If the condition in Equation (12.49) is satisfied, then
stability is guaranteed in the presence of any time delay in the range T1 … T … T2
[5, 32]. If the condition is not satisfied, the system may or may not be stable.
Suppose we have an uncertain time delay that is known to lie in the range
0.1 … T … 1. We can determine a suitable weighting function W1s2 by plotting
the magnitude of e-jvT - 1, as shown in Figure 12.27 for various values of T
in the range T1 … T … T2. A reasonable weighting function obtained by trial
and error is
W1s2 = 2.5s
1.2s + 1.
10-2 10-1 100 101 102 103
0
0.5
1
1.5
2
2.5
Frequency (rad/s)
Magnitude
T = 0.1 s
T = 1 s T = 0.5 s
0W( jv)0
FIgUre 12.27
Magnitude plot
of  e-jvT - 1 for
T = 0.1, 0.5, and 1.Section 12.8 Design Examples 881
This function satisfies the condition
 e-jvT - 1 6  W1jv2 .
Keep in mind that the selection of the weighting function is not unique.
A digital audio tape (DAT) stores 1.3 gigabytes of data in a package the size
of a credit card—roughly nine times more than a half-inch-wide reel-to-reel tape
or quarter-inch-wide cartridge tape. A DAT sells for about the same amount
as a floppy disk, even though it can store 1000 times more data. A DAT can
record for two hours (longer than either reel-to-reel or cartridge tape), which
means that it can run longer unattended and requires fewer changes and hence
fewer interruptions of data transfer. DAT gives access to a given data file within
20 seconds, on the average, compared with several minutes for either cartridge or
reel-to-reel tape [2].
The tape drive electronically controls the relative speeds of the drum and tape
so that the heads follow the tracks on the tape, as shown in Figure 12.28. The control system is complex because motors have to be accurately controlled: capstan,
take-up and supply reels, drum, and tension control. The elements of the design
process emphasized in this example are highlighted in Figure 12.29.
Consider the speed control system shown in Figure 12.30. The motor and load
transfer function varies because the tape moves from one reel to the other. The
transfer function is
G1s2 =
Km
1s + p121s + p22, (12.50)
where nominal values are K
m = 4, p1 = 1, and p2 = 4. However, the range of variation is 3 … K
m … 5, 0.5 … p1 … 1.5, and 3.5 … p2 … 4.5. Thus, the process belongs to a family of processes, where each member corresponds to different values
of K
m, p1, and p2. The design goal is
Design Goal
Control the DAT speed to the desired value in the presence of significant process uncertainties.
Take-up
reel
Supply
reel
Write
head
Guide roller (out)
Guide roller (in)
Fixed post
Capstan
Guide roller
Fixed post
Pinch
roller
Fixed post
Tension post
Fixed post
Tape
Read
head
Rotary drum
y(t)
FIgUre 12.28
Digital audio tape
driver mechanism.882 Chapter 12 Robust Control Systems
See Equation (12.50).
See Equation (12.51).
See Figures 12.32–12.34.
Establish the system configuration
Obtain a model of the process, the
actuator, and the sensor
If the performance meets the specifications,
then finalize the design.
If the performance does not meet the
specifications, then iterate the configuration.
Identify the variables to be controlled
Establish the control goals
Topics emphasized in this example
Write the specifications
Optimize the parameters and
analyze the performance
Describe a controller and select key
parameters to be adjusted
Control the DAT speed
to the desired value
in the presence of significant
plant uncertainties.
DAT speed, Y(s).
Design specifications:
DS1: P.O. 6 13% and T
s 6 2s
DS2: Robust stability
See Figures 12.28 and 12.30.
FIgUre 12.29 Elements of the control system design process emphasized in this digital audio
tape speed control design.
G
c(s) G(s)
Controller
+ -
R(s) Y(s)
Motor and load
FIgUre 12.30
Block diagram of
the digital audio
tape speed control
system.
Associated with the design goal we have the variable to be controlled defined as
Variable to Be Controlled
DAT speed Y1s2.
The design specifications are
Design Specifications
DS1 Percent overshoot of P.O. … 13% and settling time of Ts … 2 s for a unit
step input.
DS2 Robust stability in the presence of a time delay at the plant input. The time delay
value is uncertain but known to be in the range 0 … T … 0.1.Section 12.8 Design Examples 883
Design specification DS1 must be satisfied for the entire family of plants. Design
specification DS2 must be satisfied by the nominal process 1Km = 4, p1 = 1, p2 = 42.
The following constraints on the design are given:
❏❏ Fast peak time requires that an overdamped condition is not acceptable.
❏❏ Use a PID controller:
G
c1s2 = KP +
KI
s
+ KDs. (12.51)
❏❏ K
mKD … 20 when Km = 4.
The key tuning parameters are the PID gains:
Select Key Tuning Parameters
KP, KI, and KD.
Since we are constrained to have K
mKD … 20 when Km = 4, we must select KD … 5.
We will design the PID controller using nominal values for Km, p1, and p2. We then
analyze the performance of the controlled system for the various values of the process parameters, using a simulation to check that DS1 is satisfied. The nominal process is given by
G1s2 = 4
1s + 121s + 42.
The closed-loop transfer function is
T1s2 = 4KDs2 + 4KPs + 4KI
s3 + 15 + 4KD2s2 + 14 + 4KP2s + 4KI.
If we choose KD = 5, then we write the characteristic equation as
s3 + 25s2 + 4s + 41KPs + KI2 = 0,
or
1 +
4KP1s + KI>KP2
s1s2 + 25s + 42 = 0.
Per specifications, we try to place the dominant poles in the region defined by
zvn 7 2 and z 7 0.55. We need to select a value of t = KI>KP, and then we can
plot the root locus with the gain 4KP as the varying parameter. After several iterations, we choose a reasonable value of t = 3. The root locus is shown in Figure
12.31. We determine that 4KP = 120 represents a valid selection since the roots lie
inside the desired performance region. We obtain KP = 30, and KI = tKP = 90.
The PID controller is then given by
G
c1s2 = 30 + 90
s
+ 5s. (12.52)
The step response (for the process with nominal parameter values) is shown in
Figure 12.32. A family of responses is shown in Figure 12.33 for various values of884 Chapter 12 Robust Control Systems
-25 -20 -15 -10 -5 0
-25
-20
-15
-10
-5
5 0
10
15
20
25
Real axis
Imaginary axis
Performance
region
Selected
FIgUre 12.31 point 4KP = 120
Root locus for
the DAT system
with KD = 5 and
t = KI>Kp = 3.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (s)
y(t)
FIgUre 12.32
Unit step response
for the DAT system
with Kp = 30,
KD = 5, and
KI = 90.Section 12.8 Design Examples 885
Km
, p1, and p2. None of the responses suggests a percent overshoot over the specified value of P.O. = 13%, and the settling times are all under the Ts … 2 s specification as well. As we can see in Figure 12.33, all of the tested processes in the
family are adequately controlled by the single PID controller in Equation (12.52).
Therefore DS1 is satisfied for all processes in the family.
Suppose the system has a time delay at the input to the process. The actual
time delay is uncertain but known to be in the range 0 … T … 0.1 s. Following the
method discussed previously, we determine that a reasonable function W1s2 which
bounds the plots of  e-jvT - 1 for various values of T is
W1s2 = 0.29s
0.28s + 1.
To check the stability robustness property, we need to verify that
 W1jv2 6 2 1 + Gc1jv21G1jv2 2 for all v. (12.53)
The plot of both  W1jv2 and 2 1 + Gc1jv21G1jv2 2 is shown in Figure 12.34. It can
be seen that the condition in Equation (12.53) is indeed satisfied. Therefore, we
expect that the nominal system will remain stable in the presence of time-delays up
to 0.1 seconds. ■
0 0.5 1 1.5 2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (s)
y(t)
FIgUre 12.33
A family of step
responses for the
DAT system for
various values
of the process
parameters Km, p1,
and p2.886 Chapter 12 Robust Control Systems
12.9 the PseuDO-Quantitative feeDback system
Quantitative feedback theory (QFT) uses a controller, as shown in Figure 12.35, to
achieve robust performance. The goal is to achieve a wide bandwidth for the closedloop transfer function with a high loop gain K. Typical QFT design methods use
graphical and numerical methods in conjunction with the Nichols chart. Generally,
QFT design seeks a high loop gain and large phase margin so that robust performance is achieved [24–26, 28].
In this section, we pursue a simple method of achieving the goals of QFT with
an s-plane, root locus approach to the selection of the gain K and the compensator
G
c1s2. This approach, dubbed pseudo-QFT, follows these steps:
1. Place the n poles and m zeros of G1s2 on the s-plane for the nth order G1s2. Also, add
any poles of Gc1s2.
2. Starting near the origin, place the zeros of Gc1s2 immediately to the left of each of the
1n - 12 poles on the left-hand s-plane. This leaves one pole far to the left of the lefthand side of the s-plane.
3. Increase the gain K so that the roots of the characteristic equation (poles of the closedloop transfer function) are close to the zeros of Gc1s2G1s2.
This method introduces zeros so that all but one of the root loci end on finite
zeros. If the gain K is sufficiently large, then the poles of T1s2 are almost equal
to the zeros of G
c1s2G1s2. This leaves one pole of T1s2 with a significant partial
fraction residue and the system with a phase margin of approximately 90° (actually
about 85°).
10-2 10-1 100 101 102
6 5 4 3 2 1 0
Frequency (rad/s)
Magnitude
0W( jv)0
G
c( jv)G( jv)
1
P 1 + P
FIgUre 12.34
Stability robustness
to a time delay
of uncertain
magnitude.Section 12.9 The Pseudo-Quantitative Feedback System 887
ExamplE 12.11 Design using the pseudo-QFT method
Consider the system of Figure 12.35 with
G1s2 = 1
1s + p121s + p22,
where the nominal case is p1 = 1 and p2 = 2, with {50, variation. The worst case
is with p1 = 0.5 and p2 = 1. We wish to design the system for zero steady-state
error for a step input, so we use the PID controller
G
c1s2 =
1s + z121s + z22
s
.
We then invoke the internal model principle, with R1s2 = 1>s incorporated within
G
c1s2G1s2. Using Step 1, we place the poles of Gc1s2G1s2 on the s-plane, as shown
in Figure 12.36. There are three poles (at s = 0, -1, and -2), as shown. Step 2 calls
for placing a zero to the left of the pole at the origin and at the pole at s = -1, as
shown in Figure 12.36.
The compensator is thus
G
c1s2 =
1s + 0.821s + 1.82
s
. (12.54)
R(s) K Gc(s) G(s) Y(s)
-
+
FIgUre 12.35
Feedback system.
-3 -2 -1
jv
s
-1.8 -0.8
FIgUre 12.36
Root locus for
KG
c1s2G1s2.888 Chapter 12 Robust Control Systems
We select K = 100, so that the roots of the characteristic equation are close to the
zeros. The closed-loop transfer function is
T1s2 =
1001s + 0.8021s + 1.802
1s + 0.79821s + 1.79721s + 100.42 L
100
s + 100. (12.55)
This closed-loop system provides a fast response and possesses a phase margin
of P.M. = 85°. When the worst-case conditions are realized 1p1 = 0.5 and p2 = 12,
the performance remains essentially unchanged. Pseudo-QFT design results in very
robust systems. ■
12.10 rObust cOntrOl systems using cOntrOl Design sOftware
In this section, we investigate robust control systems using control design software.
In particular, we will consider the commonly used PID controller in the feedback
control system shown in Figure 12.16. Note that the system has a prefilter Gp1s2.
The objective is to choose the PID parameters KP, KI, and KD to meet
the performance specifications and have desirable robustness properties.
Unfortunately, it is not immediately clear how to choose the parameters in the
PID controller to obtain certain robustness characteristics. An illustrative example will show that it is possible to choose the parameters iteratively and verify
the robustness by simulation. Using the computer helps in this process, because
the entire design and simulation can be automated using scripts and can easily be
executed repeatedly.
ExamplE 12.12 Robust control of temperature
Consider the feedback control system in Figure 12.16, where
G1s2 = 1
1s + c022,
and the nominal value is c0 = 1, and Gp1s2 = 1. We can design a compensator
based on c0 = 1 and check robustness by simulation. Our design specifications are
1. A settling time (with a 2% criterion) Ts … 0.5 s, and
2. An optimum ITAE performance for a step input.
For this design, we will not use a prefilter to meet specification (2), but will
instead show that acceptable performance (i.e., low percent overshoot) can be
obtained by increasing a cascade gain.
The closed-loop transfer function is
T1s2 = KDs2 + KPs + KI
s3 + 12 + KD2s2 + 11 + KP2s + KI. (12.56)Section 12.10 Robust Control Systems Using Control Design Software 889
The associated root locus equation is
1 + Kn ¢ s2 + sas 3 + b ≤ = 0,
where
Kn = KD + 2, a = 1 + KP
2 + KD, and b =
KI
2 + KD
.
The settling time requirement Ts 6 0.5 s leads us to choose the roots of s2 + as + b
to the left of the s = -zvn = -8 line in the s-plane, as shown in Figure 12.37, to
ensure that the locus travels into the required s-plane region. We have chosen
a = 16 and b = 70 to ensure the locus travels past the s = -8 line. We select a
point on the root locus in the performance region, and using the rlocfind function,
we find the associated gain Kn and the associated value of vn. For our chosen point,
we find that
Kn = 118.
Then, with Kn, a, and b, we can solve for the PID coefficients as follows:
KD = Kn - 2 = 116,
KP = a12 + KD2 - 1 = 1887,
KI = b12 + KD2 = 8260.
To meet the overshoot performance requirements for a step input, we will use a
cascade gain K that will be chosen by iterative methods using the step function,
as illustrated in Figure 12.38. The step response corresponding to K = 5 has an
-15 -10 -5 0 5
Imaginary axis
Real axis
  CDPWO=CD?FGP=?U[UVH
PWOFGP
  TNQEWU
U[U
  TNQEƂPF
U[U
Selected roots
s = -8
-20
-15
-10
-5
5 0
10
15
20
Triple
pole
FIgUre 12.37
Root locus for the
PID-compensated
temperature
controller as Kn
varies.890 Chapter 12 Robust Control Systems
acceptable percent overshoot of P.O. = 2%. With the addition of the gain K = 5,
the final PID controller is
G
c1s2 = K
KDs2 + KPs + KI
s
= 5 116s2 + 1887s + 8260
s
. (12.57)
We do not use the prefilter. Instead, we increase the cascade gain K to obtain satisfactory transient response. Now we can consider the question of robustness to
changes in the plant parameter c0.
The investigation into the robustness of the design consists of a step response
analysis using the PID controller given in Equation (12.57) for a range of plant
parameter variations of 0.1 … c0 … 10. The results are displayed in Figure 12.39.
The script is written to compute the step response for a given c0. It can be convenient to place the input of c0 at the command prompt level to make the script more
interactive.
The simulation results indicate that the PID design is robust with respect to
changes in c0. The differences in the step responses for 0.1 … c0 … 10 are barely
discernible on the plot. If the results showed otherwise, it would be possible to iterate on the design until an acceptable performance was achieved. The interactive
capability of the m-file allows us to check the robustness by simulation. ■
-U
CD
-
-&-U-2C

-&-+D

-&
PWOIE-=-&-2-+?FGPIE=?U[UIEVH
PWOIEFGPIE
PWOI=?FGPI=?U[UIVH
PWOIFGPI

U[UQUGTKGU
U[UIEU[UI

U[UHGGFDCEM
U[UQ=?
UVGR
U[U
0 0.05 0.1 0.15 0.2 0.25 0.3
Amplitude
Time (s)
Gain from uncompensated
root locus.
Increase system gain
to reduce overshoot.
PID gains.
0
0.2
0.4
0.6
0.8
1
1.2
K = 1
K = 2
K = 5
FIgUre 12.38
Step response of
the PID temperature
controller.Section 12.11 Sequential Design Example: Disk Drive Read System 891
12.11 seQuential Design examPle: Disk Drive reaD system
In this section, we design a PID controller to achieve the desired system response.
Many disk drive head control systems use a PID controller and use a command
signal r1t2 that utilizes an ideal velocity profile at the maximum allowable velocity
until the head arrives near the desired track, when r1t2 is switched to a step-type
input. Thus, we want zero steady-state error for a ramp (velocity) signal and a step
signal. Examining the system shown in Figure 12.40, we note that the forward path
possesses two pure integrations, and we expect zero steady-state error for a velocity
input r1t2 = At, t 7 0.
The PID controller is
G
c1s2 = KP + KI
s
+ KDs =
KD1s + z121s + zn12
s
.
The motor field transfer function is
G11s2 = 5000
1s + 10002 L 5.
0 0.05 0.1 0.15 0.2 0.25 0.3
Amplitude
Time (s)
0
0.2
0.4
0.6
0.8
1
1.4
1.2
E
PWOI=?FGPI=EE@?
PWOIE=?FGPIE=?
U[UIVH
PWOIFGPI
U[UIEVH
PWOIEFGPIE

U[UQUGTKGU
U[UIEU[UI

U[UHGGFDCEM
U[UQ=?

UVGR
U[U
0.1 … c0 … 10
Specify process parameter.
FIgUre 12.39
Robust PID
controller analysis
with variations in c0.892 Chapter 12 Robust Control Systems
The second-order model uses G11s2 = 5, and the design is determined for this model.
We use the second-order model and the PID controller for the s-plane design
technique illustrated in Section 12.6. The poles and zeros of the system are shown in
the s-plane in Figure 12.41 for the second-order model and G11s2 = 5. Then we have
the loop transfer function
L1s2 = Gc1s2G11s2G21s2 =
5KD1s + z121s + zn12
s21s + 202 .
We select -z1 = -120 + j40 and determine 5KD so that the roots are to the left of
the line s = -100. If we achieve that requirement, then
Ts
6
4
100,
and the percent overshoot to a step input is (ideally) P.O. … 2% since z of the
complex roots is approximately 0.8. Of course, this sketch is only a first step. As a
+ -
+
+
R(s) Y(s)
PID controller Motor coil
G1(s)
Td(s)
Load
G2(s) = 1
Gc(s) = s(s + 20)
KD
(s + z1)(s + z1)
s
FIgUre 12.40 Disk drive feedback system with a PID controller.
j120
j80
j50
j40
-j40
-j50
-j100
-j120
-140 -120 -100 -80 -60 -40 -20
2 poles
-z1
jv
s
FIgUre 12.41 -zN1
A sketch of a
root locus at KD
increases for
estimated root
locations with a
desirable system
response.Section 12.12 Summary 893
second step, we determine KD. We then obtain the actual root locus as shown in
Figure 12.42 with KD = 800. The system response is recorded in Table 12.6. The
system meets all the specifications.
12.12 summary
The design of highly accurate control systems in the presence of significant plant
uncertainty requires the designer to seek a robust control system. A robust control
system exhibits low sensitivities to parameter change and is stable over a wide range
of parameter variations.
The PID controller was considered as a compensator to aid in the design of
robust control systems. The design issue for a PID controller is the selection of
the gain and two zeros of the controller transfer function. We used three design
methods for the selection of the controller: the root locus method, the frequency
response method, and the ITAE performance index method. An operational amplifier circuit used for a PID controller is shown in Figure 12.43. In general, the 