 best gpus for machine learning for your next project need a gpu for machine learning explore this guide to find the best gpus for machine learning available in the market projectpro get access to all machine learning projects view all machine learning projects best gpus for machine learning for your next project last updated nov by nishtha confused about which gpu to choose for your project this blog covers the top gpus for machine learning and also guides you through the relevant factors to consider to make an informed decision when selecting a gpu for your next machine learning project according to jpr the gpu market is expected to reach million units by at an annual rate of this statistic is a clear indicator of the fact that the use of gpus for machine learning has evolved in recent years deep learning a subset of machine learning necessitates dealing with massive data neural networks parallel computing and the computation of a large number of matrices all these methods use algorithms that process large volumes of data and transform it into usable software this necessitates using a graphic card for processing to perform these tasks with deep learning and neural networks gpus come into play here using gpus you can break down complex tasks and perform multiple operations simultaneously in addition gpus are ideal for developing deep learning and artificial intelligence models as they can handle numerous computations simultaneously before diving into the best gpus for deep learning or the best graphics card for machine learning let us know more about gpus table of contents what is a gpu for machine learning why are gpus better than cpus for machine learning how do gpus for machine learning work how to choose the best gpu for machine learning factors to consider when selecting gpus for machine learning algorithm factors affecting gpu use for machine learning best gpus for machine learning in the market best gpus for deep learning best nvidia gpus for deep learning best gpus for deep learning best budget gpus for deep learning key takeaways faqs on gpu for machine learning what is a gpu for machine learning a gpu graphic processing unit is a logic chip that renders graphics on display images videos or games a gpu is sometimes also referred to as a processor or a graphics card gpus are used for different types of work such as video editing gaming designing programs and machine learning ml thus they are ideal for designers developers or anybody looking for high quality visuals it is possible however to find a gpu integrated into a motherboard or in the daughterboard of a graphics card initially graphic cards were only available on high configuration computers but today most desktop computers use a separate graphics card with a gpu rather than one built into the motherboard for increased performance why are gpus better than cpus for machine learning when it comes to machine learning even a very basic gpu outperforms a cpu but why so gpus offer significant speed ups over cpus when it comes to deep neural networks gpus are faster for computing than cpus this is because they are ideal for parallel computing and can perform multiple tasks simultaneously while cpus perform sequential tasks in addition gpus are ideal for the computation of artificial intelligence and deep learning applications since data science model training is based on simple matrix operations gpus can be used safely for deep learning gpus can execute many parallel computations and increase the quality of images on the screen gpus assemble many specialized cores that deal with huge data sets and deliver massive performance a gpu devotes more transistors to arithmetic logic than a cpu does to caching and flow control deep learning gpus provide high performance computing power on a single chip while supporting modern machine learning frameworks like tensorflow and pytorch with little or no setup how do gpus for deep learning work graphical processing units gpus are built explicitly for graphics processing which requires complex mathematical calculations running parallel to display images on the screen a gpu receives graphic information such as image geometry color and textures from the cpu and executes them to draw images on the screen thus this is how a gpu works to render images on the screen this complete process of taking instructions to create a final image on the screen is known as rendering for instance video graphics are made up of polygonal coordinates translated into bitmaps and then into signals shown on a screen this translation necessitates massive processing power from the graphics processing unit gpu which makes gpus useful in machine learning artificial intelligence and other deep learning tasks that necessitate complex computations why use gpus for ml the next most important question to be answered is why use gpus for machine learning or why are gpus better for machine learning read on to find out the concept of deep learning involves complex computing tasks such as training deep neural networks mathematical modeling using matrix calculations and working with d graphics all these deep learning tasks require choosing a reasonably powerful gpu a distinctive gpu not only helps with a high quality image but also increases the efficiency of the cpu and achieves outstanding performance thus investing in a top quality gpu is best to speed up the model training process on the other hand gpus have dedicated video ram vram which provides the required memory bandwidth for massive datasets while freeing up cpu time for different operations they also enable you to parallelize training tasks by dividing them among processor clusters and carrying out computation operations simultaneously gpus can perform simultaneous computations involved in machine learning it is also important to note that you dont need gpus to learn machine learning or deep learning they are essential only when you want to speed up your things while working with complex models huge datasets and a large number of images but how do you choose the best gpu for machine learning lets find out how to choose the best gpu for machine learning with this rapidly growing field of gpus various options are available in the market to keep the need of designers and data scientists therefore it is essential to keep several factors in mind before purchasing a gpu for machine learning factors to consider when selecting gpus for machine learning below you will find the factors one must consider while deciding on the best graphics card for ai ml dl projects factors to consider when selecting a gpu compatibility the gpus compatibility with your computer or laptop should be your primary concern does your devices gpu perform well you can also check the display ports and cables for deep learning applications memory capacity the first and most important requirement for selecting gpus for machine learning is more ram deep learning necessitates intense gpu memory capacity for example algorithms that use long videos as training data sets require gpus with greater memory compared to this the fundamental training data sets function effectively on cloud gpus with less memory memory bandwidth large datasets require a lot of memory bandwidth which gpus may provide this is due to the separate video ram vram found in gpus which lets you save cpu memory for other uses gpus interconnecting ability the ability to connect multiple gpus is closely related to your scalability and distributed training strategies as a result one should consider which gpu units can be interconnected when selecting a gpu for machine learning tdp value gpus can sometimes overheat as indicated by the tdp value they can heat up more quickly when they need more electricity to operate so it is necessary to keep gpus at a cool temperature steam processors steam processors also called cuda cores are suitable for professional players and deep learning a gpu with a high cuda core increases work efficiency in deep learning applications looking for end to end solved data science projects check out projectpros repository of solved data science projects with source code algorithm factors affecting gpu use for machine learning algorithm factors affecting gpu usage when it comes to gpu usage algorithmic factors are equally important and must be considered listed below are three factors to consider when scaling your algorithm across multiple gpu for ml data parallelism it is essential to consider how much data your algorithms will need to handle if the data set is large the chosen gpu should be able to function efficiently on multi gpu training if the data set is large you must ensure the servers can communicate quickly with storage components to enable effective distributed training memory use another essential factor you must consider for gpu usage is the memory requirements for training datasets for example algorithms that use long videos or medical pictures as training data sets require a gpu with large memory on the other hand simple training data sets used for basic predictions need less gpu memory to work gpu performance the models performance also influences gpu selection regular gpus for example are used for development and debugging strong and powerful gpus are required for model fine tuning to accelerate training time and reduce waiting hours best gpus for machine learning in the market so what makes gpu ideal for machine learning this is because of a variety of reasons gpus are designed to do multiple computations in parallel which is excellent for deep learning algorithms highly parallel nature they also contain a large amount of memory which is useful for deep learning models that require a lot of data it is also important to note that large scale operations rarely buy gpus unless they have their specialized processing cloud organizations running machine learning workloads instead acquire cloud space optimized for high performance computing these cloud providers platforms feature high performance gpus and fast memory but from where do they buy the best gpu for ai training gpu market players nvidia and amd gpu market players there are two major players in the machine learning gpu market amd and nvidia there are a large number of gpus used for deep learning however nvidia makes the majority of the best ones nvidia dominates the market of gpus especially for deep learning and complex neural networks because of their substantial support in the forum software drivers cuda and cudnn explore categories data science projects in python data science projects in r machine learning projects in python machine learning projects in r deep learning projects neural network projects tensorflow projects keras deep learning projects nlp projects pytorch data science projects in banking and finance data science projects in retail ecommerce data science projects in entertainment media data science projects in telecommunications nvidia gpu for deep learning nvidia is a popular choice because of its libraries known as the cuda toolkit these libraries make it simple to set up deep learning processes and provide the foundation of a robust machine learning community using nvidia products in addition to gpus nvidia also provides libraries for popular deep learning frameworks such as pytorch and tensorflow the nvidia deep learning sdk adds gpu acceleration to popular deep learning frameworks data scientists may use powerful tools and frameworks to create and deploy deep learning applications nvidias downside is that it has lately set limits on when you may use cuda due to these constraints the libraries can only be used with tesla gpus not with less costly rtx or gtx hardware this has significant financial implications for firms training deep learning models it is also problematic when you consider that while tesla gpus may not provide considerably greater performance than the alternatives the units cost up to ten times as much amd gpu for deep learning amd gpus are excellent for gaming but nvidia outperforms when deep learning comes into the picture amd gpus are less in use because of software optimization and drivers that need to be frequently updated while on the nvidia side they have superior drivers with frequent updates and on top of that cuda and cudnn help accelerate computation amd gpus have extremely minimal software support amd provides libraries such as rocm all significant network architectures as well as tensorflow and pytorch support these libraries however community support for the development of new networks is minimal best gpu for deep learning looking at the factors mentioned above for choosing gpus for deep learning you can now easily pick the best one from the following list based on your machine learning or deep learning project requirements best nvidia gpus for deep learning check out the best nvidia gpus for deep learning below nvidia titan rtx nvidia titan rtx is a high end gaming gpu that is also great for deep learning tasks built for data scientists and ai researchers this gpu is powered by nvidia turing architecture to offer unbeatable performance the titan rtx is the best pc gpu for training neural networks processing massive datasets and creating ultra high resolution videos and d graphics additionally it is supported by nvidia drivers and sdks enabling developers researchers and creators to work more effectively to deliver better results technical features cuda cores tensor cores gpu memory gb gddr memory bandwidth gb s compute apis cuda directcompute opencl nvidia tesla v nvidia tesla is the first tensor core gpu built to accelerate artificial intelligence high performance computing hpc deep learning and machine learning tasks powered by nvidia volta architecture tesla v delivers tflops of deep learning performance for training and inference in addition it consumes less power than other gpus nvidia tesla is one of the markets best gpus for deep learning due to its outstanding performance in ai and machine learning applications with this gpu data scientists and engineers may now focus on building the next ai breakthrough rather than optimizing memory usage technical features cuda cores tensor cores memory bandwidth gb s gpu memory gb clock speed mhz compute apis cuda directcompute opencl openacc unlock the projectpro learning experience for free nvidia quadro rtx nvidia quadro rtx is the worlds most powerful graphics card built by pny for deep learning matrix multiplications a single quadro rtx card can render complex professional models with realistically accurate shadows reflections and refractions providing users with rapid insight powered by the nvidia turingtm architecture and nvidia rtxtm platform quadro provides professionals with the latest hardware accelerated real time ray tracing deep learning and advanced shading when used with nvlink its memory may be expanded to gb technical features cuda cores tensor cores gpu memory gb gddr memory bandwidth gb s compute apis cuda directcompute opencl nvidia tesla p based on nvidia pascal architecture the nvidia tesla p is a gpu built for machine learning and hpc tesla p with nvidia nvlink technology provides lightning fast nodes to reduce time to solution for large scale applications significantly with nvlink a server node may link up to eight tesla p s at x the bandwidth of pcie technical features cuda cores tensor cores memory bandwidth gb s compute apis cuda opencl cudnn nvidia rtx a one of the latest gpus is the nvidia rtx a which is excellent for deep learning based on the turing architecture it can execute both deep learning algorithms and conventional graphics processing tasks the rtx a also has deep learning super sampling as a feature dlss this feature can render images at higher resolutions while maintaining quality and speed a geometry processor texture mapper core rasterizer core and video engine core are some of the other features of this gpu technical features cuda cores tensor cores gpu memory gb if you are specifically interested in a good gpu for llm projects then we recommend you check out the nvidia geforce rtx one of the best gpu for llm project ideas get confident to build end to end projects access to a curated library of end to end industry projects with solution code videos and tech support best gpus for deep learning find below the top five gpu for deep learning examples nvidia geforce rtx ti nvidia geforce rtx ti is one of the best gpu for deep learning if you are a data scientist that performs deep learning tasks on your machine its incredible performance and features make it ideal for powering the most advanced neural networks than other gpus powered by nvidia ampere architecture it provides the fastest speeds possible with this nvidia geforce rtx gpu gaming enthusiasts may experience a maximum setting of k ray traced games at the fastest possible rates and even k nvidia dlss accelerated gaming on monitors that support k hz as stated in hdmi technical features cuda cores memory bandwidth gb s gpu memory gb gddr memory evga geforce gtx evga geforce gtx is one of the most advanced gpus designed to deliver the fastest and most efficient gaming experiences based on nvidias pascal architecture it provides significant improvements in performance memory bandwidth and power efficiency additionally it provides cutting edge visuals and technologies that redefine the pc as the platform for enjoying aaa games and fully utilizing virtual reality via nvidia vrworks technical features cuda cores gpu memory gb of gddr x pascal architecture zotac geforce gtx geforce gtx mini is one of the best gpu for deep learning due to its top notch specifications low noise levels and small size the gpu has an hdmi connector that you may use to attach your pc to an hdtv or other display device additionally the zotac geforce gtx mini is compatible with nvidia g sync which reduces input latency and screen tearing while enhancing speed and smoothness while developing deep learning algorithms technical features cuda cores cores gpu memory gb gddr clock speed mhz gigabyte geforce rtx the gigabyte geforce rtx is the best gpu for deep learning since it was designed to meet the requirements of the latest deep learning techniques such as neural networks and generative adversarial networks the rtx enables you to train your models much faster than with a different gpu the geforce rtx also offers a k display output allowing you to connect multiple displays and design neural networks more quickly technical features cuda cores clock speed mhz gpu memory gb of gddr nvidia a the nvidia a gpu built on the ampere architecture excels in powering deep learning tasks it features tensor cores for efficient matrix operations offers high memory capacities supports nvlink for multi gpu configurations and boasts a rich ai software ecosystem data centers widely adopt it and it is compatible with popular frameworks making it a premier choice for accelerating the training of large neural networks technical features cuda cores clock speed ghz thermal design power tdp watts tensor cores what makes python one of the best programming languages for ml projects the answer lies in these solved and end to end machine learning projects in python check them out now best budget gpu for deep learning examples here you will find a few examples of the best budget gpu for ai projects deep learning project ideas nvidia gtx super the nvidia gtx super is one of the budget friendly gpu offering decent performance for its price with gb of gddr memory and a reasonable number of cuda cores it was suitable for smaller deep learning tasks and well supported by popular frameworks like tensorflow and pytorch its power efficiency and affordability made it an attractive option for budget conscious users interested in deep learning or gaming technical features cuda cores gpu memory gb of gddr vram clock speed mhz gpu chip tu gpu chip turing architecture gtx super one of the greatest low cost gpus for deep learning is the gtx super its performance is not excellent as more costly models because its an entry level graphic card for deep learning this gpu is the best option for you and your pocketbook if youre just starting with machine learning technical features cuda cores memory bandwidth gb s power w clock speed mhz nvidia geforce rtx ti nvidia geforce rtx ti is the ideal gpu for deep learning and artificiai intelligence both from a pricing and performance perspective it has dual hdb fans that provide greater performance cooling significantly less acoustic noise and real time ray tracing in games for cutting edge hyper realistic visuals the rtx s blower architecture enables much denser system configurations including up to four gpus in a single workstation in addition the nvidia geforce rtx ti is a low cost solution suited best for small scale modeling workloads than large scale training developments because of having less gpu memory per card only gb technical features cuda cores memory bandwidth gb s clock speed mhz nvidia tesla k the nvidia tesla k is the worlds most popular and budget friendly gpu that significantly reduces data center costs by offering a great performance boost with fewer more powerful servers for example if youve used google colab to train mask rcnn youll have noted that the nvidia testa k is among the video gpus that google makes available it is ideal for deep learning but isnt the perfect option for deep learning professionals for their projects technical features cuda cores gpu memory gb of gddr memory bandwidth gb s evga geforce gtx the evga geforce gtx ftw gaming graphics card based on nvidias pascal architecture and equipped with a factory overclocked core offers significant enhancements in performance memory bandwidth and power efficiency over the high performing maxwell architecture additionally it provides cutting edge visuals and technologies that redefine the pc as the platform for enjoying aaa games and entirely using virtual reality with nvidia vrworks technical features cuda cores gpu memory gb of gddr x memory bandwidth gb s key takeaways the gpu market will continue to grow in the future as we make innovations and breakthroughs in machine learning deep learning and hpc gpu acceleration will always be helpful for students and developers looking to break into this sector especially as their costs continue to decrease if you are still confused about picking the right gpu for learning machine learning or deep learning projectpro experts are here to help you out you can also schedule a one to one mentoring session with our industry experts to help you pick the right gpu for your next machine learning project while you kickstart your career in machine learning access data science and machine learning project code examples faqs on gpu for machine learning which is the top gpu for deep learning nvidia the market leader offers the best deep learning gpus in the top nvidia models are titan rtx rtx quadro rtx and rtx a can gpus be used for machine learning yes gpus are capable of doing several calculations at the same time this enables the distribution of training processes which may significantly speed up machine learning activities you can build many cores with gpus that consume fewer resources without losing efficiency or power how many gpus are enough for deep learning it all depends on the deep learning model being trained the quantity of data available and the size of the neural network are gaming gpus good for machine learning graphics processing units gpus initially designed for the gaming industry feature many processing cores and a significant amount of ram on board gpus are increasingly employed in deep learning applications because they can significantly accelerate neural network training about the author nishtha is a professional technical content analyst at projectpro with over three years of experience in creating high quality content for various industries she holds a bachelors degree in electronics and communication engineering and is an expert in creating seo reasons id buy a d printer this black friday by anj bryant published november its the best time to shop for a d printer and spark your creativity black friday sales are here and some of the best d printers and best budget d printers are being offered with great discounts at amazon best buy and other major online retail stores so if you have been thinking about owning one of these machines for yourself or giving it as a gift to a loved one now is a good time to take the plunge and take advantage of the savings while it lasts you dont need to be a master maker or a small business owner to make use of and enjoy printing models or creating diy projects its been a few years since i embraced the world of d printing and not only has it been something that my kids and i bond over but it has been an essential tool we use throughout the year for printing personalized gifts or making cool gadgets and little trinkets for ourselves its a lot of fun and a money saver why buy it when you can make it yourself if you are still on the fence about whether you should buy a d printer here are five good reasons why its time to take advantage of the best black friday d printer deals its cheaper yes there was a time when d printers werent affordable for the average consumer however the cost of desktop d printers has become more reasonable these days presumably to address the growing demand for low cost options the great thing about this is that you can now find d printers for less than that are still capable and do not skimp on high quality features like the elegoo neptune pro which remains one of our favorite budget d printers and is currently on sale for at amazon or alternatively the creality ender s is also on sale right now from a retail price of down to ive been using this machine for two years now and it still turns out great print quality its a reliable workhorse if you dont mind the sticky build plate both d printers are on huge black friday sales best black friday budget d printer deals elegoo neptune pro now at amazon was one of our favorite d printers the neptune pro boasts extremely powerful and high quality features while still maintaining its low cost value creality ender s now at amazon was this budget friendly bed slinger offers direct drive dual z axis auto bed leveling silent stepper motors a removable flex plate and a color display screen perfect for beginners or expert makers its faster d printers have also gotten a lot faster if youve ever waited for over ten hours for a single model to print like i have then you know the pain of wasting almost an entire day waiting for your build to finish so thankfully there are now d printers that can save you time and get back hours of your day granted some high speed machines may come at a premium like the bamboo lab p s on sale for but still quite pricey but would be well worth the time you save time is money as the saying goes and you still come out ahead in the end when i first got the bambu lab a mini i was so impressed with its speed it does have a smaller footprint so if you plan to print larger models this might not be a good fit for you but the quality of the prints is smooth and comes out almost perfect every time its worth the cost another option is the sovol sv which is also a fast machine and on sale for you can check out our d printer speed hierarchy for a list of the best speedsters in the market today best black friday high speed d printer deals bambu lab a mini now at bambu lab this bed slinger may come with a small x x mm build volume but it has an impressive top speed of mm s perfect for anyone wanting to print multi color models without breaking the bank sovol sv now at amazon was the sovol sv can hit mm s speed with an acceleration rate of mm s which is way faster than most bed slingers can handle if you can handle the noise this speedster is a great get at off msrp its easier to use we like the process of tinkering and assembling things here at tomshardware but if you are intimidated with the whole building process or arent a fan of putting things together yourself there are lots of ready to use out of the box d printers available as well my first d printer from three years ago was the voxelab aries it came shipped fully assembled and ready to print as soon as you take it out of its packaging this is a great beginner friendly machine because all you need to do is load your filament and you are ready to go it retailed for when it was released and you can find it on sale for today im still using it regularly because it is so easy to use and since it has the enclosure form factor its safe for my youngest daughter to use as well alternatively the creality ender v se is both easy to assemble and an affordable option currently on sale for best black friday d printer for beginners deals voxelab aries now at walmart was the voxelab aries was designed for stem classrooms it ships fully assembled and ready to use right out of the box its very user friendly and has wi fi capability but no auto bed leveling great for first time users who dont want to tinker with building the machine creality ender v se now at amazon was the creality ender v se is simple to build has deluxe features and is easy to set up and use thats everything a novice user needs unfortunately it does not have wi fi its great family fun its so much fun and practical to make things ourselves again why buy it when you can make it yourself from printing fidget toys to keepsake trinkets and even replacement parts for gadgets around the house my kids and i have done them all and have enjoyed all the time we spent making and crafting its awesome to see them inspired and see their creative juices going as mentioned earlier voxelab aries has an enclosure and thus is a safe choice and recommended for kid use another great option is the aoseed x maker which is currently on sale for best black friday d printer for kids aoseed x make now at amazon was the x make is fully enclosed to keep kids and pets from touching it it comes with an easy to use app that has a slew of built in toy designs kids can choose from it can also work with standard slicers like cura its a money saver money earner weve mentioned that you can save money by making and printing things yourself but you could also go beyond the personal hobby route and do like what many others have which is to take their creative passions a step further and use their d printers to launch small businesses to make some extra cash in this economy it may not be a bad idea depending on the type of business you embark on you may need higher end performers that can do the workload you require you may need something like the prusa mk which is a powerful machine but also tends to be on the pricey side msrp it is available as a kit direct from the prusa store for an equally capable built for volume machine is the bambu lab p p which is compatible with ams for multi color printing and on sale for best black friday premium d printer deal bambu lab p p now at bambu lab was this super fast core xy printer is built for complex projects includes input shaping and has the option for four color ams no matter how you plan to use your d printer we hope we helped you make a more informed decision and saved you money along the way for more deals on d printers and materials head on to our black friday best d printer deals page happy printing stay on the cutting edge join the experts who read toms hardware for the inside track on enthusiast pc tech news and have for over years well send breaking news and in depth reviews of cpus gpus ai maker hardware and more straight to your inbox your email address contact me with news and offers from other future brands receive email from us on behalf of our trusted partners or sponsors by submitting your information you agree to the terms conditions and privacy policy and are aged or over anj bryant is the assistant managing editor at toms hardware she provides content layout and development support and coordinates editorial initiatives for all the talented groups of freelancers contributors and editors in the team zero etl chatgpt and the future of data engineering the post modern data stack is coming are we ready barr moses if you dont like change data engineering is not for you little in this space has escaped reinvention the most prominent recent examples are snowflake and databricks disrupting the concept of the database and ushering in the modern data stack era as part of this movement fivetran and dbt fundamentally altered the data pipeline from etl to elt hightouch interrupted saas eating the world in an attempt to shift the center of gravity to the data warehouse monte carlo joined the fray and said maybe having engineers manually code unit tests isnt the best way to ensure data quality today data engineers continue to stomp on hard coded pipelines and on premises servers as they march up the modern data stack slope of enlightenment the inevitable consolidation and trough of disillusionment appear at a safe distance on the horizon and so it almost seems unfair that new ideas are already springing up to disrupt the disruptors zero etl has data ingestion in its sights ai and large language models could transform transformation data product containers are eyeing the tables thrown as the core building block of data are we going to have to rebuild everything again the body of the hadoop era isnt even all that cold the answer is yes of course we will have to rebuild our data systems probably several times throughout our careers the real questions are the why when and the how in that order i dont profess to have all the answers or a crystal ball but this article will closely examine some of the most prominent near ish future ideas that may become part of the post modern data stack as well as their potential impact on data engineering practicalities and tradeoffs the modern data stack didnt arise because it did everything better than its predecessor there are real trade offs data is bigger and faster but its also messier and less governed the jury is still out on cost efficiency the modern data stack reigns supreme because it supports use cases and unlocks value from data in ways that were previously if not impossible then certainly very difficult machine learning moved from buzz word to revenue generator analytics and experimentation can go deeper to support bigger decisions the same will be true for each of the trends below there will be pros and cons but what will drive adoption is how they or the dark horse idea we havent yet discovered unlock new ways to leverage data lets look closer at each zero etl what it is a misnomer for one thing the data pipeline still exists today data is often generated by a service and written into a transactional database an automatic pipeline is deployed which not only moves the raw data to the analytical data warehouse but modifies it slightly along the way for example apis will export data in json format and the ingestion pipeline will need to not only transport the data but apply light transformation to ensure it is in a table format that can be loaded into the data warehouse other common light transformations done within the ingestion phase are data formatting and deduplication while you can do heavier transformations by hard coding pipelines in python and some have advocated for doing just that to deliver data pre modeled to the warehouse most data teams choose not to do so for expediency and visibility quality reasons zero etl changes this ingestion process by having the transactional database do the data cleaning and normalization prior to automatically loading it into the data warehouse its important to note the data is still in a relatively raw state at the moment this tight integration is possible because most zero etl architectures require both the transactional database and data warehouse to be from the same cloud provider pros reduced latency no duplicate data storage one less source for failure cons less ability to customize how the data is treated during the ingestion phase some vendor lock in whos driving it aws is the driver behind the buzzword aurora to redshift but gcp bigtable to bigquery and snowflake unistore all offer similar capabilities snowflake secure data sharing and databricks delta sharing are also pursuing what they call no copy data sharing this process actually doesnt involve etl and instead provides expanded access to the data where its stored practicality and value unlock potential on one hand with the tech giants behind it and ready to go capabilities zero etl seems like its only a matter of time on the other ive observed data teams decoupling rather than more tightly integrating their operational and analytical databases to prevent unexpected schema changes from crashing the entire operation this innovation could further lower the visibility and accountability of software engineers toward the data their services produce why should they care about the schema when the data is already on its way to the warehouse shortly after the code is committed with data steaming and micro batch approaches seeming to serve most demands for real time data at the moment i see the primary business driver for this type of innovation as infrastructure simplification and while thats nothing to scoff at the possibility for no copy data sharing to remove obstacles to lengthy security reviews may result in greater adoption in the long run although to be clear its not an either or one big table and large language models what it is currently business stakeholders need to express their requirements metrics and logic to data professionals who then translate it all into a sql query and maybe even a dashboard that process takes time even when all the data already exists within the data warehouse not to mention on the data teams list of favorite activities ad hoc data requests rank somewhere between root canal and documentation there is a bevy of startups aiming to take the power of large language models like gpt to automate that process by letting consumers query the data in their natural language in a slick interface at least until our new robot overlords make binary the new official language this would radically simplify the self service analytics process and further democratize data but it will be difficult to solve beyond basic metric fetching given the complexity of data pipelines for more advanced analytics but what if that complexity was simplified by stuffing all the raw data into one big table that was the idea put forth by benn stancil one of datas best and forward thinking writer founders no one has imagined the death of the modern data stack more as a concept its not that far fetched some data teams already leverage a one big table obt strategy which has both proponents and detractors leveraging large language models would seem to overcome one of the biggest challenges of using the one big table which is the difficulty of discovery pattern recognition and its complete lack of organization its helpful for humans to have a table of contents and well marked chapters for their story but ai doesnt care pros perhaps finally delivering on the promise of self service data analytics speed to insights enables the data team to spend more time unlocking data value and building and less time responding to ad hoc queries cons is it too much freedom data professionals are familiar with the painful eccentricities of data time zones what is an account to an extent most business stakeholders are not do we benefit from having a representational rather than direct data democracy whos driving it super early startups such as delphi and getdot ai startups such as narrator more established players doing some version of this such as amazon quicksight tableau ask data or thoughtspot practicality and value unlock potential refreshingly this is not a technology in search of a use case the value and efficiencies are evident but so are the technical challenges this vision is still being built and will need more time to develop perhaps the biggest obstacle to adoption will be the infrastructure disruption required which will likely be too risky for more established organizations data product containers what it is a data table is the building block of data from which data products are built in fact many data leaders consider production tables to be their data products however for a data table to be treated like a product a lot of functionality needs to be layered on including access management discovery and data reliability containerization has been integral to the microservices movement in software engineering they enhance portability infrastructure abstraction and ultimately enable organizations to scale microservices the data product container concept imagines a similar containerization of the data table data product containers may prove to be an effective mechanism for making data much more reliable and governable especially if they can better surface information such as the semantic definition data lineage and quality metrics associated with the underlying unit of data pros data product containers look to be a way to better package and execute on the four data mesh principles federated governance data self service treating data like a product domain first infrastructure cons will this concept make it easier or more difficult for organizations to scale their data products another fundamental question which could be asked of many of these futuristic data trends is do the byproducts of data pipelines code data metadata contain value for data teams that is worth preserving whos driving it nextdata the startup founded by data mesh creator zhamak dehgahni nexla has been playing in this space as well practicality and value unlock potential while nextdata has only recently emerged from stealth and data product containers are still evolving many data teams have seen proven results from data mesh implementations the future of the data table will be dependent on the exact shape and execution of these containers the endless reimagination of the data lifecycle to peer into the data future we need to look over our shoulder at the data past and present past present future data infrastructures are in a constant state of disruption and rebirth although perhaps we need some more chaos the meaning of data warehouse has changed drastically from the term introduced by bill inmon in the s etl pipelines are now elt pipelines the data lake is not as amorphous as it was just two years ago with these innovations ushered in by the modern data stack data engineers have still played a central technical role in dictating how data moves and how data consumers access it but some changes are bigger and scarier than others the term zero etl seems threatening because it inaccurately suggests the death of pipelines and without pipelines do we need data engineers for all the hype behind chatgpts ability to generate code this process is still very much in the hands of technical data engineers who still need to review and debug the scary aspect about large language models is how they might fundamentally warp the data pipeline or our relationship with data consumers and how data is served to them however this future if and when it comes to pass is still strongly reliant on data engineers what has endured since the dawn of time is the general lifecycle of data it is emitted it is shaped it is used and then it is archived best to avoid dwelling on our own mortality here while the underlying infrastructure may change and automations will shift time and attention to the right or left human data engineers will continue to play a crucial role in extracting value from data for the foreseeable future its not because future technologies and innovations cant simplify the complex data infrastructures of today its because our demand and uses for data will continue to increase in sophistication and scale big data has and always will be a pendulum swinging back and forth we make a leap forward in capacity and then we just as quickly figure out a way to reach those boundaries until the next leap is required take comfort in this cycle its nice to be needed shane murray co authored this article subscribe to get his stories delivered to your inbox interested in the future of data quality reach out to the monte carlo team data engineering data architecture large language models data quality data platforms written by barr moses